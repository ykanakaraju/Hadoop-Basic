
  Agenda  (20 sessions of 4 hours each)
  -------------------------------------
    	Understanding Big data
    	Hadoop - basics
    	HDFS - Architecture & File Operations
	YARN - Cluster Manager
	MapReduce 
	Hive - Data Warehousing
	Impala (introduction)	
	Sqoop (structured data ingestion tool)
	HBase ( NoSQL database )
	Kafka (pubsub messaging engine)	
	NiFi (Workflow Scheduler)
	Spark (Python) - PySpark  (Spark Core & Spark SQL)   
 ----------------------------------------------------------

   What is Big Data ?
   ------------------
  
    Refers to data that is large and complex data that is difficult to store and process
    using traditional data management tools or data processing technologies.

    Big Data Challenges:
	-> Storing huge amount of data
	-> Processing data faster
	-> Processing complex data

    What qualities of data make some data as Big Data ?   
	-> Volume  (10 TBs or PB scala data stores)
	-> Velocity (real time data processing)
	-> Variety (unstructured & semi-structured data)
	-> Veracity (data in doubt)
	-> Value (deep analytics)

   => Hadoop provides a solution to big data.


   What is a Cluster?
   ------------------		
        -> Is a unified entity comprising og many nodes whise cumulative resources (disk space, RAM, 
           CPU cores etc) can be used to distribute your storage and processing. 
  
   What is Hadoop ?
   ----------------
	Is an opensource framework for "storing" and "processing" big data on 
	"computing clusters" made of commodity machines. 

         1. HDFS   -> Hadoop Distributed File System
		   -> Distributed Storage Solution
		   -> Allows data of a single file to be distributed across many machines.
	
	 2. MapReduce -> Distributed Processing Framework
		    -> Used to distribute processing across many nodes in a cluster

	 3. YARN    -> Introduced in Hadoop 2 as a dedicated Resource Management Framework.


   
   HDFS (Hadoop Distributed File System)
   -------------------------------------	

     -> Is a distributed storage framework

     -> Stores the data as blocks of 128 MB each.
 
      1. Name Node (NN)
		-> Is a master daemon (continuously running background job)
		-> Is a Java background process
		-> Receives "Block reports" from all the DNs
		-> The meta-data of the entire file system name space is maintained using 2 files.

		   1. FSImage
			-> Contains the state of the entire cluster from the boot time upto
			   the last check point

		   2. EditLog
			-> Contains current data (on going changes to the cluster) from the last check
			   point upto the current moment. 

      2. Data Node (DN)
		-> Is a slave daemon
		-> Runs on all the slave machines
		-> DNs send heart-beats to NNs once every 3 sec.
		   -> if three successive heart-beats are missed, NN marks the DN as dead. 
		-> DNs send "block report" to NN
		   -> 'Block Report' contains such information as:
			-> Total Disk Space
			-> Available Space
			-> Reserved Space
			-> List of all the files, blocks of the file, owner of the file, permissions,
			   Replication-factor, etc
			-> If it is a dircetory, owner, timestamp etc.
	
      3. Secondary Name Node (SNN)
		-> Is a master daemon	
	
      4. Balancer
	       -> Is a daemon that runs on the master (same machine as NN)
	       -> Maintains the replication-factor of the HDFS files.	


   Hadoop EcoSystem
   ----------------
	-> A set of tools that are commonly used for various data analytics, data flow,
           data ingestion use-cases on top of Hadoop.	   

    
   Hadoop Distributions
   --------------------
	-> Third-party Hadoop Distributions help you to install many of the commonly used
	   components of Hadoop very easily.

	-> They will provide cluster-access or image-copies (pre-intslled hard-disk files)
	   containing all the hadoop components pre-installed.

	Hardware Requirements:	
	  -> Minimum 8 GB for CDH 4.7 (old and light-weight)
	  -> Minimum 12 GB to 16 GB RAM is required for CDH 5.13 and above.
	
	On-Premesis Installation
	------------------------
	-> Cloudera
	-> HortonWorks (merged with cloudera)
	-> MapR

	Cloud Hadoop Clusters
	---------------------
	-> Amazon: EMR (Elastic MapReduce - AWS)
	-> Microsoft: HDInsights (Azure)
	-> Google: DataProc (GCP)
		

   Setup Hadoop On your Local Machine
   ----------------------------------
     -> You can use Cloudera "ImageCopy" (CDH) as a virtual machine.
	
	https://docs.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh_download.html

      -> Get any one CDH Quickstrat version.

      -> Download Oracle VirtualBox from https://www.virtualbox.org/wiki/Downloads
	 (click on Windows Hosts)
	
      -> Follow the steps mentoned in the document shared with you.


   HDFS File Operations
   --------------------
     
      Linux File System  (local file system)
	-> Home Directory: /home/cloudera

      HDFS File System   (distributed file system)
	-> Home Directory: /user/cloudera  (hdfs://localhost.localdomain:8020/user/cloudera)

      Two command formats:
	
	-> hadoop fs -<command>
	-> hdfs dfs -<command>


      Common File Operations on HDFS
      ------------------------------

	List the files & directories:   
		
		hadoop fs -ls
		hdfs dfs -ls

		hdfs dfs -ls datasets
		hadoop fs -ls -R datasets    -> recursive listing

	Help
		hadoop fs -help
		hadoop fs -help stat

	Remove a files/directories:

		hadoop fs -rm <file-name>
		hadoop fs -rm dir1/*.txt
		hadoop fs -rm -R practice1    // remove non-empty directory		

	Create a directory:

		hadoop fs -mkdir demodir

	Delete an empty directory:

		hadoop fs -rmdir demodir

	Copy from one directory to other

		hadoop fs -cp <source> <destination>
		hadoop fs -cp practice1/NYSE_*.txt practice2

	To print the content of a file on stdout

		hadoop fs -cat wordcount_input.txt

	Move from one directory to other

		hadoop fs -mv practice1/NYSE_*.txt practice2/dir1

		
	Changing permissions on an HDFS file

		hadoop fs -chmod 777 practice1
		hadoop fs -chmod -R 777 practice1    // recursive permission


       Copy/Move files between Local File System and HDFS
       ---------------------------------------------------
	
	Copy from Linux to HDFS

	    -> hadoop fs -copyFromLocal datasets/NYSE_dividends.txt practice1
	    -> hadoop fs -put datasets/NYSE_daily.txt practice1

	    -> hadoop fs -put datasets practice1

       Copy from HDFS to Linux

	   -> hadoop fs -copyToLocal <hdfs-file-path> <linux-directory>
	   -> hadoop fs -get  <hdfs-file-path> <linux-directory>

       Move from Linux to HDFS
	
	   -> hadoop fs -moveFromLocal <linux-file-path> <hdfs-directory>
	      hadoop fs -moveFromLocal test1/baseball.txt practice2

       
	

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                  Some important file locations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 Default Paths:

        * Cloudera Local Home :  /home/cloudera
        * HDFS Home: /user/cloudera

 Configuration Files:
	
	* /etc/hadoop/conf/core-site.xml
	* /etc/hadoop/conf/hdfs-site.xml
	* /etc/hadoop/conf/mapred-site.xml

 There are individual folders for each node in the cluster in directories as shown below :
  
        * /etc/hadoop/conf.cloudera.hdfs1
	* /etc/hadoop/conf.cloudera.mapred1
	* /etc/hadoop/conf.cloudera.yarn1

 All daemon log files are written under:

	* /var/log/hadoop/


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                  HDFS Commands
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        
 * hadoop fs -help   				=> lists all the commands with a brief discription
 * hadoop fs -help <command-name>  
 * hadoop fs -chmod -R 755 <file/dir>		=> change permission for a file/directory
 * hadoop fs -cat <file-name>
 * hadoop fs -ls
 * hadoop fs -ls <dir-path>
 * hadoop fs -ls -R  <dir-path>

 * hadoop fs -cp <source file> <target dir>     => Copies within HDFS frpm source to target
 * hadoop fs -mv <source file> <target dir>     => Moves within HDFS frpm source to target
 * hadoop fs -rm -r <file-name/directory-name>
 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 Moving files to/from local file system to/from hdfs 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 * hadoop -copyFromLocal <local file path>  <hdfs file path> 
 * hadoop -put <local file path>  <hdfs file path> 

 * hadoop -copyToLocal <hdfs file path> <local file path>   
 * hadoop -get <hdfs file path> <local file path>   

 * hadoop -moveFromLocal <local file path>  <hdfs file path> 
 * hadoop -moveToLocal <hdfs file path> <local file path> 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 Admin Commands
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 * sudo -u hdfs hdfs dfsadmin -report   => reports the usage stats of the cluster
 * sudo -u hdfs hdfs fsck / -files -blocks

Safe Mode:

* sudo -u hdfs hdfs dfsadmin -safemode leave
  sudo -u hdfs hdfs dfsadmin -safemode enter
  sudo -u hdfs hdfs dfsadmin -safemode get
  sudo -u hdfs hdfs dfsadmin -safemode wait

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 More Commands
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 * du - disk usage  
      
        ex:  hadoop fs -du /user/cloudera


 * du -s : Like -du, but prints a summary of disk usage of all files/directories in the path

        ex: hadoop fs -du -s /user/cloudera


 * df : Shows the capacity, free and used space of the filesystem
         -h  Formats the sizes of files in a human-readable fashion.

        ex: hadoop fs -df -h /user/cloudera/
            hadoop fs -df /user/cloudera/


 * getmerge <src> <localDest>    :  Retrieves all files that match the path src in HDFS, 
                                    and copies them to a single, merged file in the local 
                                    file system identified by localDest. 

        ex: hadoop fs -getmerge test/*.txt test1/merged.txt


 * Get the replication factor of a file:

        ex: hadoop fs -stat %r test/empinfo.txt


 * Set the replication factor of a file while copying or moving from linux

        ex: hadoop fs -D dfs.replication=5 -copyFromLocal <sourc e> <destination>
  
            hadoop fs -D dfs.replication=5 -copyFromLocal datasets/empinfo.txt test2
            hadoop fs -stat %r test2/empinfo.txt


 * Set the replication factor for an existing file:  -setrep [-R] [-w] <rep-factor> <path>
       
        ex: hadoop fs -setrep 2 test2/empinfo.txt   => sets the rep-factor of the file to 2
	    hadoop fs -stat %r test2/empinfo.txt    => test the rep-factor of the file


 * appendToFile

	Usage: hdfs dfs -appendToFile <localsrc> ... <dst>

	Append single src, or multiple srcs from local file system to the destination file system. 
	Also reads input from stdin and appends to destination file system.

	hdfs dfs -appendToFile localfile /user/hadoop/hadoopfile
	hdfs dfs -appendToFile localfile1 localfile2 /user/hadoop/hadoopfile
	hdfs dfs -appendToFile localfile hdfs://nn.example.com/hadoop/hadoopfile
	hdfs dfs -appendToFile - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin.


 * chmod, chgrp, chown  => Change the permissions of files.


====================================================
   YARN  -- (Yet another resource negotiator)
====================================================

   1. Resource Manager
	-> Master daemon
	-> One per cluster
	-> receives job submissions and schedules jobs
	-> Allocates containers for Application masters
	-> Dynamically manages the resource allocation to all the jobs

   2. Node Manager
	-> Slave daemon
	-> One per node
	-> Sends heart-beats to RM
	-> reports the status of all the jobs/tasks running on that node to RM
	-> Launches App.Master for a job

   3. Application Master
	-> One instance per application
	-> manages the tasks related to a job on the cluster
	-> requests for resources required for the job

   4. Container    
	-> Multiple containers are allocated for a single application
	-> The actual tasks of a job are executed in these containers
	-> These containers communicates with the application master.

	
  Documentation  -> https://hadoop.apache.org/docs/r3.3.1/

  Book -> https://www.isical.ac.in/~acmsc/WBDA2015/slides/hg/Oreilly.Hadoop.The.Definitive.Guide.3rd.Edition.Jan.2012.pdf

  ====================================================
      MapReduce
  ====================================================

   -> Hadoop's distributed processing framework. 
	
    MapReduce can be seen from two perspectives

    1. MapReduce Programming Paradigm	
	
		-> What kind of code that the developer has to write.	

		1. Driver Program
			-> All the configurations are defined here.

		2. Mapper Program
			-> Define the program logic to be implememted during the Map phase

		3. Reducer Program
			-> Optional
			-> It will reduce the various values of each unique key produced by the
			   mapper phase into aggregrated values with unique keys.
		
		4. Combiner Program
			-> Optional
			-> Will apply a min-reducer on the output of each mapper
			-> reduced the amount of data shuffled across the network there by increasing
			   the performance

		5. Partititoner Program
			-> Allows uds to define a custom partitioning logic to control which
			   data goes to which partition.
			-> Applied on the output of shuffle and sort phase.
	

	2. MapReduce Execution Model (Framework)	
		-> How the program is executed by the framework

		1. Mapper Phase
			-> Driver process lauches a set of mappers across the cluster
			   in the container allocated to it.

			-> All the mappers execute the same code.

			-> All the mapper write (k,v) pairs to context-buffer from which
			   a lot of local intermediate files are generated (across the cluster)

		2. Combiner Phase
			-> Optional phase
			-> Applied to the output of every map instance.

		3. Shuffle & Sort Phase
			-> Collect all the intermediate data and group the data by keys
			   (i.e it will produce unique keys with grouped values)
				(key, [val1, val2, val3, ....])

			-> Number of partitions are created with this shuffled data.
			   The number of partitions are equal to the number of reducers. 
			   (the default number of reducers = 1)

		4. Partitioning Phase
			-> Optional Phase
			-> A custom partitioner can be applied to decide which keys go to which
			   partition.

		5. Reducer Phase
			-> All these partitions are then processed by the reducer and the
			   output the reducer is written to HDFS in a specified directory.

			-> The number of output files is equal to the number of reducers.

      Submit an MR Program to the Cluster (YARN Resource Manager)
      -----------------------------------------------------------
	  $hadoop jar /home/cloudera/mrpractice.jar com.sample.WordCountJob wordcount_input.txt wcout
	  $yarn jar /home/cloudera/mrpractice.jar com.sample.WordCountJob wordcount_input.txt wcout	  


      Input Split
      -----------
	-> Always has whole number of records
		-> record in atext file is a line (ending with \n)

	-> InputSplits are created from the blocks where where InputSplit
	   starts at a new record and ends at a record boundary

	-> InputSplits are given as input data to the mappers.

	InputFormat  -> Identified the records and creates InputSpits
	InpuSplit    -> One intergral number of records that could span multple blocks
     	RecordReader -> Creates a (K, V) pair from each record of the inputsplit
	             -> RecordReader calles the map() method with each (K, V) pair

InputSplit						   RecordReader
=========== 						    =========== 
hadoop hive mapreduce hdfs hadoop mapreduce     	=>  (0, hadoop hive mapreduce hdfs hadoop mapreduce)        ---> map
mapper reducer sqoop flume hadoop mapreduce		=>  (40, mapper reducer sqoop flume hadoop mapreduce)	    ---> map	
mapreduce hive flume kafka hadoop hive			=>  (74, mapreduce hive flume kafka hadoop hive)            ---> map
hadoop hadoop mapreduce mapreduce hive hive
sqoop flume kafka mapper reducer partitioner combiner
driver hadoop mapreduce driver mapper combiner
reducer partitioner hadoop sqoop mapreduce


   public class WordCountMapper extends Mapper<LongWritable, Text, Text, LongWritable>

	LongWritable, Text   -> Key data type and Value datatype of input (K,V) pair
				(0, hadoop hive mapreduce hdfs hadoop mapreduce)

	Text, LongWritable   -> Key data type and Value datatype of output (K,V) pair
			        (hadoop, 1), (hive, 1), (mapreduce, 1), (hdfs, 1), (hadoop, 1), (mapreduce,1)


  Input-Split		RecordReader
  -----------          --------------
  hadoop hive mapreduce -> (0, hadoop hive mapreduce) --> map(....) --> (hadoop, 1) (hive, 1) (mapreduce, 1)
  mapper reducer sqoop	-> (23, mapper reducer sqoop)	            --> (mapper, 1) (reducer, 1) (sqoop, 1)									
  mapreduce hive flume
  hadoop hadoop
  sqoop flume kafka


    Creating a MapReduce Program from Scratch
    -----------------------------------------
	
	Two approaches: 

	1. Adding the required dependent jars from your local installation.

		-> Create a blank Java Project
		-> Add the following libraries in the build-path
		   
                    Right Click on Project -> Build Path -> Configure Build Path

		    Go to Libraries Tab -> Add External Jars
			-> /usr/lib/hadoop/hadoop-common.jar
			-> /user/lib/hadoop-0.20-mapreduce/hadoop-core.jar

		-> Start writing your MR code

	2. Using maven (package manager) you can add dependencies from maven-repo.

 
   Mapper Life Cycle Methods
   -------------------------
	
	1. setup
		-> executed once at the beginning of the mapper.
		-> perform initializations.

	2. map
		-> invoked once for every record, after setup() method
		-> runs as many times as the records in the input-split

	3. cleanup
		-> executed once after map() is completed.
		-> cleanup the resources (close the connections)

	4. run
		-> To gain complete control of execution-flow


   Reducer Life Cycle Methods
   ---------------------------
	
	1. setup
		-> executed once at the beginning of the reducer.
		-> perform initializations.

	2. reduce
		-> invoked once for every record, after setup() method.
		-> runs as many times as the records in the partition.

	3. cleanup
		-> executed once after reduce() is completed.
		-> cleanup the resources (close the connections)

	4. run
		-> To gain complete control of execution-flow


   Hadoop Writable Data Types
    ---------------------------

	-> Hadoop uses its own data types which are optimizes for serialization and deserialization.
	-> All Hadoop API classes work with Hadoop data types only.
	
	Java		Hadoop
	----		------
	String		Text
	Int		IntWritable
	Double		DoubleWritable
	Long		LongWritable
	Bool		BoolWritable
	Null		NullWritable

	
   Use-case
   --------
   From facebook.tsv dataset, fetch the record with highest "likes_received" from among the people
   who are born between 1990 and 2000.

    Two MR programs :

    1. fetch all the records with dob_year between 1990 and 2000  		    => age
    2. fetch the record with max value of likes_received from the output of step 1  => max-likes

  ==============================================
     Hive
  ==============================================

     -> Distributed Data Warehousing Infrastructure built on top of Hadoop.
     -> Provides an SQL interface for analysts to analyse big data on Hadoop.
     -> Hive uses MapReduce as its execution engine.
     -> Hive generates MR programs for the user queries and runs them on Hadoop cluster.

        NOTE: Hive is not an RDBMS database
	      Hive is not a NoSQL database


     Working with Hive
   
	1. Using CLI

		-> Hive Shell (connects to the local hive)
		-> Beeline Shell (connects to hive using JDBC drive - secure)

	2. Using HUE (Beeswax interface)
		-> provides WebUI to send queries to Hive

	3. Using Client Application
	

    Hive Warehouse
    --------------
	-> Directory where hive stores all data files of managed tables. 
	-> default path:  /user/hive/warehouse

    Hive Metastore
    --------------
	-> Is an external servive using which Hive manages its metadata (table schemas)
	-> In production deployments, we use MySQL/PostgreSQL as metastore 
	-> Hive comes with an embedded matastore called "Derby"

		
   NOTES
   -----
	-> Hive does not support primary-key and foreign-key
	-> Hive does not support update and delete ops on regular managed tables.

   Databases / Schemas
   -------------------
	show databases;
	show schemas;

	drop database <database-name>;  // drop an empty database
	
	create database ctsdb1;
	create database hivedb1 location '/user/cloudera/hivedb/hivedb1.db';

	describe database hivedb1;

   Describe
   --------

	describe <table-name>;
	desc <table-name>;

	describe extended <table-name>;
	describe formatted <table-name>;

   Table
   ------
	
	CREATE TABLE empinfo (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
	ROW FORMAT DELIMITED
	FIELDS TERMINATED BY ','
	LINES TERMINATED BY '\n' 
	STORED AS TEXTFILE;

	defaults:

	default storage format:  TEXTFILE
	default line terminator: "\n"
	default field terminator: Ctrl-A  (\0001)
	

   Loading Data
   -------------

   => local file system
   
        File is "copied" from the local file system to hive warehouse

	-- append
	LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/data/emp.txt' INTO TABLE empinfo;

	--overwrite
	LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/data/emp.txt' OVERWRITE INTO TABLE empinfo;

   => hdfs file system
   
        File is "moved" from the linux file system to hive warehouse

	-- append
	LOAD DATA INPATH '/user/cloudera/hive/data/emp.txt' INTO TABLE empinfo;

	--overwrite
	LOAD DATA INPATH '/user/cloudera/hive/data/emp.txt' OVERWRITE INTO TABLE empinfo;

   
   Managed Tables
   ---------------
	-> Tables that are managed by Hive (created in hive warehouse directory)
	-> When you drop the table both data files as well as schema are deleted.


   External Tables
   ---------------
	-> Tables are created in custom location specified in the create command
	-> When you drop the table, ONLY schema is dropped. The data files are NOT dropped.


   Partitioned Tables
   ------------------
	
	=> Allows you to organize your table data into multiple partitions (separate directories) based
	   on unique values of one or more partition columns.
	
       From the way data is written to partition tables, we can think of two types of partitioning:

	1. Static Partitioning
	    -> When you know into which partition you load data, then you can use static partitioning
	       approach.
	    -> To load data you may use LOAD DATA command or INSERT..SELECT command


	2. Dynamic Partitioning  

	    -> When your data file contains records that belong to multiple partitions and you can't
	       determine the exact partition into which you want to load that data.

	    -> In such scenarios you use dynamic partitioning approach to data loading.

		 -> create a base table and load all the data into that base table.
		 -> set dynamic partition mode to nonstrict
			-> set hive.exec.dynamic.partition.mode=nonstrict;
		 -> Load data from the base table into the partitioned table using 
		    INSERT..SELECT command.


CREATE TABLE students_static (id int, name string, mobile string, email string, project string)
PARTITIONED BY (country STRING)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/data/students_IN.txt' 
INTO TABLE students_static
PARTITION (country='IN');

INSERT INTO students_static
PARTITION (country = 'Germany')
SELECT id, name, mobile, email, project FROM students_static
WHERE country = 'UK';

--dynamic partitioning
-----------------------
CREATE TABLE students_dynamic (id int, name string, mobile string, email string, project string)
PARTITIONED BY (country STRING, batch INT)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;


CREATE TABLE students (id int, name string, mobile string, email string, project string, country STRING, batch INT)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/data/students_country_batch.csv' 
INTO TABLE students;

set hive.exec.dynamic.partition.mode=nonstrict;

INSERT INTO TABLE students_dynamic
PARTITION (country, batch)
SELECT id, name, mobile, email, project, country, batch FROM students;

INSERT INTO TABLE students_dynamic
PARTITION (country='Germany', batch=1)
SELECT id, name, mobile, email, project FROM students
WHERE country='Germany' and batch=1;

INSERT INTO TABLE students_dynamic
PARTITION (country='USA', batch)
SELECT id, name, mobile, email, project, batch FROM students
WHERE country='USA';

INSERT INTO TABLE students_dynamic
PARTITION (country, batch=1)
SELECT id, name, mobile, email, project, country FROM students
WHERE batch=1;

NOTE: Dynamic partition cannot be the parent of a static partition

CTAS command
--------------------------------------
-- using defaults
create table students_static_2
as 
select * from students_static;


-- using custom field separator.

create table students_static_3 
row format delimited 
fields terminated by ','
as 
select * from students_static;

-----------------------------------------------
   Bucketted Tables
-----------------------------------------------

CREATE TABLE students_buckets (id int, name string, phone string, email string, project string, country string, batch int)
CLUSTERED BY (id) INTO 4 BUCKETS
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n' 
STORED AS TEXTFILE;

NOTE: We can not use "LOAD DATA .." command to load data into a bucketted table. 
      We have to load data from a base table into a bucketted table using INSERT..SELECT

NOTE: Make sure bucketting is enforced before loading data into bucketted table
set hive.enforce.bucketing = true;

INSERT OVERWRITE TABLE students_buckets
SELECT * from students;

create table students_part_bk (id int, name string, phone string, email string, project string, country string)
partitioned by (batch int)
clustered by (id) sorted by (id ASC) into 3 buckets
row format delimited
fields terminated by '|'
lines terminated by '\n'
stored as textfile;

INSERT INTO TABLE students_part_bk
PARTITION (batch)
SELECT id, name, mobile, email, project, country, batch FROM students;
   

================================================
 SAMPLING Queries
================================================

  Three types of sampling can be performed

	=> bucket sample -> only on bucketted tables
	=> block sample  -> Fetch some blocks randomly
	=> row level sampling -> Fetch some number of rows from every block


  Bucket Sampling: Fetches minimum 1 bucket

     	=> SELECT * FROM students_part_bk TABLESAMPLE (BUCKET 2 OUT OF 3 ON id) 
  	=> SELECT * FROM students_buckets TABLESAMPLE (BUCKET 1 OUT OF 2 ON id) 

  Block Sampling:  Fetches minimum 1 block

  	=> SELECT * FROM students TABLESAMPLE (50 PERCENT)
	=> SELECT * FROM students TABLESAMPLE (5M)  // 4 blocks

  Row Sampling:  Fetches n rows from each input split

  	=> SELECT * FROM students TABLESAMPLE (5 ROWS)

 
================================================
 Managing outputs & Hadoop File Formats
================================================

  // append mode
  insert into empinfo2 select * from empinfo;

  // overwrite mode
  insert overwrite table empinfo2 select * from empinfo;

  // write to a local directory
  insert overwrite local directory '/home/cloudera/hive_data/empinfo' select * from empinfo;

  // write to a hdfs directory
  insert overwrite directory '/user/cloudera/hive_data/empinfo' select * from empinfo;

  // write to a local directory
  insert overwrite local directory '/home/cloudera/hive_data/empinfo_tab' 
  row format delimited 
  fields terminated by '\t'
  select * from empinfo;

  // write to a local directory in sequence file format
  insert overwrite local directory '/home/cloudera/hive_data/empinfo_seq' 
  stored as sequencefile
  select * from empinfo;

  // write to a local directory in avro file format
  insert overwrite local directory '/home/cloudera/hive_data/empinfo_avro' 
  stored as avro
  select * from empinfo;

  // write to a local directory in avro file format
  insert overwrite local directory '/home/cloudera/hive_data/empinfo_parquet' 
  stored as parquet
  select * from empinfo;

  // write to a local directory in avro file format
  insert overwrite local directory '/home/cloudera/hive_data/empinfo_orc' 
  stored as orc
  select * from empinfo;

  // write to a local directory in rcfile file format
  insert overwrite local directory '/home/cloudera/hive_data/empinfo_rc' 
  stored as rcfile
  select * from empinfo;
	
 ---------------------------------------  
   Creating tables in Hadoop File Formats
 ---------------------------------------

  CREATE TABLE empinfo_seq (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
  STORED AS SEQUENCEFILE;

  LOAD DATA LOCAL INPATH '/home/cloudera/hive_data/empinfo_seq/000000_0' INTO TABLE empinfo_seq;
 
  ----------------------------------
  Sorting data
  ----------------------------------

	select * from empinfo order by empname;

	select * from empinfo sort by empname;
	select * from empinfo distribute by deptid sort by empname asc;
	select * from empinfo cluster by deptid;


  ---------------------------------
   Joins
  ---------------------------------

   Supported Joins: inner, left-outer, right-outer, full-outer, left-semi
   => Only equi-joins are supported.



   Bucket Map Join Optimization
   ----------------------------

	-> Both tables that are joined must be bucketted on the join column(s)
	-> The number of buckets of one table must be a multiple of other table.
	-> Set the following config parameters:
		set hive.enforce.bucketing = true; 
		set hive.optimize.bucketmapjoin = true;


   Sort-Merge Join Optimization
   ----------------------------

	-> Both tables that are joined must be bucketted on the join column(s)
	-> The number of buckets of one table must be equals other table.
	-> Set the following config parameters:
		set hive.enforce.bucketing = true; 
		set hive.optimize.bucketmapjoin = true;

 CREATE TABLE IF NOT EXISTS stocks 
(stock_exchange STRING, symbol STRING, trxn_date DATE, open DOUBLE, 
high DOUBLE,low DOUBLE, close DOUBLE, volumn INT, adj_close DOUBLE)

ROW FORMAT DELIMITED

FIELDS TERMINATED BY '\t'

LINES TERMINATED BY '\n' 

STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '/home/cloudera/datasets/stocks' 
OVERWRITE INTO TABLE stocks; 

CREATE TABLE IF NOT EXISTS dividends 
(stock_exchange STRING, symbol STRING, trxn_date DATE, dividend DOUBLE)

ROW FORMAT DELIMITED

FIELDS TERMINATED BY '\t'

LINES TERMINATED BY '\n' 

STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '/home/cloudera/datasets/dividends' 
OVERWRITE INTO TABLE dividends; 


=================================================
   JOINS & STREAMING JOINS
=================================================

 ------------------------------------------------
 -- regular join
 ------------------------------------------------
 SELECT s.trxn_date, s.symbol, s.adj_close, d.dividend
 FROM stocks s JOIN dividends d
 ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol; 

 ------------------------------------------------
 -- regular join with better placement of tables
 ------------------------------------------------
 SELECT s.trxn_date, s.symbol, s.adj_close, d.dividend
 FROM dividends d JOIN stocks s
 ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol;

 ------------------------------------------------
 -- stream join hint (stream a table to reducer)
 ------------------------------------------------
 SELECT /*+ STREAMTABLE(s) */
 s.trxn_date, s.symbol, s.adj_close, d.dividend
 FROM stocks s JOIN dividends d
 ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol;
 

=================================================
  Map Joins
=================================================

 ------------------------------------------------
 -- map join hint
 ------------------------------------------------
 SELECT /*+ MAPJOIN(d) */
 s.trxn_date, s.symbol, s.adj_close, d.dividend
 FROM stocks s JOIN dividends d
 ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol;

 ------------------------------------------------
 --property to be set (default is true anyway)
 ------------------------------------------------
 set hive.auto.convert.join=true

 set hive.mapjoin.smalltable.filesize = 25000000

 SELECT s.trxn_date, s.symbol, s.adj_close, d.dividend
 FROM stocks s JOIN dividends d
 ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol; 

=================================================
  Bucket Map Joins
=================================================

CREATE TABLE IF NOT EXISTS stocks_bkt_8 
(stock_exchange STRING, symbol STRING, trxn_date DATE, open DOUBLE, 
high DOUBLE,low DOUBLE, close DOUBLE, volumn INT,adj_close DOUBLE)
CLUSTERED BY (trxn_date, symbol) INTO 8 BUCKETS 
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;

CREATE TABLE IF NOT EXISTS dividends_bkt_4 
(stock_exchange STRING, symbol STRING, trxn_date DATE, dividend DOUBLE)
CLUSTERED BY (trxn_date, symbol) INTO 4 BUCKETS 
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;

set hive.enforce.bucketing = true; 
set hive.optimize.bucketmapjoin = true;

INSERT INTO stocks_bkt_8 SELECT * FROM stocks SORT BY trxn_date, symbol;

INSERT INTO dividends_bkt_4 SELECT * FROM dividends SORT BY trxn_date, symbol;

SELECT /*+ MAPJOIN(dividends_bkt_4) */ s.trxn_date, s.symbol, s.adj_close, d.dividend
FROM stocks_bkt_8 s JOIN dividends_bkt_4 d
ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol;

=================================================
  Sort Merge Joins
=================================================

CREATE TABLE IF NOT EXISTS stocks_bkt_4
(stock_exchange STRING, symbol STRING, trxn_date DATE, open DOUBLE, 
high DOUBLE,low DOUBLE, close DOUBLE, volumn INT,adj_close DOUBLE)
CLUSTERED BY (trxn_date, symbol) INTO 4 BUCKETS 
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;

INSERT INTO stocks_bkt_4 SELECT * FROM stocks SORT BY trxn_date, symbol;

set hive.auto.convert.sortmerge.join=true;
set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.auto.convert.sortmerge.join.nonconditionaltask=true;

SELECT /*+ MAPJOIN(dividends_bkt_4) */ s.trxn_date, s.symbol, s.adj_close, d.dividend
FROM stocks_bkt_4 s JOIN dividends_bkt_4 d
ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol;

===================================================

  Multi-Table Insert
  ------------------
    
 from students_static  
   insert into students_usa 
   select * where country = 'USA'
   insert into students_uk 
   select * where country = 'UK'
   insert into students_india
   select * where country = 'IN';
	
===================================================
  Complex Data Types
===================================================
   
CREATE TABLE complex_data
(col1 array<string>,
 col2 map<int,string>,
 col3 struct<name:string,score:int>)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
COLLECTION ITEMS TERMINATED BY '/'
MAP KEYS TERMINATED BY ':'
LINES TERMINATED BY '\n'


LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/hive_complex_data' INTO TABLE complex_data;


sample data
-----------
Apple/Grapes/Orange,4306:Sneha/4307:Renu,Sneha/90
Apple/Microsoft/Google,4306:Renu/4307:Anita,Raju/100
Apple/Micromax/Samsung,4306:Raju/4307:Harsha,Harsha/80


===================================================
  immutable tables & transactional tables
===================================================

Using Table properties

CREATE TABLE emp_tp_metadata 
(empid INT, empname STRING, salary DOUBLE, deptid SMALLINT) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ',' 
LINES TERMINATED BY '\n' 
STORED AS TEXTFILE 
TBLPROPERTIES ('creator'='raju', 'created_at'='2020-03-01 10:00:00')

 Immutable tables
 -----------------

As of Hive 0.13.0, a table can be made immutable by creating it with 
TBLPROPERTIES ("immutable"="true"). 

The default is "immutable"="false".

INSERT INTO behavior into an immutable table is disallowed if any data is already present, 
although INSERT INTO still works if the immutable table is empty. 

The behavior of INSERT OVERWRITE is not affected by the "immutable" table property.

An immutable table is protected against accidental updates due to a script loading data 
into it being run multiple times by mistake. The first insert into an immutable table 
succeeds and successive inserts fail, resulting in only one set of data in the table, 
instead of silently succeeding with multiple copies of the data in the table. 


#========================================
#       Transactional tables  
#========================================

CREATE TABLE table_name (
  id                int,
  name              string
)
CLUSTERED BY (id) INTO 2 BUCKETS STORED AS ORC
TBLPROPERTIES ("transactional"="true",
  "compactor.mapreduce.map.memory.mb"="2048",
  "compactorthreshold.hive.compactor.delta.num.threshold"="4", 
  "compactorthreshold.hive.compactor.delta.pct.threshold"="0.5" 
);
=====
Set the following properties in hive-site.xml file. 
---------------------------------------------------
  <property>
    <name>hive.support.concurrency</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.enforce.bucketing</name>
    <value>true</value>
  </property>
  <property>
   <name>hive.exec.dynamic.partition.mode</name>
    <value>nonstrict</value>
  </property>
  <property>
    <name>hive.txn.manager</name>
    <value>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager</value>
  </property>
  <property>
    <name>hive.compactor.initiator.on</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.compactor.worker.threads</name>
    <value>1</value>
  </property>
=====
configurations to be set in hive CLI session
--------------------------------------------

set hive.support.concurrency = true;
set hive.enforce.bucketing = true;
set hive.exec.dynamic.partition.mode = nonstrict;
set hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
set hive.compactor.initiator.on = true;
set hive.compactor.worker.threads = 1;

CREATE TABLE user_acid (userid int, name string, age int, gender string) 
CLUSTERED BY (userid) INTO 3 buckets 
STORED AS ORC
TBLPROPERTIES('transactional'='true');

INSERT INTO user_acid(userid, name, age, gender) VALUES 
(1, 'Raju', 40, 'male'), 
(2, 'Raghu', 42, 'male'),
(3, 'Vinay', 20, 'male'), 
(4, 'Amrita', 14, 'female'), 
(5, 'Aditya', 14, 'male'), 
(6, 'Keertana', 12, 'female'), 
(7, 'Pranav', 24, 'male'), 
(8, 'Hari', 24, 'male'), 
(9, 'Syamala', 23, 'female')

SELECT * from user_acid;

UPDATE user_acid SET age = 41 where name='Raju';   	 ---> bck - 1
DELETE FROM user_acid WHERE userid = 9;	

 
  Temporary tables, explode, lateral views
  ----------------------------------------

-----------------------
lateral views
------------------------

create table tab_lateral (id int, phones array<string>, emails array<string>)
row format delimited 
fields terminated by '\t'
collection items terminated by ','
stored as textfile;

load data local inpath '/home/cloudera/hive/Hive/data/lateral_view_data.txt' overwrite into table tab_lateral;

# note: expode can not be merged with any other columns as is.
# use lateral views if you want to do so.

select explode(phones) from tab_lateral;

select id, phones_l from tab_lateral 
lateral view explode(phones) p as phones_l;

-- this is not the way we wanted the data...
select id, phones_l, emails_l from tab_lateral 
lateral view explode(phones) p as phones_l
lateral view explode(emails) e as emails_l;


select row_number() over(), id, phones_l from tab_lateral 
lateral view explode(phones) p as phones_l;


--- solution ----

create temporary table if not exists tab_lateral_tmp_phones
(rowid int, id int, phone string)
row format delimited
fields terminated by '|';
insert overwrite table tab_lateral_tmp_phones
select row_number() over(), id, phones_l from tab_lateral 
lateral view explode(phones) p as phones_l;

create temporary table if not exists tab_lateral_tmp_emails
(rowid int, id int, email string)
row format delimited
fields terminated by '|';

insert overwrite table tab_lateral_tmp_emails
select row_number() over(), id, emails_l from tab_lateral 
lateral view explode(emails) p as emails_l;
create table if not exists tab_lateral_breakdown
(rowid int, id int, phone string, email string)
row format delimited
fields terminated by '|';

insert overwrite table tab_lateral_breakdown
select a.rowid, a.id, a.phone, b.email
from   tab_lateral_tmp_phones a, tab_lateral_tmp_emails b
where  a.rowid = b.rowid;

==================================
  Window Functions
==================================

stock_exchange      	string              	                    
symbol              	string              	                    
trxn_date           	date                	                    
open                	double              	                    
high                	double              	                    
low                 	double              	                    
close               	double              	                    
volume              	int                 	                    
adj_close           	double 

--- lag ----
SELECT symbol, trxn_date, close, LAG(close,1) OVER(PARTITION BY symbol) as yesterday_price 
FROM stocks LIMIT 10;

SELECT Ticker, TrxnDate, Close, LAG(Close,2) OVER(ORDER BY TrxnDate) as yesterday_price 
FROM stocks LIMIT 10;

--- lead ----
SELECT symbol, trxn_date, close, LEAD(close,1) OVER(PARTITION BY symbol) as nextday_price 
FROM stocks LIMIT 10;


SELECT symbol, trxn_date, close, LEAD(close,1) OVER(PARTITION BY symbol) as nextday_price 
FROM stocks LIMIT 10;

SELECT symbol, trxn_date, close, 
CASE(LEAD(close,1) OVER(PARTITION BY symbol) - close) > 0 
WHEN TRUE THEN "higher" WHEN FALSE THEN "lesser" END as changes 
FROM stocks LIMIT 10

--- first_value ------
select symbol, high, first_value(high) over(partition by symbol) as first_high 
from stocks limit 10;

-- sum & avg ----
select symbol, close, sum(close) over(partition by symbol order by close ) as total from stocks limit 40;
select symbol, close, sum(close) over(partition by symbol order by close rows between 2 preceding and 2 following)) as total from stocks limit 40;
select symbol, close, (sum(close) over(partition by symbol order by close rows between 4 preceding and current row))/5 as 5_day_moving_avg from stocks limit 40;
select symbol, close, avg(close) over(partition by symbol order by close rows between 4 preceding and current row) as 5_day_moving_avg from stocks limit 40;

--- rank & dense_rank -----
select symbol, close, rank() over(partition by symbol order by close) as rank from stocks limit 100;
select symbol, close, dense_rank() over(partition by symbol order by close) as rank from stocks limit 100;

------------------------------------
  How to run a script file in Hive
------------------------------------
 NOTE: Run this from command prompt (not from within the shell)

  hive -f '/home/cloudera/hive/Hive/churn_modelling.hql'

----------------------------------------------
 Running Hive commands in interactive mode
----------------------------------------------
 NOTE: Run this from command prompt (not from within the shell)
 
  hive -e 'select * from ctsdb1.empinfo'

 =========================================
    Impala
 =========================================

   -> A sql query engine built using Massivly parallel processing (MPP) query engine
   -> For processing huge amount of data (PB scala) stored in Hadoop clusters (HDFS)
   -> Uses the same metastore as well as the warehouse as that of Hive
   -> Uses the same query language as that of Hive (Hive QL)
   -> Written in C++ & Java
   -> High performance and Low Latency.
   -> Supports in-memory data processing.  
   -> Developed by Cloudera
	-> Shipped with all major Hadoop managed cloud service providers.


    "invalidate metadata" command
     -----------------------------
	-> To refresh any metatdata changes happened outside impala, you need to run "invalidate metadata"
           command to refresh the local metadata of impala.

   When to use impala ?
   --------------------

	-> Impala is not fault-tolerant because of in-memory computation
	   -> If any of the servers/nodes running impala queries fail, the job will be aborted 
	-> Hence impala is not suitable for long-running jobs. 
	-> Use impala for short queries (random access of data etc.)

        => Impala is good for real time quries
	   Hive is good for large batch-jobs

     NOTE: 
	=> invalidate metadata
		-> To refresh any metatdata changes happened outside impala, you need to 
		   run "invalidate metadata" command to refresh the local metadata of impala.

	=> refresh <tablename>

		-> Whenever table data changes outside of impala, you need to use 
		   "refresh <tablename>" to get the latest data.

	-> Impala does not support "LOAD DATA LOCAL INPATH .." command


 =========================================
    Sqoop
 =========================================

   -> Structured data ingestion tool.

   -> Used to import data from RDBMS databases to Hadoop (HDFS/Hive)
      Used to export data from Hadoop (HDFS/Hive) to external RDBMS databases.

   -> Sqoop launches "map-only" jobs to import data parallellly from an RDBMS into HDFS.


   Sqoop Commands
   ---------------
  codegen            Generate code to interact with database records
  create-hive-table  Import a table definition into Hive
  eval               Evaluate a SQL statement and display the results
  export             Export an HDFS directory to a database table
  help               List available commands
  import             Import a table from a database to HDFS
  import-all-tables  Import tables from a database to HDFS
  job                Work with saved jobs
  list-databases     List available databases on a server
  list-tables        List available tables in a database
  merge              Merge results of incremental imports
  metastore          Run a standalone Sqoop metastore
  version            Display version information


  Sqoop Commands
  --------------
   # List available databases on the mysql server
   sqoop list-databases --connect jdbc:mysql://localhost:3306 --username root -P

   # List available tables in a database on the mysql server
   sqoop list-tables --connect jdbc:mysql://localhost:3306/ctsdb --username root -P

   #Import a table from a database to HDFS

    sqoop import --connect jdbc:mysql://localhost:3306/ctsdb 
		 --username root 
		 -P
		 --table users 
		 -m 1 
		 --target-dir /user/cloudera/sqoop/users

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users -m 1 --target-dir /user/cloudera/sqoop/users

	=> default field terminator: comma (",")
	=> default line terminator: \n

	Import Modes:

	1. delete mode (overwrite mode)
		--delete-target-dir

	2. append mode 
		--append


sqoop list-databases --connect jdbc:mysql://localhost:3306 --username root -P

sqoop list-tables --connect jdbc:mysql://localhost:3306/ctsdb --username root -P


======= IMPORT =======

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users -m 1 --target-dir /user/cloudera/sqoop/users

	=> default field terminator: comma (",")
	=> default line terminator: \n

   Import Modes:

	1. delete mode (overwrite mode)
		--delete-target-dir

	2. append mode 
		--append

-- import data in delete mode --
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users -m 1 --target-dir /user/cloudera/sqoop/users --delete-target-dir

-- import data in append mode --
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users -m 1 --target-dir /user/cloudera/sqoop/users --append

-- import data of specified columns --
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users --columns 'id,name,age' -m 1 --target-dir /user/cloudera/sqoop/users2

-- import data with query --
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query "select * from users where id > 1 AND \$CONDITIONS" -m 1 --target-dir /user/cloudera/sqoop/users3

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query "select country, count(1) from students where 1=1 AND \$CONDITIONS group by country" -m 1 --target-dir /user/cloudera/sqoop/students_group --delete-target-dir

-- import data with custom field terminator --
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users -m 1 --target-dir /user/cloudera/sqoop/users4 --fields-terminated-by '|' 

-- import data with enclosing char --
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root --table users -m 1 --target-dir /user/cloudera/sqoop/users5 --enclosed-by '"'

-- import data avro/sequence file formats --
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root --table users -m 1 --target-dir /user/cloudera/sqoop/users_avro --as-avrodatafile

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root --table users -m 1 --target-dir /user/cloudera/sqoop/users_seq --as-sequencefile

-- import data using multiple mappers --
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root --table students -m 3 --target-dir /user/cloudera/sqoop/students1 --split-by 'id'

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root --table students -m 3 --target-dir /user/cloudera/sqoop/students2 --split-by 'country'


======= EXPORT =======


sqoop export --connect jdbc:mysql://localhost:3306/demo       

             --table emp2 
	     --username root 
	     --password <password>                            			
	     -m 1                                       			

             --export-dir /user/cloudera/sqoop/demo    		

             --input-fields-terminated-by ',' 

sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop/users/ --input-fields-terminated-by ',' 

sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop/users2/part-m-00000 --input-fields-terminated-by ',' --columns "id,name,age"

sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table students2 --username root -P -m 1 --export-dir /user/cloudera/sqoop/students1/part-m-* --input-fields-terminated-by ',' 

sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop/users_avro/ 

-- does not work - sequence file can not be exported
sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop/users_seq/ 

sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir  /user/cloudera/sqoop/users3/part-m-00000 --update-key 'id'

-- does not work, as the table require a primary-key
sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir  /user/cloudera/sqoop/users3/part-m-00000 --update-key 'id' --update-mode allowinsert

--this one works.
sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table userspk --username root -P -m 1 --export-dir  /user/cloudera/sqoop/users3/part-m-00000 --update-key 'id' --update-mode allowinsert


------ import entire database ------

sqoop import-all-tables --connect jdbc:mysql://localhost:3306/ctsdb --username root -P -m 1 --warehouse-dir /user/cloudera/sqoop/ctsdb_db 


------- importing to hive ----------
sqoop create-hive-table --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users --fields-terminated-by ',' --hive-table sqoopdb.users

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users --fields-terminated-by ',' -m 1 --hive-table sqoopdb.users --hive-import 


sqoop create-hive-table --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students --fields-terminated-by '\t' --hive-table sqoopdb.students

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students --fields-terminated-by '\t' -m 2 --split-by 'id' --hive-table sqoopdb.students --hive-import 


-------------- eval -------------------
sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'select * from students limit 5'

sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'create table t1(c1 int, c2 varchar(30))'

sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'insert into t1(c1, c2) values(1, "raju"), (2, "ramesh")'

sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'update t1 set c2="kanakaraju" where c1=1'

sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'delete from t1 where c1=1'

sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'drop table t1'

-------- codegen ----------
sqoop codegen --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students --class-name Students


 ==============================
       HBase
 ==============================

   -> HBase is a NoSQL / NOn-relational database
   -> Provides a transactional data processing facility built on top of HDFS

   -> HBase is created based on "Google Big Table" architecture.


   What are short-comings of RDBMS databases
   -----------------------------------------
	=> Does not provide unlimited scalability (because they are not distributed databases)
	=> Does not support schema evolution.

   What are NoSQL databases
   ------------------------
	-> Are created as distributed databases that run on a cluster.
	   -> Hence offer unlimited scalability
	-> Does not follow SQL like regid schemas. They offer schema evolutions by following 
	   non-relational model.
	-> They use denormalized tables so that joins can be completly avoided.


   Types of NoSQL databases
   ------------------------
	1. Wide Column or Column Family orietented databases  (ex: Hbase, Cassandra )
	2. Document oriented databases (ex: MongoDB, CouchDB)
	3. Key-Value Stores (ex: Redis, DynamoDB )
	4. Graph Database (ex: Neo4J)


   HBase Data Model
   ----------------

	-> A table definition contains only table name and column families.
	-> The data belonging to each column-family is physically stored in one place.
	-> each column belongs to one column family.
	-> A row can have as many columns as needed.
	   -> Each row is identified by a row-id.
	-> HBase manitains multiple versions of each cell
	   -> The number of versions to keep is configured at column-family level

   HBase and RDBMS
   ---------------
	-> HBase does not suppot SQL
	-> HBase only supports a basic set of operations (CRUD) on a single-row
		-> All operations are applied at row-level
		-> No JOINs, No Grouping, No Sub-Queries
	-> Denormalized
	-> Uses column-oriented storage
		-> HBase table more like a multi-map than an RDBMS table
	-> Limited ACID supports
		-> ACID complaint at row-level only.

	-> Lot of common DB operations are not supported in HBASE
		-> No JOINS
		-> No GroupBy, No aggregation.
		-> No OrderBy
		-> No Sub-Queries

HBase Operations
================================
  
create 'emp', 'contact', 'basic', 'skills', 'salary' 

create 't1', {NAME => 'f1', VERSIONS => 5}, {NAME => 'f2', VERSIONS => 2}

-------------------------------------------------

describe 'emp'

-------------------------------------------------

put 'emp', '1', 'basic:empid', '100'
put 'emp', '1', 'basic:name', 'Raju'
put 'emp', '1', 'basic:age', '45'
put 'emp', '1', 'contact:phone', '9246657890'
put 'emp', '1', 'contact:email', 'raju@gmail.com'
put 'emp', '1', 'skills:rdbms', 'mysql,oracle'
put 'emp', '1', 'salary:basic', '40000'

put 'emp', '2', 'basic:empid', '101'
put 'emp', '2', 'basic:name', 'Ramesh'
put 'emp', '2', 'basic:dob', '2000-01-01'
put 'emp', '2', 'contact:city', 'Hyderabad'
put 'emp', '2', 'contact:email', 'ramesh@gmail.com'
put 'emp', '2', 'skills:nosql', 'hbase'
put 'emp', '2', 'salary:basic', '40000'
put 'emp', '2', 'salary:hra', '10000'
put 'emp', '2', 'salary:travel', '5000'

put 'emp', '10', 'basic:empid', '102'
put 'emp', '10', 'basic:fname', 'Mahesh!!!!'
put 'emp', '10', 'basic:dob', '2000-02-01!!!!'
put 'emp', '10', 'contact:city', 'Hyderabad!!!!'
put 'emp', '10', 'contact:state', 'TS!!!!'
put 'emp', '10', 'contact:email', 'ramesh@gmail.com!!!!'
put 'emp', '10', 'skills:nosql', 'hbase!!!!'
put 'emp', '10', 'skills:rdbms', 'oracle!!!!'
put 'emp', '10', 'salary:basic', '40000!!!!'
put 'emp', '10', 'salary:travel', '5000!!!!'
put 'emp', '10', 'salary:hra', '10000!!!!'


put 't1', '1', 'f1:name', 'raju-v6'
put 't1', '1', 'f2:age', '45-v6'

-------------------------------------------------

scan 'emp'    // show all the data

scan 'emp', {COLUMNS=>['basic:empid', 'basic:name']}

scan 'emp', {COLUMNS => ['basic:empid', 'basic:name', 'salary:basic'], TIMERANGE => [1632133000299, 1632133824220]}

-------------------------------------------------

get 'emp', '10', {COLUMN => ['basic:empid', 'basic:fname', 'basic:dob', 'contact:city'], VERSIONS => 3}

get 't1', '1', {COLUMN => ['f1:name', 'f2:age'], VERSIONS => 5}

-------------------------------------------------

--update: is essentially same as insert but with latest timestamp
put 'emp', '1', 'basic:name', 'Kanakaraju'

---------------------------------------------------

delete 'emp', '10', 'skills:rdbms'

---------------------------------------------------

truncate 'emp'

---------------------------------------------------

Note: to alter, you first need to disable the table

alter 't1', NAME => 'f1', VERSIONS => 4        // alter or add a column-family
alter 't1', NAME => 'f3', METHOD => 'delete'   // delete a column-family
alter 't1', METHOD => 'table_att_unset', NAME => 'MAX_FILESIZE'   // remove a table-scope attribute

---------------------------------------------------
 
   HBase & Hive Integration
   ------------------------

# run this from linux while launching hive shell - this adds the required jar files to the hive path for that session
$ hive --auxpath /usr/lib/hive/lib/hive-hbase-handler-0.10.0-cdh4.7.0.jar,/usr/lib/hive/lib/hbase.jar,/usr/lib/hive/lib/zookeeper.jar,/usr/lib/hive/lib/guava-11.0.2.jar


CREATE TABLE hiveempinfo(key int, empname string, salary double) 
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' 
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,emp:empname,emp:salary") 
TBLPROPERTIES ("hbase.table.name" = "hbaseempinfo");

-------------------------------------------------------

  Running a HBase Script file
  ---------------------------
	
    $ hbase shell <script-file-path>
    $ hbase shell /home/cloudera/hbase/orders_script

	
 ======================================
    Apache KAFKA
 ======================================
   
  -> Opensource distributed event streaming platform
 
     Event Streaming
     ---------------
	-> processing data in "real-time"

	-> Capturing data from event sources like DBs, Sensors, Mobile devices, Cloud Service and
           software applications in the form of streams of events. 

        -> Event streams enable continous flow and interpretation of data. 

        Some Use Cases:
	
	-> Processing payments and transactions in real-time 
	   (stock exchanges, banks, insurance etc..)
	-> Track an montitor cars, trucks in real time (logistics & automotive)
	-> Capture and analyze sensor data from IoT devices (such as in factories, hospitals)
	-> To collect and immediatly react to customer orders (retails, hotels, travels, uber/ola)


    Messaging Systems
    -----------------
	-> A messaging system is a system that is used to transfer data from one application to 
           another, so that application can focus on data and not on how to share it. 

	1. Point-to-point (Queues) -> only one consumer per event
	2. Publisher/Subscriber (pubsub) -> events are not intended for any consumer. 

   
    What is Kafka ?
    ---------------

	Is a distributed streaming platform that is used for:

	1. Publish & subscripe streams of events
	2. Store streams of events in a fault-tolerant way
	3. Process streams of events as they occus (real-time processing APIs)
		
   
   Kafka Building Blocks
   ---------------------

	1. Broker 
		-> Are the nodes on which Kafka service is running
		-> Responsible for storing published data.
		-> Each broker is stateless, so they use "Zookeeper" for maintaining state.
		-> each broker may have zero or more partitions per topic.

		NOTE: Broker-ID must be unique in a cluster

	2. Zookeeper
		-> Used for maintaining and coordinating Kafka brokers
		-> ZK is used to notify producers and consumers about the presence of any
		   new broker in the cluster.

	3. Topics & Logs
		-> Is a feed/category to which records are published by producers and are
                   consumed by consumers.
		-> For every topic Kafka maintains partition-logs (distributed commit log)

	4. Partitions
		-> A topic is organized as partitions allowing kafka to handke arbitrary amount 
		   of data
		-> each partition is an "ordered commit log"

	5. Partition-offset
		-> Each message in a topic-partition has a unique sequence id called "offset".

	6. Replicas
		-> Backups of the partition.
		-> Used to prevent data loss
		-> Only one replica is designated as the "Leader replica"
			-> All writes & reads are served ONLY by leader replicas. 
			-> When the broker which contains a leader-replicas goes down, one the existing
			   follower replicas will be elected as the leader. 

	7. Producers
		-> External application that write messages to various partitions (leader replicas) 
		   of a topic
		-> A single producer can write to any number of topics.

	8. Consumers
		-> Application that subscribe to one or more topics, poll the messages and process
		   the messages.
			

   Getting started with Kafka
   --------------------------

     1. Make sure you are running Java 8 (JDK 1.8.x). If not, make sure you upgrade to Java 8.
		https://www.liquidweb.com/kb/install-java-8-on-centos-7/

		NOTE: To check the current version of Java, run the following command:
		$ java -version

     2. Download Kafka from  https://kafka.apache.org/downloads
	  -> Download the binary ( such as  kafka_2.12-2.6.1.tgz )
	  -> Extract it to a suitable location. 

	Important Directories:
	---------------------
	 -> <kafka>/bin    : You have all the tools/services here
	 -> <kafka>/config : All configuration files are located here   
	 -> <kafka>/libs   : All Kafka API libraries (jar file) are location	


  Let start working with Kafka
  ----------------------------

	1. Start the zookeeper service.
		-> Zookeeper runs on port 2181 (by default)
	
		To check if zookeeper is running or not, use the following command:
		$netstat -nlp | grep 2181

		Starting zookeeper service:
		$<kafka>/bin/zookeeper-server-start.sh config/zookeeper.properties

	2. Start Kafka broker.
	
		-> Kafka broker, by default, runs on port 9092

		$<kafka>/bin/kafka-server-start.sh config/server.properties

		Important Confog Properties:

		1. broker.id=0
		2. listeners=PLAINTEXT://:9092
		3. log.dirs=/tmp/kafka-logs
		4. zookeeper.connect=localhost:2181
		5. log.retention.hours=168

	3. Working with Topics
	   
		1. List the existing topics

			bin/kafka-topics.sh --bootstrap-server localhost:9092 --list

		2, Describe a topic

			bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic ctstopic1

		3. Delete a topic
		
			bin/kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic ctstopic1

		4. Create a topic

			bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic demotopic1 --partitions 3 --replication-factor 1

	4. Launch a Console Producer

		bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic demotopic1
		
		bin/kafka-console-producer.sh --bootstrap-server localhost:9092 
					      --topic demotopic1
					      --property "parse.key=true" 
					      --property "key.separator=:"
	
	5. Launch a Console Consumer

		bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic demotopic1
		
		bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 
					      --topic demotopic1 
					      --property print.key=true 
					      --property print.value=true 
                                              --property key.separator=" | "


		bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 
					      --topic demotopic1
					      --partition 0
					      --offset 0

		bin/kafka-console-consumer.sh --topic demotopic1
					--bootstrap-server localhost:9092 
					--group g1


	6. Consumer Groups

		bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list



   What a message can have?
   ------------------------
	-> topic (required)
	-> value (required)  
	-> key (optional, but always used)
	-> partition-id (optional)
	-> timestamp (optional)
	

   Message Distribution Logic in Kafka
   ------------------------------------

	1. If the message has a partition-id specified, then the message is sent to that partition. 

	2. If the message has no partition-id specified, then

		2.1 If the message has a key specified, then the hashcode of the key is mapped
		    to a specific partition (using hash partitioning logic)

		2.2 If the message does not have any key, then, the messages are distributed randomly
		    across partitions in aload balanced manner.


    Single Node, Multiple Broker Setup
    -----------------------------------
      
     -> Create three different copies of server.properties file and modify the following setting:

				broker.id	listeners			log.dirs			     
	
	server.properties	0		---				/tmp/kafka-logs		
	server1.properties      1		listeners=PLAINTEXT://:9093	/tmp/kafka-logs-1
	server2.properties      2		listeners=PLAINTEXT://:9094	/tmp/kafka-logs-2


   Consumers - Standalone Consumer & Consumer Groups  
   --------------------------------------------------

    Standalone consumer is a consumer that consumes messages from all the partitions of a topic
    or from a one (or more) partitions

    NOTE: Having a lot of partitions for a topics allows us to launch multiple instances of
	  standalone consumers, each consuming from 1 partition, there by allowing us to
	  increase consumer side scalability. 
	
	
   Consumer Groups
   ---------------
	-> One or more consumer instances subscribing to the same topic using the same group-id form
	   a 'consumer group'
	-> The partitions of the topics are distributed to the consumers in the consumer group.
    
        Partition Rebalance
	-------------------
	-> Changing the ownership of partitions among the existing consumers in the group is 
	   called 'rebalance'
	-> Rebalance happens automatically when a new consumer joins a group or an existing
           consumer leaves a group.
	

   Kafka APIs
   ----------
 	-> Kafka APIs allows us to write client applications that interact with Kafka
	
	1. Producer API   -> to write producer application
	2. Consumer API	  -> to write consumer application
	3. Streams API	  -> to write streaming data processing application that read data from topics,
			     transform/filter/enrich the data and write to some other topics.
	4. Connect API	  -> to write cusom source connectors and sink connectors
			     source conncetor -> apllication that writes to a Kafka topic
			     sink connector -> reads from a kafka topic and writes to a custom application.

   Producer API
   ------------

    -> Writing a simple Producer Application using Kafka Producer API
	
	1. Define the properties object where you define all the desired properties. 
	
		-> bootstrap.servers 
		-> key.serializer
		-> value.serializer		

		 Properties kafkaProps = new Properties();
	      	 kafkaProps.put("bootstrap.servers", "localhost:9092,localhost:9093,localhost:9094");
	      	 kafkaProps.put("key.serializer","org.apache.kafka.common.serialization.StringSerializer");         
	      	 kafkaProps.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

	2. Define the "KafkaProducer" object
		-> defines a connection to the cluster

		Producer<String, String> producer = new KafkaProducer<>(kafkaProps);

	3. Define you messages using "ProducerRecord" objects
		ProducerRecord<String, String> record = new ProducerRecord<>(topicName, key, value); 


	4. Send the ProducerRecord using "KafkaProducer.send( ProducerRecord )" method.


       Sample Program:


	    public static void main(String[] args) throws Exception {
        	   
	      String topicName = "demotopic3p3r";
		  
	      Properties kafkaProps = new Properties();
	      kafkaProps.put("bootstrap.servers", "localhost:9092,localhost:9093,localhost:9094");
	      kafkaProps.put("key.serializer","org.apache.kafka.common.serialization.StringSerializer");         
	      kafkaProps.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
	            
	      Producer<String, String> producer = new KafkaProducer<>(kafkaProps);				  		   
		  
	      System.out.println("Starting KafkaProducerFF ...");	      
	      	      
	      String key = "";
	      String value = "";
	      
		  try { 
			  for (int i = 1; i <= 30; i++) {	
				  key = "Key " + i;
				  value = "Java Msg " + i;
				  
				  ProducerRecord<String, String> record = new ProducerRecord<>(topicName, key, value); 
				  
				  producer.send(record);   
				  
				  Thread.sleep(200);
				  System.out.println("sent message - " + i);		  
			  }
		  }
		  catch (Exception e) { 
			  e.printStackTrace();			  
		  } 
		  
		  producer.close();
		  
		  System.out.println("--- done ---- ");
	 }


    ProducerRecord
    --------------
	-> Represents Kafka message 

	ProducerRecord(String topic, V value)
	ProducerRecord(String topic, K key, V value)
	ProducerRecord(String topic, Integer partition, K key, V value)
	ProducerRecord(String topic, Integer partition, Long timestamp, K key, V value)


   Ways to Send Messages to Kafka
   ------------------------------

    NOTE: Producer.send() -> returns Future<RecordMetaData>

	1. Fire and Forget
		-> You send the messages continuously, but do not wait for the acks.

		Advantages:
		-> very fast
	
		Disadvantage:
		-> You don't know of the record fails. 
		-> record delivery order is not guaranteed

		Use-case
		-> Non-critical applications such as click-stream analysis

	2. Synchronous Messaging
		-> We block the Future using get() method and waut until the RecordMetaData is returned.
			RecordMetadata metaData = producer.send(record).get();

		Advantages
		-> We know about what happened to the record
		-> record delivery order is guaranteed

		Disadvantage:
		-> Slow (because it has to wait for the ack)

		Use-Cases:
		-> Critical applications such as credir-card transactions.	

	3. Asynchrounous Messaging
		-> We attach a callback method to the send request
		-> The callback method is invoked when the Future eventually returns.

		Advantages:
		-> very fast
		-> We know about what happened to the record
	
		Disadvantage:
		-> record delivery order is not guaranteed

		Use-Cases:
		-> Critical applications where message order may not be important. 


   Producer Configurations
   ------------------------
	1.   bootstrap.servers  (required)
	2.   key.serializer  	(required)
	3.   value.serializer  	(required)

	4.   acks   (acknowledgements)
	      -> The number of acknowledgements the producer requires the leader to have
		 received before considering a request as complete.
		
		acks -> 0, 1 (default), all (-1)

		acks: 0    -> no acks at all
		acks: 1    -> only leader acks
		acks: all  -> all ISRs must acknowledge.   (Safe Producer)
				-> ONLY this can prevent message loss

	5. buffer.memory	(def: 32MB in bytes)
	6. batch.size		(def: 16KB in bytes)
	7. compression.type	(default: none) -> none, gzip, snappy, lz4, zstd
	8. retries		(default: no limit)
				-> controlled by "delivery.timeout.ms" (2 min - 120000)
	9.  delivery.timeout.ms	(default: 120000)
	10. max.block.ms	(default: 60000)
	11. linger.ms		(default: 5)
	12. partitioner.class	(custom partition)

	
     Consumer API
     ------------

	1. Define the properties object.	

		Properties props = new Properties();
		props.put("bootstrap.servers", "localhost:9092,localhost:9093,localhost:9094");		
		props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
		props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");		
		props.put("group.id", "demo");

	2. Define the KafkaConsumer object

		Consumer<String, String> consumer = new KafkaConsumer<>(props);

	3. Subscribe to one or more topics.
		consumer.subscribe(Arrays.asList(topicName));

	4. Create a poll loop   (infinite loop)  

	5. Within the loop, send a poll request.
            -> The poll returns a 'ConsumerRecords' object, which is a collection of 'ConsumerRecord' objects
            -> Loop through the 'ConsumerRecords' collection and process the messages.

		
    Kafka Delivery Semantics for Consumers
    --------------------------------------
   
      1. At most once
	
	 -> The offsets are committed as soon as the messages are received. 
	 -> If the processing goes wrong, the messages will be lost and will not be read again.

      2. At least once  (default)

	-> Offsets are committed after the messages are processed.
	-> If the processing goes wrong, the messages that were already processed may also be
	   reprocessed again.

      3. Exactly once
	
	-> Is not supported in Kafka -> Sink workloads. (consumer API)


    AutoCommit Vs Manual Commit
    ---------------------------

	When "enable.auto.commit" is set to "true" (default is "true"), Kafka automatically
	commits the offset and then poll the next batch of messages. This follow "atleast once"
	semantics approach. 

	If you set "enable.auto.commit" to "false", we have to programmatically commit the offsets
	using consumer.commitSync() or consumer.commitAsync() methods.

  
    Important Consumer Configurations
    ---------------------------------
	-> bootstrap.server (required)
	-> key.deserializer
	-> value.deserializer
	-> group.id (if you are launching in a consumer groups)

	
	1. enable.auto.commit   (default: true)
		-> Kafka automatically takes care of committing the offset.

	2. fetch.min.bytes (def: 1)
		-> The minimum amount of data to poll in each request.

	3. max.poll.records (def: 500)
		-> max. how many message to receive per poll request.
		-> increase this, if the messages are small and if have lot of RAM

	4. max.partitions.fech.bytes (def: 1 MB in bytes)
		-> max data returned by the broker per partition
		-> if you read from 100 partitions, you need 100MB RAM for the client app.

	5. fetch.max.bytes (def: 50 MB)
		-> max data returned for each fetch request (covers all partitions)

	6. heartbeat.intervel.ms (def: 3000)
		-> specific to consumer groups (group managemet facility)
		-> time interval at which heartbeats are expected to be sent to coordinator broker.

	7. session.timeout.ms (def: 10000)
		-> Timeout used to detect client failures. 

	8. auto.offset.reset
		-> latest (default), earliest, none

		-> what should happen when the current-offset is not set or is invalid.

		
    Streams API
    -----------

	1. Define the properties.

		Properties props = new Properties();
             	props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        	props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());
        	props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());

	2. Create a StreamBuilder object.

		 StreamsBuilder builder = new StreamsBuilder();

	3. Define the topology on the streambuilder (entire functionality)

		
        	KStream<String, String> source = builder.stream(
        	    "wordcount-input", 
        	    Consumed.with( Serdes.String(), Serdes.String() )
        	);

        	KTable<String, Long> counts = source
            		.flatMapValues(value -> Arrays.asList(value.split(" ")))
            		.groupBy((key, value) -> value)
            		.count();
        
        	// need to override value serde to Long type
        	counts.toStream().to("wordcount-output", Produced.with(Serdes.String(), Serdes.Long()));


	4. Define the KafkaStreams object using the builder and propertier. 

		final KafkaStreams streams = new KafkaStreams(builder.build(), props);

	5. start the KStreams..
		
		streams.start()

 =========================================
     NiFi - Workflow Scheduler
 =========================================
     
     => NiFi ---> Niagara Files

     => Uses a Web UI and runs on a WebServer (Jetty)

     => Apache NiFi supports powerful and scalable directed graphs of data-routing, transformations,
	and can define mediation logic.

     => Automate the flow of data between systems:
				
	=> JSON  -> Databases
	=> FTP   -> Hadoop
	=> Kafka -> ElasticSearch 
	
     => Drag & Drop Interface

     => Focus on Configuration of Processors
	
     => Scalabale across a cluster of machine
	   -> NiFi cluster are small clusters (typically 10 to 15 nodes)
	   -> Each machine is very high config with LOTs & LOTs of disk space.

     => Manage Data Buffering & Back Pressure management etc

    NiFi is good at:	
     	=> Reliable and secure transfer of data between systems
	=> Delivery of data from various sources and analytics platforms
	=> For enrichment & preparation data.

    NiFi is not good at
	=> Distributed Computing
	=> Complex Data Procesing
		-> Joins, rolling windows, aggregations etc.


   Getting Started with NiFi
   -------------------------

    Installing NiFi is very straight forward
	
	-> Download NiFi bianaries as a tar or zip file
	   URL: https://nifi.apache.org/download.html

	-> Extract the downloaded archive into a suitable folder

    => Let us assume that NiFi is installed in a directory called "nifi-1.12.1" 

  
   Starting the NiFi
   -----------------

   1. cd nifi-1.12.1

   2. For Windows Users
	-> For Windows users, navigate to the folder where NiFi was installed. 
        -> Within this folder is a subfolder named "bin". 
        -> Navigate to this and double-click the "run-nifi.bat" file.
	-> This will launch NiFi and leave it running in the foreground.
        -> To shut down NiFi, select the window that was launched and Ctrl+C

   3. For Linux/Mac OS X users
	-> $ bin/nifi.sh run     (to start NiFi app in the foreground)
        -> $ bin/nifi.sh status  (to get the status)

   4. Installing NiFi as a service
	-> This is supported only for Linux / Mac

	-> $ bin/nifi.sh install    
             -> This will install with the default service name 'nifi'

        -> $ bin/nifi.sh install dataflow
             -> This will install with the custom service name 'dataflow'

        -> Once installed, the service can be started and stopped as below:
	     -> $ sudo service nifi start    (or sudo service dataflow start)
		$ sudo service nifi stop
		$ sudo service nifi status


   Opening NiFi Web UI
   -------------------

     => Goto:   http://localhost:8080/nifi

     => The port can be changed by editing the nifi.properties file in the NiFi conf directory
     => The default port is 8080.


     Basic Concepts
     --------------

	1. Flow File
		-> It is basically the data
		-> Comprises of
			1. Content -> the data itself
			2. Attributes -> (K, V) pairs assciated with the data.
		-> Persisted on the disk.

	2. Processor 
		-> There are ~288 processors in version 1.12
		-> Applies a set of transformations and rules to flow-files to generate
	           new flow files.
		-> Any processor can process any flow file.
		-> Processors are passing flowfile references to each other to 
		   advance the data processing.
		-> They are all running in parallel on different threads.

	3. Connector
		-> Its basically a queue of all the flow files that are yet to be processed
		   by the downstream processor.
		-> Defines rules about how flow files are proritized
		-> Can define backpressure to avoid overflow.

	4. Process Groups
		-> A container ofr defining a group of processors.

	5. Controller Services
		-> Different services provides by NiFi to help with data manipulations
		-> The controlers can be define at Global scope (NiFi) or at process group 
		   scope
			

   Use-Cases-1  ==> CSV files to MySQL  
   ------------------------------------      

       -> GetFile (Source Data - CSV file)
	  -> SplitText (splits single FF into many FFs)
            -> ConvertRecord (Convert CSV into JSON FF)               
	       -> ConvertJSONtoSQL (Convert JSON to SQL statement)
                   -> PutSQL (to run SQL against MySQL)

     Configurations
     --------------

     Processor 1: GetFile
		-> Input Directory: source directory where you put CSV files

	GetFile  -> success -> SplitText

     Processor 2: SplitText
		-> Line Split Count: 1
		   Header Line Count: 1

	SplitText -> splits -> ConvertRecord

     Processor 3: ConvertRecord
		-> RecordReader: CSVReader (Controller Service)
		-> RecordWriter: JSONRecordSetWriter (Controller Service)

	CSVReader Properties
	-> Schema Access Strategy: Use String Fields From Header
	   Treat First Line As Header: true

	JSONRecordSetWriter Properties
	-> No changes
	
	ConvertRecord -> success -> ConvertJSONToSQL
	  
     Processor 4: ConvertJSONToSQL	  
	-> JDBCConnectionPool: DBCPConnectionPool (Controller Service)
	   Statement Type: INSERT
	   Table Name: <MySQL Table Name>
	   Catalog Name: <MYSQL Database Name>

	DBCPConnectionPool properties   
	-> Database Connection URL: jdbc:mysql://localhost:3306/<mysql db name>
	   Database Driver Class Name: com.mysql.jdbc.Driver
           Database Driver Locations: <directory path where you placed the mysql driver>
		    (ex: /home/cloudera/nifi-1.11.4/lib/mysql-connector-java-5.1.44.jar)
	   Database User: <mysql user name>
	   Database password: <mysql user password>

       ConvertJSONToSQL -> sql -> PutSQL 

     Processor 5: PutSQL  
	-> JDBCConnectionPool: DBCPConnectionPool (Controller Service)


   Use-Case-2
   ----------
     ==> Kafka - NiFi Integration
 
       -> GenerateFlowFile (Generates random Flow Files at the desired)
          -> PublishKafka (Publishes Flow Files to a Kafka Topic)

       -> ConsumeKafka (Subscribes to Kafka topic and created flow-file)
          -> LogAttribute (LOg the flow files) 
 

     Configurations
     --------------

     Processor 1: GenerateFlowFile
     -> File Size: 100B
	Batch Size: 5
	Unique Flow Files: true

	GenerateFlowFile -> success -> PublishKafka_0_10 1.11.4

     Processor 2: PublishKafka_0_10 1.11.4
     -> Kafka Brokers: localhost:9092
	Topic Name: <kafka topic name>   ex: nifi-topic


    NOTE: No connection between PublishKafka_0_10 1.11.4 & ConsumeKafka_0_10 1.11.4


    Processor 3: ConsumeKafka_0_10 1.11.4
     -> Kafka Brokers: localhost:9092
	Topic Name: <kafka topic name>   ex: nifi-topic
	
	ConsumeKafka_0_10 1.11.4 -> success -> LogAttribute

    Processor 4: LogAttribute
    -> No changes


   NiFi Repositories
   -----------------

   1. Flow Repository
	-> Stores the current state (content & attributes) of each flow file.

   2. Content Repository
	-> Contains all the content present in all flow files of NiFi
        -> This directory uses large disk space

   3. Provenance Repository   
	-> Tracks and stores all the events of all the flowfiles in NiFi.

   Templates
   ---------

 =======================================================
      Spark  (PySpark)
 =======================================================
     
   Spark is a unified in-memory distributed processing framework. 

	-> Spark is written in Scala.
	-> Spark applications can be written in Scala, Java, Python and R
	-> Spark can run on multiple cluster managers:
		-> Spark Standalone, YARN, Mesos, Kubernetes.

        in-memory => the intermediate outputs of various distributed tasks can 
		     be persisted in-memory and subsequent tasks can directly operate on them. 	


   Spark Unified Framework
   -----------------------

	-> Spark provides a set of consitent APIs running on the same execution engine to process
	   different analytical workloads.

	
	  Hadoop Ecosystem (excluding Spark)
          ----------------------------------
	   -> Batch Analytics of unstructured data	: MapReduce
	   -> Batch Analytics of structured data	: Hive, Impala, Drill, HBase
	   -> Streaming Analytics			: Storm, Kafka, Samza, Flink
	   -> Predictive Analytics (Machine Learning)	: Mahout
	   -> Graph Parallel Computations		: Giraph

	  Spark Unified Framework
          ----------------------------------
	   -> Batch Analytics of unstructured data	: Spark Core
	   -> Batch Analytics of structured data	: Spark SQL
	   -> Streaming Analytics			: Spark Streaming, Structured Streaming
	   -> Predictive Analytics (Machine Learning)	: Spark MLlib
	   -> Graph Parallel Computations		: Spark GraphX


   Spark Architecture
   ------------------

    1. Cluster Manager (CM)
	-> Jobs are submitted to CM
	-> Spark can submit jobs to multiple cluster managers:
		-> Spark Standalone, YARN, Mesos, Kubernetes.
	-> CM allocates executors for spark application

    2. Driver
	-> Master Process (one process per application)
	-> Creates a "SparkContext" object
	-> Analyses user code and sends tasks to be executed on the cluster

	Deploy Modes
	1. Client Mode   -> The driver runs on the client machine
	2. Cluster Mode	 -> The driver runs on one of the nodes in the cluster

    3. Executors
	-> Several executors (resource containers with RAM and CPU cores) are allocated by the CM
	-> Tasks sent by driver will be executed here.
	-> Each task does the same process but on different partitions.
	-> Tasks report the status back to the driver.

    4. SparkContext	
	-> Starting point of execuion and runs within driver.
	-> Represents an application context
	-> Is a link between the driver and various tasks running on the cluster.


    RDD (Resilient distributed dataset)
    -----------------------------------

	-> Fundamental data abstract of Spark Core API

	-> RDD represents a logical entity which contains a logical plan and set of partitions.

		RDD's metadata : RDD Lineage (Logical Plan)
		RDD's data: a collection of distributed in-memory partitions.

	-> RDD partitions are immutable

	-> RDDs are lazily evaluated
		-> Transformations does not cause execution
		-> Action commands triggers execution.

  
    What can we do with an RDD ?
    -----------------------------
	Only two things:

	1. Transformations 
		-> Transformation applies some function and produces another RDD
		-> Output of transformation is an RDD.
		-> Transformations does not cause execution. 
		-> Transformations only create Lineage DAG (logical plan)

	2. Actions
		-> Action command produces some output
		-> Trigger execution.
		-> Cause the driver program to create a physical execution plan and send the
		   tasks to the cluster to be executor.
 

    How to create RDDs
    ------------------
	Three ways to create:

	1. Create an RDD from external data files such as text files.
		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt")
		-> The default number of partitions is given by the value of sc.defaultMinPartitions
		-> The value of sc.defaultMinPartitions = 2, if you have atleast two cores.

	2. Create an RDD from programmatic data	
		rdd1 = sc.parallelize( [1,2,3,4,5,6,3,4,5,6,7,8,9,0,10], 2 )

		rdd1 = sc.parallelize( [1,2,3,4,5,6,3,4,5,6,7,8,9,0,10])
		-> The default number of partitions is given by the value of sc.defaultParallelism
		-> The value of sc.defaultParallelism = # of cores allocated to the app.
		
	3. Create an RDD by applying transformations on existing RDDs
		rdd2 = rdd1.map(lambda x: x.upper())


   Understanding RDD Lineage
   -------------------------
    Lineage DAG (directed acyclic graph) refres to a graph of dependencies all the way from the first RDD which
    caused creation of this RDD.
		
	rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	   => Lineage:  rdd1 -> sc.textFile

	rdd2 = rdd1.map(lambda x: x.upper())
	    => Lineage:  rdd2 -> rdd1.map -> sc.textFile

	rdd3 = rdd2.filter(lambda x: len(x) > 50)
	   => 	Lineage:  rdd3 -> rdd2.filter -> rdd1.map -> sc.textFile

	rdd3.collect()
	
	rdd3 -> rdd2.filter -> rdd1.map -> sc.textFile

	sc.textFile (rdd1) -> map (rdd2) -> filter (rdd3) --> collect() - send the array to driver


   Getting started with PySpark
   -----------------------------

      1. Install the Spark locally (on Windows or Linux) and run PySpark shell.
 	  ==> download url: https://spark.apache.org/downloads.html		
		for example, my installation dir is:  E:\spark-3.0.0-bin-hadoop2.7

	  ==> Setup environment variables:
		SPARK_HOME  : E:\spark-3.0.0-bin-hadoop2.7
		HADOOP_HOME : E:\spark-3.0.0-bin-hadoop2.7
		Add <spark>\bin  (E:\spark-3.0.0-bin-hadoop2.7\bin) to PATH variable.

     2. Installing an IDE

	-> PyCharm IDE (from Jetbrains)

	-> Spyder (which comes with Anaconda Navigator)
		-> Install Anaconda -> https://www.anaconda.com/products/individual#windows
	-> Follow the instructions from the document shared with you (after installing Anaconda)
	   to setup PySpark with Spyder & Jupyter Notebook.

    3. Signup to Databricks Community edition  ( Free Cloud Account )
	-> https://databricks.com/try-databricks
		
	-> Signup to the free account
	-> Login
	-> Read "Quickstart Tutorial" & "Import and Explore Data" 
   

    Types of Transformations
    ------------------------

	1. Narrow Transformations   
		-> No shuffling of the data
		-> partition to partition transformations
		-> Output RDD will always have same number of partitions as input RDD
		-> ex: map, filter, glom, flatmap

	2. Wide Transformations
		-> Shuffling of data will be there
		-> The data to compute an output partition can come from multiple input partitions
		-> Output RDD can have different number of partitions than input RDD
		-> distinct
   

    RDD Persistence
    ---------------
        rdd1 = sc.textFile( ... , 4)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd2.t4(...)
	rdd5 = rdd2.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.MEMORY_ONLY )  --> instruction to spark to not remove the rdd6 partitions. 
	rdd7 = rdd6.t7(...)

	rdd6.collect()

	Lineage of rdd6 => rdd6 -> rdd5.t6 -> rdd2.t5 -> rdd1.t2 -> sc.textFile
	sc.textFile (rdd1) -> t2 (rdd2) -> t5 (rdd5) -> t6 (rdd6) --> collect()

	rdd7.collect()

	Lineage of rdd7 -> rdd6.t7 -> rdd5.t6 -> rdd2.t5 -> rdd1.t2 -> sc.textFile
	rdd6 -> t7 (rdd7) -> collect()

	=> Spark, by default, garbage collects (remove) the unreference objects. Hence the 
	   partitions that go go out of reference, are subjected to GC, and hence, deleted.

	=> When you need the same partitions, for multiple job, you can instruct Spark to
	   persist them either in-memory or on-disk, so that you avoid recomputation of these
	   partition, if you need them later.


	Persistence    --->   in-memory deserialized
			      in-memory serialized
			      disk
	  
      Storage Levels
      --------------

	1. MEMORY_ONLY		-> deseriliazed in-memory    (deserialized: CPU efficient, takes more Memory)

	2. MEMORY_AND_DISK	-> deseriliazed - store in-memory if available
						  store on disk if memory is not available.
	3. DISK_ONLY

	4. MEMORY_ONLY_SER	-> seriliazed in-memory   (deserialized: requires more CPU, takes less Memory) 
	
	5. MEMORY_AND_DISK_SER

	6. MEMORY_ONLY_2	-> 2x replication 

	7. MEMORY_AND_DISK_2


     Persistence Commands
     --------------------
	
	 rdd.persist()     -> Memory Serialized 1x Replicated 
	 rdd.persist(StorageLevel.MEMORY_AND_DISK)
	 rdd.cache()	   -> Memory Serialized 1x Replicated

	 rdd.unpersist()	
 

    RDD Transformations
    -------------------

     1. map			P: U -> V
				Element to Element Transformation
				input RDD: N elements, output RDD: N elements

     2. filter			P: U -> Boolean
				The elements that return True as function output will be in the 
				output RDD
				input RDD: N elements, output RDD: <= N elements

     3. glom			P: None
				-> Creates one array object per partition with all the elements of
				   the partition.
		
		rdd1			rdd2 = rdd1.glom()
		P0: 2,1,3,4,5,6 -> glom -> P0: [2,1,3,4,5,6]
		P1: 4,2,1,5,6,8 -> glom -> P1: [4,2,1,5,6,8]
		P2: 5,2,4,3,7,9 -> glom -> P2: [5,2,4,3,7,9]
		
		rdd1.count = 18		   rdd2.count = 3

     4. flatMap			P: U -> Iterable[V]   (iterable is something you can loop through)
				Flattens the iterables produced by the function.
				input RDD: N elements, output RDD: > N elements
					
		rddWords.flatMap(lambda x: x.upper()).collect()
		rddWords.flatMap(lambda x: x.split(" ")).collect()

		rddWords.flatMap(lambda x: len(x)) => Invalid function as Int is not iterable.

    5. distinct			P: None, Optional: Number of partitions
				Return unique/distinct elements of the RDD.

    6. mapPartitions		P: Iterator[U] -> Iterator[V]

		rdd1			rdd2 = rdd1.mapPartitions(lambda x: [sum(x)] )							

		P0: 2,1,3,4,5,6 -> mapPartitions -> P0: 21
		P1: 4,2,1,5,6,8 -> mapPartitions -> P1: 26
		P2: 5,2,4,3,7,9 -> mapPartitions -> P2: 30

    7. mapPartitionsWithIndex	P: Int, Iterator[U] -> Iterator[V] 

		rdd1.mapPartitionsWithIndex(lambda id, data : [(id, sum(data))] ).collect()
	
    8. sortBy			P: U -> V, Optional: Number of partitions
				The objects of the RDD are sorted based on the function output
				produced by the objects.	

		rdd1.sortBy(lambda x: x % 5).glom().collect()		// ASC order
		rdd1.sortBy(lambda x: x % 5, False).glom().collect()    // DESC order
		rdd1.sortBy(lambda x: x % 5, True, 5).glom().collect()  // ASC sort with 5 partitions.
   

   Types of RDDs from usage perspective
   ------------------------------------

	1. Generic RDD 	-> RDD[U]    	-> RDD of any type
	2. Pair RDD	-> RDD[(U, V)]  -> RDD of (K, V) pairs


    9. mapValues		P: U -> V
				-> Applied only on Pair RDDs
				-> Will transform the "value" part of the (K, V) pair by applying the 
				   function. 
		
    10. groupBy			P: U -> V, Optional: Number of partitions
				-> The output RDD will have a pair RDD ((K, V) pairs) where 
				   each 'Key' is unique value of the function output and 'value'
				   is an iterable (ResultIterable object) of the RDD objects that 	
				   produces the same key. 

				   RDD[U].groupBy(U -> V) => RDD[(V, ResultIterable[U])]

		rdd1 = sc.textFile(inputFile) \
        		.flatMap(lambda x: x.split(" ")) \
        		.groupBy(lambda x: x) \
        		.mapValues(len) \
        		.sortBy(lambda x: x[1], False, 1)


    11. randomSplit		P: ratios to split the RDD
				-> Returns an array of RDD randomly split in the specified ratios	
	
		rddArr = rdd1.randomSplit([0.6, 0.4])       -> split into 60%, 40% RDDs
		rddArr = rdd1.randomSplit([0.6, 0.4], 35)   -> here 35 is a seed

    12. repartition		P: # of partitions
				Increase or decrease the number of partitions of the output RDD.
				Causes global shuffle.

		rdd2 = rdd1.repartition[n]    -> where n is the number output partitions (of rdd2)

    13. coalesce		P: # of partitions
				Used only to decrease the number of partitions
				Cause partition-merging.

		rdd2 = rdd1.coalesce[n]  -> where n is the number output partitions (of rdd2)
					    n should be snmaller than the partition count of rdd1.


    14. union, intersection, subtract, cartesian

	  Let us say rdd1 has M partitions and rdd2 has N partitions...

	  command				number of output partitions
          -----------------------------------------------------------------
	   rdd1.union(rdd2)			M + N, narrow
	   rdd1.intersection(rdd2)		M + N, wide
	   rdd1.subtract(rdd2)			M + N, wide
	   rdd1.cartesian(rdd2)			M * N, wide	

    ..ByKey Transformation

	-> Applied only to Pair RDD
	-> Are all wide transformations.

    15. sortByKey		P: None, optionally: Sorting Order, # number of partitions
				Elements of the RDD are sorted based on the key

		rdd3.sortByKey().glom().collect()  		// ASC sort
		rdd3.sortByKey(False).glom().collect()		// DESC sort
		rdd3.sortByKey(True, 4).glom().collect()	// ASC sort with 4 partitions.

    16. groupByKey		P: None, Optional: # number of partitions
				Elements of the RDD are grouped based on the key
				-> results in global shuffle and hense inefficient
				-> avoid it if possible		

		rdd1 = sc.textFile(inputFile) \
        		.flatMap(lambda x: x.split(" ")) \
        		.map(lambda x: (x, 1)) \
        		.groupByKey() \
        		.mapValues(sum)

    17. reduceByKey		P: Reduces all the values of each unique-key by iterativly applying
				   a reduce function (first within each partition and then on reduced 
				   values of all partitions) by using a reduce function.

				   -> You get unique key and reduced values. 

			rdd1 = sc.textFile(inputFile) \
        			.flatMap(lambda x: x.split(" ")) \
        			.map(lambda x: (x, 1)) \
        			.reduceByKey(lambda x, y: x + y)

    18. aggregateByKey	      P: All the values of each unique-key are folded with zero-value within
				each partition using a "sequence-function" and the outputs produced 
				in each partition are reduced using a "combine function"

			     -> Output will have one value per each unique ket of the type of "zero-value"


student_rdd = sc.parallelize([
  ("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  ("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  ("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  ("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  ("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  ("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  ("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 

def sequence_fun(z, v):
    return (z[0] + v, z[1] + 1)

def combine_fun(a, b):
    return (a[0] + b[0], a[1] + b[1])
  
rdd_avg = student_rdd.map(lambda x: (x[0], x[2])) \
            .aggregateByKey( (0,0), sequence_fun, combine_fun, 2 ) \
            .mapValues(lambda x: x[0]/x[1]) \
            .sortByKey()	


	19. joins     => join, leftOuterJoin, rightOuterJoin, fullOuterJoin


		names1 => [('vijay', 1), ('aditya', 1), ('raju', 1), ('amrita', 1)]
		names2 => [('amrita', 2), ('raju', 2), ('pavan', 2), ('pranav', 2)]

		join = names1.join(names2)   #inner Join
	   	=> [('raju', (1, 2)), ('amrita', (1, 2))]

		leftOuterJoin = names1.leftOuterJoin(names2)
	   	=> [('raju', (1, 2)), ('vijay', (1, None)), ('amrita', (1, 2)), ('aditya', (1, None))]

		rightOuterJoin = names1.rightOuterJoin(names2)
	   	=> [('raju', (1, 2)), ('pavan', (None, 2)), ('pranav', (None, 2)), ('amrita', (1, 2))]

		fullOuterJoin = names1.fullOuterJoin(names2)
	   	=> [('raju', (1, 2)), ('pavan', (None, 2)), ('pranav', (None, 2)), ('vijay', (1, None)), ('amrita', (1, 2)), ('aditya', (1, None))]


       20. cogroup       => used to join RDDs that have duplicate keys
			     -> groupByKey on each RDD (so you get unique-keys)
			     -> fullOuterJoin on grouped RDDs

		rdd1 ->  [('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
	            => (key1, [10, 7]) (key2, [12,6]) (key3, [6])

		rdd2 ->  [('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
		   => (key1, [5, 17]) (key2, [4,7]) (key4, [17])

		rdd1.cogroup(rdd2)  
	 	   => (key1, ([10, 7], [5, 17])) (key2, ([12,6], [4,7])) (key3, ([6], [])) (key4, ([], [17]))

  RDD Actions
  ------------

    	1. collect

	2. count

	3. saveAsTextFile

	4. reduce		-> Reduces the entire RDD into one final value of the same type as the 
				   RDD by iterativly applying a reduce-function, first within each partition
				   and then on reduced values of all partitions.

		
		P0: 6, 7, 8, 8    		--> -17
		P1: 8, 8, 9, 10			--> -19
		P2: 11, 12, 13, 14, 15, 16	--> -59

		rdd1.reduce(lambda x, y: x - y)  --> 61	

		
	5. aggregate		-> Reduce the entire RDD into one value of any type defined by the "zero-value"
				-> All the values of each partition are folded with a zero-value to get one 
				   output per partition of the type of zero-value using a "sequence function"
				-> The outputs produced in each partition are further reduced using a 
				   "combine function". 
	
			P0 : [10, 20, 30, 40, 10]
                        P1 : [20, 40, 50, 40, 60]

			rdd1.aggregate( (0,0),    				  # zero-value
					lambda z, v: (z[0] + v, z[1] + 1),        # seq-function
					lambda a, b: (a[0] + b[0], a[1] + b[1])   # combine-function
				      )

	6. take

		rdd1.take(n) => gives first n elements of the RDD

	7. takeOrdered

		rdd1.takeOrdered(5)   			// first 5 sorted elements
		rdd1.takeOrdered(7, lambda x: x > 40) 	// first 7 sorted elements using the function

	8. takeSample





 To Download a file from databricks:
 -----------------------------------

 /FileStore/<FILEPATH>
 https://community.cloud.databricks.com/files/<FILEPATH>?o=4949609693130439#tables/new/dbfs

 Example:
 /FileStore/tables/wordcount-5.txt
 https://community.cloud.databricks.com/files/tables/wordcount-5.txt?o=4949609693130439#tables/new/dbfs























 

