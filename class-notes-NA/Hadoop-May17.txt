
 Curriculum (40 sessions - 80 hours)
 -------------------------------------
  Introduction to Big Data & Hadoop
  HDFS
  YARN
  MapReduce (using Java)
  Hive 
  Impala - Basics
  Sqoop  
  HBase - Basic Operations
  Kafka (using Java)      
  Spark (PySpark)  


 Training Materials
 ------------------
    -> PDF Presentations
    -> Code modules, command documents, programs
    -> Class Notes
    -> Databricks Notebooks
    -> GitHub: https://github.com/ykanakaraju/Hadoop-Basic


 Pre-requisites
 --------------
	-> SQL
	-> Java
	-> Python

 ------------------------------------------------------------------ 
  
    Big Data
   --------
    
   - Type of data which large and complex that your traditional data processing systems
     can not reasonably store and process. 

   Big Data scenarios
   ------------------
       -> The volume to be handled is very high (much higher than 1 TB)
       -> You want to support real-time analytics (velocity)
       -> You data is growing very fast, so that in no time your system becomes inadeqaute. 
       -> Process unstructures, semi-structured data etc.
    
  Big Data Characteristics
  ------------------------
       -> Volume
       -> Velocity
       -> Variety 
       -> Veracity (correctness of the data)
       -> Value (extracting insights from the data)       

  Why your traditional systems have big data problem
  --------------------------------------------------
   -> Because your traditional systems ( that exists in pre-internet era ) are not designed
      for internet scale applications.
 
   -> They are designed to run on a single machine. The single machine has only limited 
      amount of resources.


  Scalability Options
  -------------------
    -> Scale up  (Vertical Scaling)   -> Migrate to a machine with higher configuration
    -> Scala out (Horizontal Scaling) -> Use many machines instead of one. 
                                      -> Use computing cluster

   What is a Computing Cluster?
   ----------------------------
   A unified network of nodes (machines) whose cumulative resources can be used to store
   or process data as if you working with a single machine. 

   
   What is a Framework ?
   ---------------------
    A framework is a set of libraries that implement all the basic low-level functionalities
    that the framework is intended for. The users of the framework should focus on the 
    business-logic and implement their solutions as per the guidelines provided by the 
    framework. 


    What is the solution to storage & processing problems associated with Big Data
    ------------------------------------------------------------------------------
    -> Hadoop provides a solution for big-data problems.

 
    What is Hadoop ?
    ----------------     
    Hadoop is a Open Source Framework to Store and Process data in a distributed way 
    using a cluster of commdity harware. 

     -> Provides two things:
	   -> Distributed Storage Solution (HDFS)
	   -> Distributed Processing Solution (MapReduce)
        And also,
           -> Resource Management (YARN)


    Hadoop Distributions
    --------------------
	Apache Hadoop => Open source	
	On-Prem & Hybrid Cloud => Cloudera & MapR
	Managed Cloud Solutions 
		-> AWS   : AWS Elastic MapReduce (EMR)
		-> Azure : HDInsights
	        -> GCP   : DataProc 


    HDFS (Hadoop Distributed File System)
    --------------------------------------

      -> Stores data in the form of "blocks" of 128 MB each
      -> Each block is replicated on other nodes so many times as defined by a 'replication-factor'
           (default rep-factor is 3)      
      
     Q1:  If I want to stote 1024 MB (1 GB) file on HDFS, how much physical storage space 
          is required (using default configs) => 3 GB   

     Q2:  If you want to store 1034 MB file, how many unique blocks are created on HDFS?
	  -> 8 blocks of 128MB each & 1 block of 10 MB
         
     
    MapReduce (Distributed Processing Solution)
    --------------------------------------------
   
     -> MR is a distributed processing solution where a "map" task is launched on small chuck
        of data (like a block) on HDFS. 
     -> All these "map" tasks produce intermediate files (results of map tasks).
     -> All these intermediate files are shuffled (collected from multiple machines) and sorted.        
     -> This shuffled data is sent to "reducer" program/task
     -> "reducer" task will aggregate the results and write them as final output on HDFS. 


   Hadoop EcoSystem
   ----------------   
     -> Refers to a set of tools/frameworks/software that run on top of hadoop to solve
        various big data related problems.          

     -> Popular Components in Hadoop EcoSystem             

          -> Resource Management Tool: YARN
          -> Coordination Management Tool: ZooKeeper
          -> Execution Frameworks: MapReduce, Spark, Tez
          -> SQL-Analytics: Hive, Impala, Drill
	  -> Streaming Data Processing: Kafka, Storm, Spark Streaming
	  -> Machine Learning: Mahout, Spark MLLib
          -> Scripting: Pig
          -> NoSQL database: HBase
          -> Data ingestion tools: Flume, Sqoop


  =======================================================================
     HDFS (Hadoop Distributed File System)
  =======================================================================

   -> Storage Solution
   -> Stores data as blocks of 128 MB 
   -> Each block is replicated.

   HDFS Architecture
   -----------------

   Hadoop uses Master-Slave Architecture


   HDFS Daemons 
   ------------
     -> Daemon: Continuosly running background job. 

     1. Name Node (NN)	
	-> runs on the master machine (master daemon)
        -> recieves heart-beats from DNs.
	   -> if three successive heart-beats are missing from any DN, it is considered as 'dead'
        -> NN maintains two "meta-data" files
	    
           1. FSImage
		-> Persistent disk file
                -> Will contain all the meta-data info from the time that the NN service is started
                   upto the last checkpoint.

	   2. EditLog
		-> Maintained in memory (RAM)
		-> All incrmental changes are recorded in this file.        

     2. Data Node (DN)
	-> runs on every slave machine (salve daemon)
  	-> every DB sends heart-beats to NN once in every 3 seconds
 
        -> In every heart-beat, DN sends "block-report" of that node
           -> A 'block-report' consists of such info as storage data, which files, which blocks, 
              rep-factors, permissions, ...        
    
     3. Secondary Name Node (SNN)
          -> responsible for check-pointing process
              -> check-pointing is a process of merging the edit-log file with fsimage file.

     4. Balancer 
	  -> Ensures that the replication-factor of the files is maintained. 

          -> A file can become "under-replicated" if one nodes that contain the blocks of the
             goes down, or if the rep-factor of the file is increased. Then balencer creates
             additional replicas of the blocks of that file.  

	  -> A file can become "over-replicated" if one nodes that contain the blocks that was 
             previously down and  comes up again, or if the rep-factor of the file is decreased.
             Then balencer removes the replicas of the blocks of that file beyond the rep-factor.   


   HDFS High Availability
   ----------------------
     
      => Availability : Means that the system do not have any down-time. Always available to the client.
        
      => Hadoop High Availability is introduced in Hadoop 2.0 onwards. 
	   -> We have "Active NN" and "Stand-by Name Node"

      => Hadoop 1.x is not high availble system.
	   -> In Hadoop 1.x, Name Node is a 'Single Point of Failure'   



      HDFS File Operations
      ---------------------

       => HDFS is non-editable file system.

       Default Paths:

        * Cloudera Local Home :  /home/cloudera
        * HDFS Home: 		 /user/cloudera
        
        HDFS FS Command Syntax:  $hadoop fs -<command-name> <parameters>
                                 $hdfs dfs -<command-name> <parameters>

         
     
       Common File Ops
       ---------------

 * hadoop fs -help   				=> lists all the commands with a brief discription
 * hadoop fs -help <command-name>  
 * hadoop fs -chmod -R 755 <file/dir>		=> change permission for a file/directory
 * hadoop fs -cat <file-name>
 * hadoop fs -ls
 * hadoop fs -ls <dir-path>
 * hadoop fs -ls -R  <dir-path>

 * hadoop fs -cp <source file> <target dir>     => Copies within HDFS frpm source to target
 * hadoop fs -mv <source file> <target dir>     => Moves within HDFS frpm source to target
 * hadoop fs -rm -r <file-name/directory-name>
	 
 // copy from Linux to HDFS
 * hadoop -copyFromLocal <local file path>  <hdfs file path> 
 * hadoop -put <local file path>  <hdfs file path> 

 // copy from HDFS to Linux
 * hadoop -copyToLocal <hdfs file path> <local file path>   
 * hadoop -get <hdfs file path> <local file path>   

 * hadoop -moveFromLocal <local file path>  <hdfs file path> 
 * hadoop -moveToLocal <hdfs file path> <local file path> 


   ==========================================================
     YARN
  ==========================================================

   -> Resource Manager

   -> Hadoop 1.0,  MapReduce used to handle resource-management
       -> MR was the only execution process.

   -> Hadoop 2.0
	->  Resource Management:  YARN  (Yet another resource negotiator)
        ->  Execution: MapReduce

        -> Allowed hadoop to support other execution frameworks as-well
      
   YARN Processing Components
   --------------------------

       1. Resource Manager (daemon)  -> one per cluster, runs in a master node
       2. Node Manager (daemon)	     -> one per node, runs in all slave nodes
       3. Application Master         -> one per application, can run in any NN
       4. Set of Containers          -> Allocated to AM where tasks are executed

  ==========================================================
     MapReduce
  ==========================================================

   -> Distributed Processing Framework.

    
   Two facets of MapReduce
   ------------------------

    1. MapReduce execution flow (framework process)
	-> How an mR program is executed (what the system does)

	1. Map Phase (developer)  
        2. Combiner
	3. Shuffle & Sort Phase (framework)
        4. Partitioner
	5. Reduce Phase (developer)


    2. MapReduce Programming Structure
	-> How to write MR programs (what developers should do)

	-> An MR Application can have several programs (upto 5 different programs)
   		1. Driver Program 
		       -> master process where configuration details are specified.
		2. Mapper
			-> Logic defined by the developer
			-> has map() method that run once for every input record
		3. Reducer
			-> Logic defined by the developer
			-> has reduce() method that run once for every input record
		4. Combiner
			-> Like a mini-reducer logic applied to the output of each mapper
                        -> Will reduce the amount of data shuffled and improves performance
                        -> Operation has to be commutative.  
		5. Partitioner
			-> Allows the developer to control which keys  goes to which partition
                           based on his own custom implementation.
   
   
   MapReduce Framework Classes
   ---------------------------

   1. InputFormat -> It will identify 'records' from the blocks

   2. InputSplit -> Creates input-split
                    An input-split will have integral number of records.
                    Each input-split can span multiple-blocks.

   3. RecordReader -> Will read input-splits and create (K,V) pairs from the records
                     Call map() method in a mapper with each record.



    MapReduce DataTypes
    -------------------
    -> MR data-types are optimized for serialization & deserialization.
    -> All MR class uses MapReduce Custom Data Types
    
      Java                MapReduce
      ----                ---------
      String		  Text
      Integer		  IntWritable
      Long		  LongWritable
      Boolean		  BoolWritable
      Null		  NullWritable


    Wordcount Program
    -----------------
    Find the word frequency count of all the words in a given text file.
   

    Input-Split
    ------------
InputSplit                          RecordReader														Shuffle & Sort  			Reduce      

hadoop spark scala flatmap    	-> (1, "hadoop spark scala flatmap") --> map(k, v, context) --> (hadoop, 1) (spark, 1) (scala, 1) (flatmap, 1)   ===>   (hadoop, [1,1,1,1])    		   => reduce => (hadoop, 4)                                  
map groupby spark spark		-> (27, "map groupby spark spark")          			(map, 1) (groupby, 1) (spark, 1) (spark, 1)       	(spark, [1,1,1,1,1,1,1,1,1,1,1,1])              (spark, 12)
hadoop spark scala flatmap 	-> (50, "hadoop scala flume oozie")	    			(hadoop, 1) (spark, 1) (scala, 1) (flatmap, 1)          (flatmap, [1,1,1,1)                             (flatmap, 4)
map groupby spark spark		-> (74, "sqoop hive hive spark spark")     			(map, 1) (groupby, 1) (spark, 1) (spark, 1)		(scala, [1,1,1,1,1,1,1])		        (scala, 6)
hadoop spark scala flatmap	-> (102, "mapreduce hadoop hive hadoop")    			(hadoop, 1) (spark, 1) (scala, 1) (flatmap, 1) 
map groupby spark spark		-> (135, "hdfs flatmap rdd scala")         			(map, 1) (groupby, 1) (spark, 1) (spark, 1) 
hadoop spark scala flatmap 	-> (168, "spark scala transformations")     			(hadoop, 1) (spark, 1) (scala, 1) (flatmap, 1)
map groupby spark spark		-> (197, "actions rdd rdd rdd rdd")         			(map, 1) (groupby, 1) (spark, 1) (spark, 1)

      
                           mapper   																															  

     input => key type: LongWritable     output =>   key type: Text 
	    value type: Text                      value type: LongWritable


 			   reducer 																															  

     input => key type: Text            output =>      key type:  Text
	    value type: LongWritable                  value type: LongWritable

   
   Submitting MR program to the cluster
   ------------------------------------

    $hadoop jar <jar-file-path> <driver-class-qualified-name> <commad-line-args>
    $yarn jar <jar-file-path> <driver-class-qualified-name> <commad-line-args>
   
    $hadoop jar /home/cloudera/mrpractice.jar com.sample.WordCountJob <input-file-path> <output-dir>
    
 
    MapReduce Life Cycle Methods
    ----------------------------


	Mapper Life Cycle methods
	--------------------------

	1. setup	-> Called once at the beginning of the MR process before the map() method
                           is called.
			-> Write initialization code.			
			  
	2. map		-> Executed once for every record in the input-split
			-> Write the logic that you want to apply on the record.

	3. cleanup	-> Called once after the map() method is completed for all records.
			-> Release your resource and other cleanup.

	4. run 		-> Generally not overridden
                        -> Can use this to change the execution flow.


	Reducer Life Cycle methods
	---------------------------

	1. setup	-> Called once at the beginning of the MR process before the reduce() method
                           is called.
			-> Write initialization code.			
			  
	2. reduce       -> Executed once for every record in the input-split
			-> Write the logic that you want to apply on the record.

	3. cleanup	-> Called once after the reduce() method is completed for all records.
			-> Release your resource and other cleanup.

	4. run 		-> Generally not overridden
                        -> Can use this to change the execution flow.
   

     Facebook Dataset Processing
     ---------------------------
    
     From the facebook dataset, extract the record with maximum likes_received from among the records
     of people born between 1990 and 2000.

     MR 1: Extract the records of people born between 1990 and 2000.
     MR 2: To find out the record with maximum likes_received 



    Hadoop Configuartion Files
    --------------------------
  
       location:  /etc/hadoop/conf

         -> core-site.xml
            hdfs-site.xml
            mapred-site.xml




























	 