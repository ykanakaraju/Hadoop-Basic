
 Curriculum (40 sessions - 80 hours)
 -------------------------------------
  Introduction to Big Data & Hadoop
  HDFS
  YARN
  MapReduce (using Java)
  Hive 
  Impala - Basics
  Sqoop  
  HBase - Basic Operations
  Kafka (using Java)      
  Spark (PySpark)  


 Training Materials
 ------------------
    -> PDF Presentations
    -> Code modules, command documents, programs
    -> Class Notes
    -> Databricks Notebooks
    -> GitHub: https://github.com/ykanakaraju/Hadoop-Basic


 Pre-requisites
 --------------
	-> SQL
	-> Java
	-> Python

 ------------------------------------------------------------------ 
  
   Big Data
   --------
    
   - Type of data which large and complex that your traditional data processing systems
     can not reasonably store and process. 

   Big Data scenarios
   ------------------
       -> The volume to be handled is very high (much higher than 1 TB)
       -> You want to support real-time analytics (velocity)
       -> You data is growing very fast, so that in no time your system becomes inadeqaute. 
       -> Process unstructures, semi-structured data etc.
    
  Big Data Characteristics
  ------------------------
       -> Volume
       -> Velocity
       -> Variety 
       -> Veracity (correctness of the data)
       -> Value (extracting insights from the data)       

  Why your traditional systems have big data problem
  --------------------------------------------------
   -> Because your traditional systems ( that exists in pre-internet era ) are not designed
      for internet scale applications.
 
   -> They are designed to run on a single machine. The single machine has only limited 
      amount of resources.


  Scalability Options
  -------------------
    -> Scale up  (Vertical Scaling)   -> Migrate to a machine with higher configuration
    -> Scala out (Horizontal Scaling) -> Use many machines instead of one. 
                                      -> Use computing cluster

   What is a Computing Cluster?
   ----------------------------
   A unified network of nodes (machines) whose cumulative resources can be used to store
   or process data as if you working with a single machine. 

   
   What is a Framework ?
   ---------------------
    A framework is a set of libraries that implement all the basic low-level functionalities
    that the framework is intended for. The users of the framework should focus on the 
    business-logic and implement their solutions as per the guidelines provided by the 
    framework. 


    What is the solution to storage & processing problems associated with Big Data
    ------------------------------------------------------------------------------
    -> Hadoop provides a solution for big-data problems.

 
    What is Hadoop ?
    ----------------     
    Hadoop is a Open Source Framework to Store and Process data in a distributed way 
    using a cluster of commdity harware. 

     -> Provides two things:
	   -> Distributed Storage Solution (HDFS)
	   -> Distributed Processing Solution (MapReduce)
        And also,
           -> Resource Management (YARN)


    Hadoop Distributions
    --------------------
	Apache Hadoop => Open source	
	On-Prem & Hybrid Cloud => Cloudera & MapR
	Managed Cloud Solutions 
		-> AWS   : AWS Elastic MapReduce (EMR)
		-> Azure : HDInsights
	        -> GCP   : DataProc 


    HDFS (Hadoop Distributed File System)
    --------------------------------------

      -> Stores data in the form of "blocks" of 128 MB each
      -> Each block is replicated on other nodes so many times as defined by a 'replication-factor'
           (default rep-factor is 3)      
      
     Q1:  If I want to stote 1024 MB (1 GB) file on HDFS, how much physical storage space 
          is required (using default configs) => 3 GB   

     Q2:  If you want to store 1034 MB file, how many unique blocks are created on HDFS?
	  -> 8 blocks of 128MB each & 1 block of 10 MB
         
     
    MapReduce (Distributed Processing Solution)
    --------------------------------------------
   
     -> MR is a distributed processing solution where a "map" task is launched on small chuck
        of data (like a block) on HDFS. 
     -> All these "map" tasks produce intermediate files (results of map tasks).
     -> All these intermediate files are shuffled (collected from multiple machines) and sorted.        
     -> This shuffled data is sent to "reducer" program/task
     -> "reducer" task will aggregate the results and write them as final output on HDFS. 


   Hadoop EcoSystem
   ----------------   
     -> Refers to a set of tools/frameworks/software that run on top of hadoop to solve
        various big data related problems.          

     -> Popular Components in Hadoop EcoSystem             

          -> Resource Management Tool: YARN
          -> Coordination Management Tool: ZooKeeper
          -> Execution Frameworks: MapReduce, Spark, Tez
          -> SQL-Analytics: Hive, Impala, Drill
	  -> Streaming Data Processing: Kafka, Storm, Spark Streaming
	  -> Machine Learning: Mahout, Spark MLLib
          -> Scripting: Pig
          -> NoSQL database: HBase
          -> Data ingestion tools: Flume, Sqoop


  =======================================================================
     HDFS (Hadoop Distributed File System)
  =======================================================================

   -> Storage Solution
   -> Stores data as blocks of 128 MB 
   -> Each block is replicated.

   HDFS Architecture
   -----------------

   Hadoop uses Master-Slave Architecture


   HDFS Daemons 
   ------------
     -> Daemon: Continuosly running background job. 

     1. Name Node (NN)	
	-> runs on the master machine (master daemon)
        -> recieves heart-beats from DNs.
	   -> if three successive heart-beats are missing from any DN, it is considered as 'dead'
        -> NN maintains two "meta-data" files
	    
           1. FSImage
		-> Persistent disk file
                -> Will contain all the meta-data info from the time that the NN service is started
                   upto the last checkpoint.

	   2. EditLog
		-> Maintained in memory (RAM)
		-> All incrmental changes are recorded in this file.        

     2. Data Node (DN)
	-> runs on every slave machine (salve daemon)
  	-> every DB sends heart-beats to NN once in every 3 seconds
 
        -> In every heart-beat, DN sends "block-report" of that node
           -> A 'block-report' consists of such info as storage data, which files, which blocks, 
              rep-factors, permissions, ...        
    
     3. Secondary Name Node (SNN)
          -> responsible for check-pointing process
              -> check-pointing is a process of merging the edit-log file with fsimage file.

     4. Balancer 
	  -> Ensures that the replication-factor of the files is maintained. 

          -> A file can become "under-replicated" if one nodes that contain the blocks of the
             goes down, or if the rep-factor of the file is increased. Then balencer creates
             additional replicas of the blocks of that file.  

	  -> A file can become "over-replicated" if one nodes that contain the blocks that was 
             previously down and  comes up again, or if the rep-factor of the file is decreased.
             Then balencer removes the replicas of the blocks of that file beyond the rep-factor.   


   HDFS High Availability
   ----------------------
     
      => Availability : Means that the system do not have any down-time. Always available to the client.
        
      => Hadoop High Availability is introduced in Hadoop 2.0 onwards. 
	   -> We have "Active NN" and "Stand-by Name Node"

      => Hadoop 1.x is not high availble system.
	   -> In Hadoop 1.x, Name Node is a 'Single Point of Failure'   



      HDFS File Operations
      ---------------------

       => HDFS is non-editable file system.

       Default Paths:

        * Cloudera Local Home :  /home/cloudera
        * HDFS Home: 		 /user/cloudera
        
        HDFS FS Command Syntax:  $hadoop fs -<command-name> <parameters>
                                 $hdfs dfs -<command-name> <parameters>

         
     
       Common File Ops
       ---------------

 * hadoop fs -help   				=> lists all the commands with a brief discription
 * hadoop fs -help <command-name>  
 * hadoop fs -chmod -R 755 <file/dir>		=> change permission for a file/directory
 * hadoop fs -cat <file-name>
 * hadoop fs -ls
 * hadoop fs -ls <dir-path>
 * hadoop fs -ls -R  <dir-path>

 * hadoop fs -cp <source file> <target dir>     => Copies within HDFS frpm source to target
 * hadoop fs -mv <source file> <target dir>     => Moves within HDFS frpm source to target
 * hadoop fs -rm -r <file-name/directory-name>
	 
 // copy from Linux to HDFS
 * hadoop -copyFromLocal <local file path>  <hdfs file path> 
 * hadoop -put <local file path>  <hdfs file path> 

 // copy from HDFS to Linux
 * hadoop -copyToLocal <hdfs file path> <local file path>   
 * hadoop -get <hdfs file path> <local file path>   

 * hadoop -moveFromLocal <local file path>  <hdfs file path> 
 * hadoop -moveToLocal <hdfs file path> <local file path> 


   ==========================================================
     YARN
  ==========================================================

   -> Resource Manager

   -> Hadoop 1.0,  MapReduce used to handle resource-management
       -> MR was the only execution process.

   -> Hadoop 2.0
	->  Resource Management:  YARN  (Yet another resource negotiator)
        ->  Execution: MapReduce

        -> Allowed hadoop to support other execution frameworks as-well
      
   YARN Processing Components
   --------------------------

       1. Resource Manager (daemon)  -> one per cluster, runs in a master node
       2. Node Manager (daemon)	     -> one per node, runs in all slave nodes
       3. Application Master         -> one per application, can run in any NN
       4. Set of Containers          -> Allocated to AM where tasks are executed

  ==========================================================
     MapReduce
  ==========================================================

   -> Distributed Processing Framework.

    
   Two facets of MapReduce
   ------------------------

    1. MapReduce execution flow (framework process)
	-> How an mR program is executed (what the system does)

	1. Map Phase (developer)  
        2. Combiner
	3. Shuffle & Sort Phase (framework)
        4. Partitioner
	5. Reduce Phase (developer)


    2. MapReduce Programming Structure
	-> How to write MR programs (what developers should do)

	-> An MR Application can have several programs (upto 5 different programs)
   		1. Driver Program 
		       -> master process where configuration details are specified.
		2. Mapper
			-> Logic defined by the developer
			-> has map() method that run once for every input record
		3. Reducer
			-> Logic defined by the developer
			-> has reduce() method that run once for every input record
		4. Combiner
			-> Like a mini-reducer logic applied to the output of each mapper
                        -> Will reduce the amount of data shuffled and improves performance
                        -> Operation has to be commutative.  
		5. Partitioner
			-> Allows the developer to control which keys  goes to which partition
                           based on his own custom implementation.
   
   
   MapReduce Framework Classes
   ---------------------------

   1. InputFormat -> It will identify 'records' from the blocks

   2. InputSplit -> Creates input-split
                    An input-split will have integral number of records.
                    Each input-split can span multiple-blocks.

   3. RecordReader -> Will read input-splits and create (K,V) pairs from the records
                     Call map() method in a mapper with each record.



    MapReduce DataTypes
    -------------------
    -> MR data-types are optimized for serialization & deserialization.
    -> All MR class uses MapReduce Custom Data Types
    
      Java                MapReduce
      ----                ---------
      String		  Text
      Integer		  IntWritable
      Long		  LongWritable
      Boolean		  BoolWritable
      Null		  NullWritable


    Wordcount Program
    -----------------
    Find the word frequency count of all the words in a given text file.
   

    Input-Split
    ------------
InputSplit                          RecordReader														Shuffle & Sort  			Reduce      

hadoop spark scala flatmap    	-> (1, "hadoop spark scala flatmap") --> map(k, v, context) --> (hadoop, 1) (spark, 1) (scala, 1) (flatmap, 1)   ===>   (hadoop, [1,1,1,1])    		   => reduce => (hadoop, 4)                                  
map groupby spark spark		-> (27, "map groupby spark spark")          			(map, 1) (groupby, 1) (spark, 1) (spark, 1)       	(spark, [1,1,1,1,1,1,1,1,1,1,1,1])              (spark, 12)
hadoop spark scala flatmap 	-> (50, "hadoop scala flume oozie")	    			(hadoop, 1) (spark, 1) (scala, 1) (flatmap, 1)          (flatmap, [1,1,1,1)                             (flatmap, 4)
map groupby spark spark		-> (74, "sqoop hive hive spark spark")     			(map, 1) (groupby, 1) (spark, 1) (spark, 1)		(scala, [1,1,1,1,1,1,1])		        (scala, 6)
hadoop spark scala flatmap	-> (102, "mapreduce hadoop hive hadoop")    			(hadoop, 1) (spark, 1) (scala, 1) (flatmap, 1) 
map groupby spark spark		-> (135, "hdfs flatmap rdd scala")         			(map, 1) (groupby, 1) (spark, 1) (spark, 1) 
hadoop spark scala flatmap 	-> (168, "spark scala transformations")     			(hadoop, 1) (spark, 1) (scala, 1) (flatmap, 1)
map groupby spark spark		-> (197, "actions rdd rdd rdd rdd")         			(map, 1) (groupby, 1) (spark, 1) (spark, 1)

      
     mapper
     -------
     input => key type: LongWritable     output =>   key type: Text 
	    value type: Text                         value type: LongWritable


    reducer 
    -------
     input => key type: Text            output =>    key type:  Text
	    value type: LongWritable                 value type: LongWritable

   
   Submitting MR program to the cluster
   ------------------------------------

    $hadoop jar <jar-file-path> <driver-class-qualified-name> <commad-line-args>
    $yarn jar <jar-file-path> <driver-class-qualified-name> <commad-line-args>
   
    $hadoop jar /home/cloudera/mrpractice.jar com.sample.WordCountJob <input-file-path> <output-dir>
    
 
    MapReduce Life Cycle Methods
    ----------------------------


	Mapper Life Cycle methods
	--------------------------

	1. setup	-> Called once at the beginning of the MR process before the map() method
                           is called.
			-> Write initialization code.			
			  
	2. map		-> Executed once for every record in the input-split
			-> Write the logic that you want to apply on the record.

	3. cleanup	-> Called once after the map() method is completed for all records.
			-> Release your resource and other cleanup.

	4. run 		-> Generally not overridden
                        -> Can use this to change the execution flow.


	Reducer Life Cycle methods
	---------------------------

	1. setup	-> Called once at the beginning of the MR process before the reduce() method
                           is called.
			-> Write initialization code.			
			  
	2. reduce       -> Executed once for every record in the input-split
			-> Write the logic that you want to apply on the record.

	3. cleanup	-> Called once after the reduce() method is completed for all records.
			-> Release your resource and other cleanup.

	4. run 		-> Generally not overridden
                        -> Can use this to change the execution flow.
   

     Facebook Dataset Processing
     ---------------------------
    
     From the facebook dataset, extract the record with maximum likes_received from among the records
     of people born between 1990 and 2000.

     MR 1: Extract the records of people born between 1990 and 2000.
     MR 2: To find out the record with maximum likes_received 



    Hadoop Configuartion Files
    --------------------------  
       location:  /etc/hadoop/conf

       -> core-site.xml
       -> hdfs-site.xml
       -> mapred-site.xml


   =============================
       Hive
   =============================

    Hive is Hadoop's Data Warehousing Framework.
    
    Hive is an SQL wrapper on top of map-reduce. So that user can directly work with SQL without
    having to write MR programs. Hive Server can create the MR programs and run them on the cluster
    releaving the analysts from writing MR programs.

    => Hive is NOT an RDBMS. 
    => Hive is NOT a NoSQL database

    Hive provides a dialect of SQL called "Hive Query Language" (HQL, HiveQL)


    Hive Components
    ----------------
  
    1. Hive Warehouse - An HDFS directory where hive stores all its data files of all its managed tables.
                        In cloudera:  /user/hive/warehouse

    2. Hive MetaStore - MetaStore is typically an RDBMS where hive stores its metadata
                        (Most popular option in production systems is MySQL)
                        -> Hive comes with an embedded metastore database called "derby"
        -> Warehouse  : DATA
	-> Meta Store : METADATA

    3. Hive Server   - Responsible for executing HQL statements (and creating MR programs if required)



    How to Access Hive ?
    --------------------

    1. Command Line Interface (CLI Shells)
	-> hive shell  
		-> Local hive installations with default user.
		-> Less secure

        -> beeline shell (preferred for security reasons)
		-> Connects to Hive using JDBC connection with a specific user credential
                -> More secure.

    2. HUE (Hadoop User Environment)
         -> Provides a Web UI to interact with various Hadoop tools.

    3. Client Applications  ( JDBC, ODBC, Thrift )


   Hive File Structure 
   -------------------
   
    Hive Warehouse:  /user/hive/warehouse

    ->  Hive created separate directories for each database in the Hive warehouse
        (exception: no separate directory for 'default' database)

    -> For every table in the database, a separate directory is created.
       All the data files of that table are saved in that directory.

       ex: db1.emp   ==> /user/hive/warehouse/db1.db/emp
	   db1.emp2  ==> /user/hive/warehouse/db1.db/emp2


   Working with Tables
   -------------------

	CREATE TABLE empinfo (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
	ROW FORMAT DELIMITED
	FIELDS TERMINATED BY ','
	LINES TERMINATED BY '\n' 
	STORED AS TEXTFILE;


   Defaults
   --------
        -> default serde: DELIMITED
        -> default storage format : TEXTFILE
        -> default line terminator: \n
        -> default field terminator: Ctrl-A  ( \0001 )           

   Describe
   ---------
    -> describe empinfo
	-> describe extended empinfo
	-> describe formatted empinfo    


    How to load/insert data into Hive tables
    ----------------------------------------    
    1. Load data from external data files (such as delimited files (csv, tsv), or parquet, orc, sequence,..)
    2. Insert data into an existing table from a base table ( INSERT .. SELECT )
    3. Insert data using INSERT..VALUES command (not recommended)..  
   

     -> location of data file: Linux, HDFS       ( LOCAL INPATH: Linux,   INPATH: HDFS )
     -> Append the data / Overwrite the data     ( INTO TABLE: append,  OVERWRITE INTO TABLE: overwrite )   

       
        // Loading from Linux
       	LOAD DATA LOCAL INPATH <linux-file-path> INTO TABLE <table-name>
	LOAD DATA LOCAL INPATH <linux-file-path> OVERWRITE INTO TABLE <table-name>
             	=> Loading from Linux a "Copy" operation

   
        // Loading from HDFS
   	LOAD DATA INPATH <hdfs-file-path> INTO TABLE <table-name>
	LOAD DATA INPATH <hdfs-file-path> OVERWRITE INTO TABLE <table-name>
		=> Loading from HDFS a "Move" operation


    Hive follows Scehma on Read
    ---------------------------
    -> Hive do not control what data goes into the table. 
    -> Schema validation does not happen while "loading" data into the table.
    -> Scahema validation happens while reading the data.
       -> If the data can not be parsed as per schema, Hive shows NULL for that field.


    Managed Tables
    --------------
        -> Managed tables are managed by Hive and are stored in Hive warehouse directory.
        -> When you drop a managed table, Hive drops the schema as well as the data files. 

    
    External Tables
    ---------------
        -> External tables are not managed by Hive.
        -> The data files of external table are not stored in Hive warehouse. 
        -> They are stored in an external location specified while creating the table.
        -> The data loaded into external table either from hive or from any other application/tool
           is not owned/managed by Hive.
        -> When we drop the table, ONLY schema is dropped. The data files are NOT DROPPED.
  

    Use Cases for External tables
    -----------------------------

       -> External tables are used when multiple hive databases are using the same data. 
       -> You can create an external table pointing to existing data, analyze that data and drop the
          table without affecting the actual data.


    Partitioned Tables
    ------------------

     In Hive, Partitions provide a way of dividing a table into coarse-grained 
     parts based on the value of a partition column, such as a date or country.


     Partitioning allows us to create tables partitioned by one or more columns.
     
     Each unique value of the partitioned column will have its own directory 
     within the table's directory.

     Based on the way we load data into the tables we can have static & dynamic partitions

     -> Partition columns should reasonably low-cardinality columns (such as year, country, month, ...)


     Static Partitioning
     -------------------
	-> The data files are loaded by the user using "LOAD DATA .." command
        -> The user has to know precisly to which partition he is loading the file.


     Dynamic Partitioning
     --------------------
	-> We dynamic partitioning when the user do not know to which partition the data goes to.
           This may be because the partition info itself is there in the data or may be we have
           data of many partitions in the same file. 

        -> We can not use "LOAD DATA.." command to load the data into the partitioned table

        -> We have to load the data using dynamic partitioning scheme from a base table using
           "INSERT .. SELECT .." command. 

        -> Data files are created dynamically by hive (hense the name dynamic partitioning)

        -> set hive.exec.dynamic.partition.mode=nonstrict


 
   Working with Bucketted Tables
   -----------------------------

	set hive.enforce.bucketing = true;


	INSERT INTO TABLE students_buckets
	SELECT id, name, mobile, email, project, country, batch FROM students;
	SELECT * FROM students_buckets WHERE id = 20;

	CREATE TABLE students_part_batch(id int, name string, mobile string, email string, project string, country string)
	PARTITIONED BY (batch int)
	CLUSTERED BY (id) INTO 5 BUCKETS
	ROW FORMAT DELIMITED 
	FIELDS TERMINATED BY ','
	LINES TERMINATED BY '\n' 
	STORED AS TEXTFILE;

	INSERT INTO TABLE students_part_batch
	PARTITION (batch)
	SELECT id, name, mobile, email, project, country, batch FROM students;


	SELECT * FROM students_part_batch TABLESAMPLE (BUCKET 1 OUT OF 2 on id);


     Managing Outputs
     ---------------

       SELECT * FROM Students
  
		-> You can return the data to the client application.
		-> insert the data into a table (INSERT ... SELECT ...)
		-> create a table with the output of  a query (CREATE TABLE AS SELECT..)
		-> Save the output of a query into a local directory or hdfs directory in desired format. 


       INSERT OVERWRITE LOCAL DIRECTORY '<linux-directory>' STORED AS TEXTFILE select * from students;  
       INSERT OVERWRITE DIRECTORY '<hdfs-directory>' STORED AS ORC select * from students;  

 ===================================================

CREATE DATABASE ctshadoop3 LOCATION '/user/cloudera/hivedb/ctshadoop3';

CREATE TABLE empinfo (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/emp.txt' INTO TABLE empinfo;


CREATE EXTERNAL TABLE empinfoexternal (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED AS TEXTFILE
LOCATION '/user/cloudera/hive/empinfoexternal';

LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/emp.txt' INTO TABLE empinfoexternal;


CREATE TABLE students_part (id INT, name STRING, phone STRING, email STRING, project STRING)
PARTITIONED BY (country STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;


LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/students_IN.txt' INTO TABLE students_part

PARTITION (country='India');


CREATE TABLE students_dynamic_part (id INT, name STRING, phone STRING, email STRING, project STRING)
PARTITIONED BY (country STRING, batch INT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;


// Base Table
CREATE TABLE students (id INT, name STRING, phone STRING, email STRING, project STRING, country STRING, batch INT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;


LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/students_country_batch.csv' INTO TABLE students;


// Loading Data using Dynamic Partitioning

INSERT INTO students_dynamic_part
PARTITION (country, batch)
SELECT id, name, phone, email, project, country, batch FROM students;

INSERT INTO students_dynamic_part
PARTITION (country='India', batch)
SELECT id, name, phone, email, project, batch FROM students
WHERE country = 'India';

INSERT INTO students_dynamic_part
PARTITION (country='Japan', batch=100)
SELECT id, name, phone, email, project FROM students
WHERE country = 'USA' and batch=1;

-- the following statement throws an exception...
-- SemanticException: "Dynamic partition cannot be the parent of a static partition"
INSERT INTO students_dynamic_part
PARTITION (country, batch=100)
SELECT id, name, phone, email, project, country FROM students
WHERE country = 'USA';

LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/students_country_batch_2.csv' 
OVERWRITE INTO TABLE students_dynamic_part

PARTITION (country='India', batch=1);

// -------------------------------------
CREATE TABLE students2 (stdid INT, stdname STRING, mobile STRING, emailid STRING, proj STRING)
PARTITIONED BY (ctry STRING, btch INT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

INSERT INTO students2
PARTITION (ctry, btch)
SELECT id, phone, name, email, project, country, batch FROM students;


// -------- BUCKETTING -----------
CREATE TABLE students_bucket 
(id INT, name STRING, phone STRING, email STRING, project STRING, country STRING, batch INT)

CLUSTERED BY (id) SORTED BY (id ASC) 
INTO 4 BUCKETS
ROW FORMAT DELIMITED
FIELDS TERMINATED BY "|"
STORED AS TEXTFILE;


//-- loading data into bucketted tables

-> To load data into bucketted table set the following config property:
   
   set hive.enforce.bucketing = true; 

INSERT OVERWRITE TABLE students_bucket SELECT * FROM students;

CREATE TABLE students_bucket_2 
(id INT, name STRING, phone STRING, email STRING, project STRING, country STRING, batch INT)

CLUSTERED BY (id) SORTED BY (id ASC) 
INTO 3 BUCKETS;

INSERT OVERWRITE TABLE students_bucket_2 SELECT * FROM students;

/user/hive/warehouse/ctshadoop2.db/students_bucket_2/000000_0

// -- using default Ctrl-A delimited data, we can create a table as below. 
CREATE TABLE students3 (id INT, name STRING, phone STRING, email STRING, project STRING, country STRING, batch INT);

LOAD DATA INPATH '/user/hive/warehouse/ctshadoop2.db/students_bucket_2/000000_0' INTO TABLE students3


// -------  Bucketting a partitioned table
CREATE TABLE students_part_bucket (id INT, name STRING, phone STRING, email STRING, project STRING)
PARTITIONED BY (country STRING)
CLUSTERED BY (id) INTO 3 BUCKETS
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

INSERT INTO students_part_bucket
PARTITION (country)
SELECT id, name, phone, email, project, country FROM students;

INSERT OVERWRITE DIRECTORY '/user/cloudera/hive_data/empinfo_text' 
STORED AS TEXTFILE 
SELECT * FROM empinfo;

==================================================================


 Working with different file formats
 -----------------------------------

     ORC & Parquet   ==> Columnar File Formats 
                       -> Store all column information together at one physical location.
                       -> This improves performance of aggrgations
                       -> As aggregations are heavily used in analytical warkloads, these formats are very
                          efficient in data-warehouseing like use-cases.

                   => Very good read performance
                   => moderatly good write performance
                   => Splittable
		   => Compressable


     Avro  => Is used where we need schema evolution. 
                 -> The data schema is embedded in the file itself.

     Sequence => Stores data in the form of (K, V) pairs.


	SEQUENCE FILE
	=============

	CREATE TABLE empinfo_seq (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT) 
	STORED AS SEQUENCEFILE;

	INSERT OVERWRITE LOCAL DIRECTORY '/home/cloudera/hive_data/empinfo_seq'
	STORED AS SEQUENCEFILE 
	SELECT * FROM empinfo;

	LOAD DATA LOCAL INPATH '/home/cloudera/hive_data/empinfo_seq/000000_0' OVERWRITE INTO TABLE empinfo_seq;

	INSERT OVERWRITE TABLE empinfo_seq SELECT * FROM empinfo;


	ORC FILE
	========

	CREATE TABLE empinfo_orc (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT) 
	STORED AS ORC;

	INSERT OVERWRITE LOCAL DIRECTORY '/home/cloudera/hive_data/empinfo_orc'
	STORED AS ORC 
	SELECT * FROM empinfo;

	LOAD DATA LOCAL INPATH '/home/cloudera/hive_data/empinfo_orc/000000_0' OVERWRITE INTO TABLE empinfo_orc;

	INSERT OVERWRITE TABLE empinfo_orc SELECT * FROM empinfo;

	LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/emp.txt' OVERWRITE INTO TABLE empinfo_orc;
	--> The above command leads to IOException.

	PARQUET   
	=======

	CREATE TABLE empinfo_parquet (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT) 
	STORED AS PARQUET;

	INSERT OVERWRITE TABLE empinfo_parquet SELECT * FROM empinfo;


	AVRO
	====

	CREATE TABLE empinfo_avro (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT) 
	STORED AS AVRO;

	INSERT OVERWRITE TABLE empinfo_avro SELECT * FROM empinfo;
 

  Hive Joins
  ----------
	
	CREATE TABLE IF NOT EXISTS stocks 
	(stock_exchange STRING, symbol STRING, trxn_date DATE, open DOUBLE, 
	high DOUBLE,low DOUBLE, close DOUBLE, volumn INT,adj_close DOUBLE)
	ROW FORMAT DELIMITED
	FIELDS TERMINATED BY '\t'
	LINES TERMINATED BY '\n' 
	STORED AS TEXTFILE;

	LOAD DATA LOCAL INPATH '/home/cloudera/datasets/stocks' 
	OVERWRITE INTO TABLE stocks; 

	CREATE TABLE IF NOT EXISTS dividends 
	(stock_exchange STRING, symbol STRING, trxn_date DATE, dividend DOUBLE)
	ROW FORMAT DELIMITED
	FIELDS TERMINATED BY '\t'
	LINES TERMINATED BY '\n' 
	STORED AS TEXTFILE;

	LOAD DATA LOCAL INPATH '/home/cloudera/datasets/dividends' 
	OVERWRITE INTO TABLE dividends; 


	=================================================
	   JOINS & STREAMING JOINS
	=================================================

	 ------------------------------------------------
	 -- regular join
	 ------------------------------------------------
	 SELECT s.trxn_date, s.symbol, s.adj_close, d.dividend
	 FROM stocks s JOIN dividends d
	 ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol; 

	 ------------------------------------------------
	 -- regular join with better placement of tables
	 ------------------------------------------------
	 SELECT s.trxn_date, s.symbol, s.adj_close, d.dividend
	 FROM dividends d JOIN stocks s
	 ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol;

	 ------------------------------------------------
	 -- stream join hint (stream a table to reducer)
	 ------------------------------------------------
	 SELECT /*+ STREAMTABLE(s) */
	 s.trxn_date, s.symbol, s.adj_close, d.dividend
	 FROM stocks s JOIN dividends d
	 ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol;
	 

	=================================================
	  Map Joins
	=================================================

	 ------------------------------------------------
	 -- map join hint
	 ------------------------------------------------
	 SELECT /*+ MAPJOIN(d) */
	 s.trxn_date, s.symbol, s.adj_close, d.dividend
	 FROM stocks s JOIN dividends d
	 ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol;

	 ------------------------------------------------
	 --property to be set (default is true anyway)
	 ------------------------------------------------
	 set hive.auto.convert.join=true
	 set hive.mapjoin.smalltable.filesize = 25000000

	 SELECT s.trxn_date, s.symbol, s.adj_close, d.dividend
	 FROM stocks s JOIN dividends d
	 ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol; 

	=================================================
	  Bucket Map Joins
	=================================================

	CREATE TABLE IF NOT EXISTS stocks_bkt_8 
	(stock_exchange STRING, symbol STRING, trxn_date DATE, open DOUBLE, 
	high DOUBLE,low DOUBLE, close DOUBLE, volumn INT,adj_close DOUBLE)
	CLUSTERED BY (trxn_date, symbol) INTO 8 BUCKETS 
	ROW FORMAT DELIMITED
	FIELDS TERMINATED BY '\t'
	STORED AS TEXTFILE;

	CREATE TABLE IF NOT EXISTS dividends_bkt_4 
	(stock_exchange STRING, symbol STRING, trxn_date DATE, dividend DOUBLE)
	CLUSTERED BY (trxn_date, symbol) INTO 4 BUCKETS 
	ROW FORMAT DELIMITED
	FIELDS TERMINATED BY '\t'
	STORED AS TEXTFILE;

	set hive.enforce.bucketing = true; 
	set hive.optimize.bucketmapjoin = true;


	INSERT INTO stocks_bkt_8 SELECT * FROM stocks SORT BY trxn_date, symbol;

	INSERT INTO dividends_bkt_4 SELECT * FROM dividends SORT BY trxn_date, symbol;

	SELECT /*+ MAPJOIN(dividends_bkt_4) */ s.trxn_date, s.symbol, s.adj_close, d.dividend
	FROM stocks_bkt_8 s JOIN dividends_bkt_4 d
	ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol;

	=======================================
	  Sort Merge Joins
	=======================================

	CREATE TABLE IF NOT EXISTS stocks_bkt_4
	(stock_exchange STRING, symbol STRING, trxn_date DATE, open DOUBLE, 
	high DOUBLE,low DOUBLE, close DOUBLE, volumn INT,adj_close DOUBLE)
	CLUSTERED BY (trxn_date, symbol) INTO 4 BUCKETS 
	ROW FORMAT DELIMITED
	FIELDS TERMINATED BY '\t'
	STORED AS TEXTFILE;

	INSERT INTO stocks_bkt_4 SELECT * FROM stocks SORT BY trxn_date, symbol;

	set hive.auto.convert.sortmerge.join=true;
	set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
	set hive.optimize.bucketmapjoin = true;
	set hive.optimize.bucketmapjoin.sortedmerge = true;
	set hive.auto.convert.sortmerge.join.nonconditionaltask=true;

	SELECT /*+ MAPJOIN(dividends_bkt_4) */ s.trxn_date, s.symbol, s.adj_close, d.dividend
	FROM stocks_bkt_4 s JOIN dividends_bkt_4 d
	ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol;


  Sorting in Hive
  ---------------
     => ORDER BY <col>
	  -> performs parallel total sort of the output files

     => SORT BY <col>
	   -> produces sorted file per reducer.

     => DISTRIBUTE BY deptid SORT BY empid    (can not use ORDER BY with DISTRIBUTE)
	   -> The data is partitioned based on the hashcode of deptid.

     => DISTRIBUTE BY deptid SORT BY deptid   (aka CLUSTER BY deptid) 
           --> There is a short form for the above command, where distribute and sort uses the same column.
           --> CLUSTER BY deptid


 multi-table insert
 ------------------
	from students
	insert overwrite table students_india select * where country='India' 
	insert overwrite table students_usa select * where country='USA' 
	insert overwrite table students_uk select * where country='UK';


 alter command
 -------------

	ALTER TABLE empinfo2 ADD COLUMNS (city STRING, state STRING);
	ALTER TABLE empinfo2 DROP COLUMN state;
	ALTER TABLE empinfo2 RENAME TO empinfo3;
	ALTER TABLE empinfo2 CHANGE empname name STRING;

	ALTER TABLE students_dynamic PARTITION (country='FRANCE', batch=1) RENAME TO PARTITION (country='France', batch=1);

	-- DONT DO THIS. Use a separate path for partition.
	ALTER TABLE students_static ADD PARTITION (country='JAPAN') LOCATION '/user/hive/warehouse/ctsdb1.db/students_static/';

	ALTER TABLE students_static ADD PARTITION (country='FRANCE');

	ALTER TABLE students_dynamic ADD PARTITION (country='FRANCE', batch=1);

	ALTER TABLE students_dynamic DROP PARTITION (country='FRANCE', batch=1);
	ALTER TABLE students_dynamic DROP PARTITION (country='FRANCE');

	INSERT INTO empinfo2(empid, empname, salary, deptid, city, state)
	VALUES(200, 'Kanakaraju', 10000, 1001, 'Hyderabad', 'Telangana');

	INSERT INTO TABLE students_dynamic
	PARTITION (country='FRANCE', batch=1)

	SELECT id, name, mobile, email, project FROM students
	WHERE country='India';


 views
 ------

 	create view if not exists students_uk as select * from students where country='UK'

 
 hive complex data types
 -----------------------
	Apple/Grapes/Orange,4306:Sneha/4307:Renu,Sneha/90
	Apple/Microsoft/Google,4306:Renu/4307:Anita,Raju/100
	Apple/Micromax/Samsung,4306:Raju/4307:Harsha,Harsha/80

	CREATE TABLE complex_data
	(col1 array<string>,
	 col2 map<int,string>,
	 col3 struct<name:string,score:int>)
	ROW FORMAT DELIMITED 
	FIELDS TERMINATED BY ','
	COLLECTION ITEMS TERMINATED BY '/'
	MAP KEYS TERMINATED BY ':'
	LINES TERMINATED BY '\n'

	LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/hive_complex_data' INTO TABLE complex_data;


 immutable tables
 ----------------
   -> Protect you from accedentally appending data to a non-empty table using INSERT INTO
      command.
   -> It won't protect from "LOAD DATA ..." command.
   -> The behavior of INSERT OVERWRITE is not affected by the "immutable" table property.

	CREATE TABLE emp_tp_immutable
	(empid INT, empname STRING, salary DOUBLE, deptid SMALLINT) 
	ROW FORMAT DELIMITED 
	FIELDS TERMINATED BY ',' 
	LINES TERMINATED BY '\n' 
	STORED AS TEXTFILE 
	TBLPROPERTIES ("immutable"="true")

	CREATE TABLE emp_tp_immutable
	(empid INT, empname STRING, salary DOUBLE, deptid SMALLINT) 
	ROW FORMAT DELIMITED 
	FIELDS TERMINATED BY ',' 
	LINES TERMINATED BY '\n' 
	STORED AS TEXTFILE 
	TBLPROPERTIES ("immutable"="true")

	show tblproperties emp_tp_immutable("immutable");

	LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/data/emp.txt' INTO TABLE emp_tp_immutable;

	INSERT INTO TABLE emp_tp_immutable SELECT * FROM empinfo WHERE empid = 101;

	INSERT OVERWRITE TABLE emp_tp_immutable SELECT * FROM empinfo WHERE empid = 101;

	TRUNCATE TABLE emp_tp_immutable;

	SELECT * FROM emp_tp_immutable;

	INSERT INTO TABLE emp_tp_immutable SELECT * FROM empinfo;

	SELECT * FROM emp_tp_immutable;

	ALTER TABLE emp_tp_immutable SET TBLPROPERTIES ("immutable"="false")

	INSERT INTO TABLE emp_tp_immutable SELECT * FROM empinfo WHERE empid = 101;


 Transactinal tables
 -------------------	

    -> Transactional tables allow updates and deletes
    -> Provides full ACID support. 

    Conditions to create transactional tables
   
     -> make the "transactional" table property as "true"
     -> table must be a bucketted table
     -> storage format should be "ORC"
     -> set some configuration options 

	set hive.support.concurrency = true;
	set hive.enforce.bucketing = true;
	set hive.exec.dynamic.partition.mode = nonstrict;
	set hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
	set hive.compactor.initiator.on = true;
	set hive.compactor.worker.threads = 1;

	CREATE TABLE user_acid (userid int, name string, age int, gender string) 
	CLUSTERED BY (userid) INTO 3 buckets 
	STORED AS ORC
	TBLPROPERTIES('transactional'='true');

	INSERT INTO user_acid(userid, name, age, gender) VALUES 
	(1, 'Raju', 40, 'male'), 
	(2, 'Raghu', 42, 'male'),
	(3, 'Vinay', 20, 'male'), 
	(4, 'Amrita', 14, 'female'), 
	(5, 'Aditya', 14, 'male'), 
	(6, 'Keertana', 12, 'female'), 
	(7, 'Pranav', 24, 'male'), 
	(8, 'Hari', 24, 'male'), 
	(9, 'Syamala', 23, 'female')

	SELECT * from user_acid;

	UPDATE user_acid SET age = 41 where name='Raju';   	 
	DELETE FROM user_acid WHERE userid = 9;			 


  Hive interactive and shell modes
  --------------------------------

	$ hive -e "select * from ctsdb.empinfo"

	$ hive -f /home/cloudera/myscript.hql 


 ===============================================
    Impala
 ===============================================

   -> General purpose SQL engine developed by Cloudera. 
	-> Works for both analytical and transactional loads (interactive queries)
        -> Supports queries that take milliseconds upto hours.

   -> Real time query processing engine for Hadoop
        -> Uses its own processing engine called "Massive Parallel Parocessing (MPP)" database engine.

   -> Runs directly on top of HDFS
	-> Does not use MR
	-> Reads widely used hadoop formats (ORC, Parquet, SequenceFile, Text, AVRO)
	-> Can co-exist with Hadoop cluster 

    -> High Peformance
	-> Written in C++ mostly (some Java)
        -> Used run-time code generation.
	-> completly new execution engine.

    -> Impala uses:
	 -> The same metastore as Hive.
	 -> Uses the same Query language (Hive Query Language)
	 -> Uses the hive warehouse to store its tables.
       

   Meta Data and Table Data in Impala
   -----------------------------------
    => Whenever hive metadata changes outside of impala (may be by hive) that metadata is not automatically
       refreshed in Impala Catalog.
          -> Impala maintains a catalog where it caches all the metadata.
       => To refresh impala catalog use the following command:
	   -> "INVALIDATE METADATA"

    => Whenever new data is added to a table outside of impala, we have to run the following command
       to refresh the table in impala:
	   -> REFRESH <table-name>


   Impala Daemons & Impala Architecture
   ------------------------------------
    1. impalad
	-> runs on every node on the impala cluster
	-> accepts client requests / queries
        -> performs planning, coordination and execution of the client queries
        -> distributes the query execution across many nodes and coordinates them
        -> sends the results back to client.

         Three Processes:
		-> Query Planner
		-> Query Coordinator
		-> Query Executors (distributed across many nodes)

    2. statestored
        -> Runs on only one node in cluster
	-> All the impalad nodes communicate with statestored continuosly
        -> if any node is down, then that info is communicated to all the impala nodes

    3. catalogd
	-> Maintains the metadata
	-> Any changes to the metadata are relayed to all impalad nodes
	-> If metadata changes happen outside of the impala cluster (for ex by hive), those
           changes have to refreshed using "invalidate metadata" command. 



  =======================
    Sqoop
  =======================

      -> Sqoop   => SQL on Hadoop  -> (SQl hadOOP)

      -> Structured Data Ingestion Tool
      -> Built on top of MapReduce.

      -> Using Sqoop, you can import data from external SQL datastores (RDBMS) to Hadoop native data 
         stores (HDFS, Hive, HBase). 

      -> Sqoop launches multiple map-only tasks on the cluster to efficiently and parallely transfer
         bulk data between RDBMS and Hadoop systems. 


sqoop list-databases --connect jdbc:mysql://localhost:3306 --username root -password cloudera

sqoop list-tables --connect jdbc:mysql://localhost:3306/ctsdb --username root -P

sqoop list-tables --connect jdbc:mysql://localhost:3306/ctsdb --connection-param-file /home/cloudera/sqoop-properties.properties

sqoop import --connect jdbc:mysql://localhost:3306/retail_db --username root --table categories -m 1 --target-dir /user/cloudera/sqoop/categories --fields-terminated-by ',' --lines-terminated-by '\n' --password cloudera

sqoop import --connect jdbc:mysql://localhost:3306/retail_db --username root --table orders -m 1 --target-dir /user/cloudera/sqoop/orders_tab --fields-terminated-by '\t' --lines-terminated-by '\n' --password cloudera

--append mode
sqoop import --connect jdbc:mysql://localhost:3306/retail_db --username root --password cloudera --table categories -m 1 --target-dir /user/cloudera/sqoop/categories --fields-terminated-by ',' --lines-terminated-by '\n' --append

--delete mode
sqoop import --connect jdbc:mysql://localhost:3306/retail_db --username root --password cloudera --table categories -m 1 --target-dir /user/cloudera/sqoop/categories --fields-terminated-by ',' --lines-terminated-by '\n' --delete-target-dir

-- specific columns
sqoop import --connect jdbc:mysql://localhost:3306/retail_db --username root --password cloudera --table products --columns 'product_id,product_category_id,product_name' -m 1 --target-dir /user/cloudera/sqoop/products_pipe --fields-terminated-by '|'

--using a query
sqoop import --connect jdbc:mysql://localhost:3306/retail_db --username root --password cloudera --query 'select * from products where product_category_id=59 and $CONDITIONS' -m 1 --target-dir /user/cloudera/sqoop/products_59 

--enclosed-by
sqoop import --connect jdbc:mysql://localhost:3306/retail_db --username root --password cloudera --table categories -m 1 --target-dir /user/cloudera/sqoop/categories_quotes --fields-terminated-by ',' --lines-terminated-by '\n' --enclosed-by '"'

--multiple mappers
sqoop import --connect jdbc:mysql://localhost:3306/retail_db --username root --password cloudera --table categories -m 3 --target-dir /user/cloudera/sqoop/categories_3 

sqoop import --connect jdbc:mysql://localhost:3306/demodb --username root --password cloudera --table categories -m 2 --target-dir /user/cloudera/sqoop/categories_3_demodb --split-by category_department_id --delete-target-dir

-- import as sequence-file

sqoop import --connect jdbc:mysql://localhost:3306/retail_db --username root --password cloudera  --table categories --num-mappers 1 --target-dir /user/cloudera/sqoop/categories_seq --as-sequencefile

-- import as avro-file
sqoop import --connect jdbc:mysql://localhost:3306/retail_db --username root --password cloudera  --table categories --num-mappers 1 --target-dir /user/cloudera/sqoop/categories_avro --as-avrodatafile


===  importing all tables in the database ===

sqoop import-all-tables --connect jdbc:mysql://localhost:3306/demodb --username root --password cloudera -m 1 --warehouse-dir /user/cloudera/sqoop/demodb_db


=== Importing data into Hive ===

sqoop create-hive-table --connect jdbc:mysql://localhost:3306/demodb --username root --password cloudera --table students --fields-terminated-by ',' --hive-table ctshadoopdb.students



sqoop import --connect jdbc:mysql://localhost:3306/demodb --table students --username root --password cloudera --hive-table ctshadoopdb.students -m 1 --fields-terminated-by ',' --hive-import 


========== EXPORT ===========

CREATE TABLE emp(empid int, empname varchar(25), salary double, deptid int);

CREATE TABLE students (id int, name varchar(50), mobile varchar(50), email varchar(50), project varchar(50), country varchar(50), batch int)
-------

sqoop export --connect jdbc:mysql://localhost:3306/demodb --table categories --username root --password cloudera -m 1  --export-dir sqoop/categories/part-m-00000 --input-fields-terminated-by ','  

sqoop export --connect jdbc:mysql://localhost:3306/demodb --table categories --username root --password cloudera -m 1  --export-dir sqoop/categories/part-m-00000 --input-fields-terminated-by ',' --columns 'category_id,category_department_id'

sqoop export --connect jdbc:mysql://localhost:3306/demodb --table categories --username root --password cloudera -m 1  --export-dir /user/cloudera/sqoop/categories_avro 

sqoop export --connect jdbc:mysql://localhost:3306/demodb --table categories --username root --password cloudera -m 1  --export-dir /user/cloudera/sqoop/categories_seq

sqoop export --connect jdbc:mysql://localhost:3306/exportdb --table employee --username root --password cloudera -m 1 --export-dir /user/cloudera/sqoop/upsert/emp2.txt --update-key 'empid' --update-mode 'allowinsert'

NOTE: --update-mode has two possible values: 'updateonly', 'allowinsert'


========== eval ==========

sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root --query 'select * from users'

sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root --query 'create table tab1(c1 int, c2 varchar(50))'

sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root --query 'insert into tab1 (c1, c2) values (1, "raju"), (2, "ramesh")'

sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root --query 'update tab1 set c2 = "kanakaraju" where c1=1'

sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root --query 'delete from tab1 where c1=1'

sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root --query 'drop table tab1'

============= codegen ============

sqoop codegen --connect jdbc:mysql://localhost:3306/ctsdb --table users --class-name UserTable



======================
    HBASE
======================

   RDBMS Database Limitations:
   ---------------------------

      -> Scalability limitations
	  -> They run on a single machine
	  -> Can not support unlimited scalability.
          -> Not designed as distributed databases.

      -> Does not support dynamic schemas
	  -> RDBMSs have fixed schema
          -> All rows of a table have same number of columns.
	  -> Does not support schema evolution. 
   

   NoSQL Databases
   ---------------
      -> Designed as distributed databases.
	  -> All NoSQL databases run on a cluster.
      -> Uses denormalized data models
	  -> Non-relational
	  -> All the data of a single entity is stored in the same table
          -> No joins
      -> Schema-less (flexible schema)
	  -> Each row need not have same number of columns
          -> Columns need not be part of the table structure.  
          -> Supports applications with schema evolution


   Types of NoSQL databases
   ------------------------
	
        1. Column (column-family) orieted databases (Ex: HBase, Cassandra)
	2. Document oriented databases (ex: MongoDB, CouchDB, DynamoDB)
	3. Key-value Stores (ex: Redis)
	4. Graph Databases (ex: Neo4J)
  
     
   Limitations of Hadoop
   ---------------------         
      -> Unstructured Data
            -> HDFS does not have any schema
	    -> They are just files
	
      -> No Random Access supported
	    -> Even to read a single record, you have process the entire file.
         
      -> High Latency
	    -> MR does not have throughput to support transactional databases.

       -> Not ACID complaint
	    -> Databases are reliable source of data because of ACID guarantees. 

    HBase   
    -----
        -> Is a distributed database management system that provides transactional processing
	    -> Part of Hadoop ecosystem.
	
        -> Three seminal papers of Google (published bw 2003 & 2006)
	     -> Google File System	: HDFS
	     -> MapReduce		: MapReduce
	     -> Google Big Table	: HBase


	-> HBase uses HDFS to store persistent data
		-> distributed storage
		-> fault tolerant.


   Features of HBase
   -----------------
	-> Is a column-family oriented database
	-> HBase does support SQL
	-> Denormalized data design
	    -> We don't normalize table
	    -> Store all data in a single table 
		-> organize data as column-families
	-> Uses column-oriented storage
	    -> more like a map than a table (in RDBMS sense)
	-> HBase only supports a basic set of operations (CRUD) on a "single-row"
	    -> All operations are applied at "row-level"
	    -> No Joins
	    -> No Group By (aggregartions)
	    -> No Sub-Queries
	-> Very limited ACID complience
	    -> ACID complience is at row-level.


   Working with HBase
   ------------------

    1. Client Application (using Java API)
    2. HBase Shell
    3. Web Clients (using thrift service) - HUE


    HBase Data Model
    ----------------

	-> The basic unit of data is a "column" (or cell)

	-> A row is identified by an "row-id"
	-> Every column belongs a column-family and has a row-id attached to it
		-> all the columns with the same row-id make up a row
	-> Each row can have its own set of columns.
	-> Each column will have 
		-> A row-id
		-> A column-family
		-> A value
		-> A timestamp (generated by Hbase)

		-> A cell is a key-value pair
			Key: (Row-Id, Col-Identifier)   col-id => column-family:column-name
			Value: Value, TimeStamp

			Key: (1, 'basic:name')
			     ("raju", 16020456345237)

	-> Each cell is versioned.
		-> A cell added with the same row-id and same column-identifier with 
		   a different value will result in a new version of the cell being created with
		   a different (latest) timestamp.
		-> version is identified by a timestamp

	-> The number of versions you want to maintain can be configured for each column-family
		-> default is 1.

   	-> Basic unit of data is a cell
	    -> We can insert one cell
	    -> We can update one cell
	    -> We can delete one cell
                  -> UNlike RDBMS, where we work with rows.

  	NOTE:  Apache HBase is usually used along with Apache Phoenix, which provides an SQL-like interface for developers. 

	----------------------------
	help

	list     // list the tables

	// create a table
	create 'emp2', 'basic', 'address', 'contact', 'other'

	create 'users', {NAME => 'basic', VERSIONS => '3'}, {NAME => 'address', VERSIONS => '2'}, {NAME => 'contact', VERSIONS => '3'}, {NAME => 'other', VERSIONS => '1'}

	// describe the table
	describe 'emp2'

	// insert data into the table 

	put 'users', '6', 'basic:userid', '106'
	put 'users', '6', 'basic:name', 'Komala'
	put 'users', '6', 'address:street', 'DU 170, Jalvayu Vihar'
	put 'users', '6', 'address:city', 'Hyderabad'
	put 'users', '6', 'address:state', 'Telangana'
	put 'users', '6', 'contact:phone', '8500010100'
	put 'users', '6', 'contact:email', 'koamla@gmail.com'
	put 'users', '6', 'other:habbies', 'reading, tv'
	put 'users', '6', 'other:facebookid', 'blah blah'

	put 'users', '7', 'basic:userid', '107'
	put 'users', '7', 'basic:name', 'Kanakaraju'
	put 'users', '7', 'basic:age', '45'
	put 'users', '7', 'address:line1', 'DU 170, Jalvayu Vihar'
	put 'users', '7', 'address:city', 'Hyderabad'
	put 'users', '7', 'contact:phone1', '9246657888'
	put 'users', '7', 'contact:email1', 'kanak@gmail.com'
	put 'users', '7', 'other:habby', 'reading, tv'

	put 'users', '8', 'basic:userid', '108'
	put 'users', '8', 'other:linkedin', 'blah blah blah...'
	put 'users', '8', 'address:city', 'Hyderabad'

	put 'users', 'ROW10', 'basic:userid', '110'
	put 'users', 'ROW10', 'other:linkedin', 'blah blah blah...'
	put 'users', 'ROW10', 'address:city', 'Hyderabad'

	// scan the table (fetch all rows - optionally with some filters)
	scan 'users'
	scan 'users', { COLUMNS=>['basic:userid', 'basic:name'] }
	scan 'users', { COLUMNS => 'basic:userid', TIMERANGE => [1616069871500, 1616070699434] }

	scan 'users', {COLUMNS => ['basic:userid', 'basic:name'], LIMIT => 10, STARTROW => '2', ENDROW => '8'}  

	scan 'users', { FILTER => SingleColumnValueFilter.new(Bytes.toBytes('basic'),Bytes.toBytes('name'), CompareFilter::CompareOp.valueOf('EQUAL'),BinaryComparator.new(Bytes.toBytes('Kanakaraju')))}

	// get - to fetch one row.
	get 'users', '5', {COLUMN => 'basic:userid', VERSIONS => 3}

	get 't1', 'r1'
	get 't1', 'r1', {TIMERANGE => [ts1, ts2]}
	get 't1', 'r1', {COLUMN => 'c1'}
	get 't1', 'r1', {COLUMN => ['c1', 'c2', 'c3']}
	get 't1', 'r1', {COLUMN => 'c1', TIMESTAMP => ts1}
	get 't1', 'r1', {COLUMN => 'c1', TIMERANGE => [ts1, ts2], VERSIONS => 4}
	get 't1', 'r1', 'c1'
	get 't1', 'r1', 'c1', 'c2'
	get 't1', 'r1', ['c1', 'c2']


	// add/delete column families
	alter 'users', NAME => 'cf10', VERSIONS => 5
	alter 'users', NAME => 'other', METHOD => 'delete'


	// delete cells
	delete 'users', '10', 'address:city'

	// truncate the table
	truncate 'users'


	// drop the table
	disable 'users'
	drop 'users'



  Hive - HBase Integration
  ------------------------

   -> We can create a  table in Hive using Hbase as the storoge Handler (HbaseStorageHandler)
   -> We can map the columns of the hive table with a hbase table
   -> We can perform update, delete and random data access etc on hive table using hbase.

	$ hive --auxpath /usr/lib/hive/lib/hive-hbase-handler-0.10.0-cdh4.7.0.jar,/usr/lib/hive/lib/hbase.jar,/usr/lib/hive/lib/zookeeper.jar,/usr/lib/hive/lib/guava-11.0.2.jar


	CREATE TABLE hiveempinfo (key int, empname string, salary double) 
	STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' 
	WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,emp:empname,emp:salary") 
	TBLPROPERTIES ("hbase.table.name" = "hbaseempinfo");


	=> In the above command:
	   -> The first column (key) is mapped to hbase row-key identified by :key
	   -> The second column (empname) is mapped to hbase column emp:empname, 
		  where 'emp' is the CF and empname is the column.
	   -> The third column (salary) is mapped to hbase column emp:salary, where 'emp' is the CF and salary is the column.
	   -> The hbaseempinfo table is automatically created. 

	# load data into hive (deserialized) table just created
	hive> INSERT OVERWRITE TABLE hiveempinfo SELECT empid, empname, salary FROM empinfo;

	NOTE: when you load data into the hive table the corresponding HBase table is also populated 
	with the same data. Now, you can do DML operations in HBase and see the changes reflected in Hive.


  HBase Architecture
  ------------------

   Google published three seminal papers between 2003 and 2006

    1. Google File System	: HDFS
    2. MapReduce			: MapReduce
    3. Google Big Table		: HBase   


  => HBase is a "sorted nested map": <Row-ID, <Column-Family, <Column, <TimeStamp Value>>>
  
  => When you read data from hbase, it performs a random-access lookup on the row-id
  => When you write data to hbase, hbase inserts the row in the 'right-place'
	-> These are possible with "regions"
  
  Region => Is a contiguous range of row-ids.
	    Each region is stored in a region-server.
            Serve as an index to perform fast lookup for where a row belongs to. 
	    Regions are distributed across the cluster among many region-servers
           
  Region-Server => run on all the slave nodes of hbase cluster
		   host zero or more regions
 

  With in each region-server:

      1. WAL (write-ahead-log)  - hdfs file which records all incoming data changes (writes/updates)
      2. MemStore  - one per column-family
		     in-memory buffer where all data changes are recorded
		     write-buffer
      3. HFile - periodically, memstore contents are flushed to HFiles (stored in HDFS)
      4. Block Cache - maintains an index of row-keys to HFile blocks
		       used for fast-random access.




















	 