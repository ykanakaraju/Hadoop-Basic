
  Agenda (20 session * 4 hrs)   -  kanakaraju.y@cognizant.com
  ---------------------------
    -> Big Data - Concepts
    -> Hadoop
	 -> HDFS
	 -> MapReduce
	 -> YARN
    -> Hive (Data warehousing platform)
    -> Impala (introduction)
    -> Sqoop (export & import tool)
    -> HBase (NoSQL database)
    -> Kafka (PubSub messaging engine)
    -> NiFi (dataflow manager)
    -> Spark ( PySpark )

 ==========================================================
    Materials:

	-> Presentations (PDFs)
	-> Code - Command sets & Programs
        -> Class Notes

        -> https://github.com/ykanakaraju/Hadoop-Basic
 ==========================================================      

    Big Data 
    --------

       Big Data is data that contains greater variety (unstructured & semu structured) arriving at
       increasing volumes and with velocity. (3 Vs of Big Data)   

	-> Volume
		-> Amount of data. We will have to process huge volumes of low-density & 
		   unstructured data. 
	-> Velocity
		-> Velocity is the fast rate at which data is received and act upon it. 

	-> Variety
 		-> Unstructured & Semi-structured data such as text, audio, video require 
		   additional preprocessing to derive meaning and support metatdata.


   Computing Cluster
   -----------------
        -> Is a unified entity containing a group of nodes whose cumulative resources can be used
	   to store and process huge amounts of data.	
    
   Hadoop
   ------
	-> Is an open source framework to store and process bigdata on a cluster 
	   made of commodity hardware. 

	-> Distributed Storage    : HDFS (Hadoop Distributed File System)
	-> Distributed Processing : MapReduce
	-> Cluster Manager	  : YARN  (added to Hadoop from Hadoop 2.0 onwards)

	=> Written in Java


   HDFS (Hadoop Distributed File System)
   -------------------------------------
	
      => Distributed Storage Framework

      => Data is split into blocks of 128 MB each and are distributed across many machines.
	-> The blocks are distributed across many slave nodes
	-> Each blocks is replicated 3 times (default) on three different node for providing fail-safety.


      HDFS Architecture
      -----------------

	-> Name Node (NN)
		-> Runs on the master machine

		-> Maintains all the meta data in two files:
			-> EditLog (in-memory)
			-> FSImage (persistent file, stored on the disk)

	-> Data Node (DN)
		-> Runs on all the slave nodes.
		-> Send heart-beats to Name node once in 3 sec.
		    -> If three successive heartbeats are missed, then the datanode is considered as 
		       unavailable. 
		-> Data Nodes send block-reports to NN

			-> Block Report:
				-> Info about all the files and directories
				-> Available Disk Space, Total disk space, reserved disk space

	-> Secondary Name Node (SNN)
		-> Responsible for check Poinitng


        -> Balancer
		-> Balancer maintains the replication factor of file.
		-> Is a background process the periodically checks for under & over repicated blocks
		   and maintains the replication.

 
    Hadoop EcoSystem
    ----------------
	=> Various different components that provide different services and run on
	   top of Hadoop make up Hadoop-Ecosystem. 

		
		Execution Engines	: Mapreduce, Spark, Tez
		Analytical Process	: Hive, Drill, Impala
		ML Libraries		: Spark MLlib, Mahout
		Web Indexing & Search	: Lucene, Solr
		Messaging Engines & Streaming : Kafka, Storm
		Coordination Mgmt	: Zookeeper, Ambari
		Data Ingestion		: Sqoop, Flume


    Hadoop Distributions
    --------------------

	Distributions package lot of Hadoop Components as one unified distribution and provide
	management tools to make managing hadoop clusters easy and convinient.

	On-Premisis Clusters
	---------------------
	-> Cloudera 
	-> HortonWorks (merged with Cloudera)
	-> MapR

	Cloud Services
	--------------
	-> Amazon's AWS EMR (Elastic Map Reduce - managed Hadoop service)
	-> Microsoft Azure HDInsights (managed)
	-> Google's GCP DataProc (managed)


   Hadoop Config Files
   --------------------
	-> core-site.xml
	-> hdfs-site.xml
	-> mapred-site.xml


   HDFS File Operation - Working with HDFS
   ---------------------------------------
	=> Home Directory on Linux File System : /home/cloudera	
	=> Home directory on HDFS File System  : /user/cloudera

        Commands:  hadoop fs -<command>  OR  
		   hdfs dfs -<command> 
	
      
    Working with HDFS
    -----------------
	hadoop fs -ls
	hadoop fs -ls <dir>
	hadoop fs -ls -R <dir>   => recursive listing

	hadoop fs -mkdir <dir>
	hadoop fs -rmdir <dir>    => remove an empty dir

	hadoop fs -rm <file-Path>
	hadoop fs -rm -R <non-empty-directory>   => deletes the dir with all its content.
		ex: hadoop fs -rm -R dir1/datasets

	hadoop fs -cp <source-file-path> <destination-dir>

	hadoop fs -cat <file-path>

	hadoop fs -chmod 755  <filePath>
	hadoop fs -chmod -R 755 <DirPath>  (you need execute permisiion on the files)

	hadoop fs -getmerge dir1/*.txt localdir/dir1merged.txt
		=> Merged multiple HDFS files and copy that as one local-file

	hadoop fs -stat %o dir1/NYSE_dividends.txt
		=> get the stats of a file  (refer   hadoop fs -help stat)

	hadoop fs -setrep 5 dir1/NYSE_dividends.txt
	hadoop fs -setrep 5 dir1/*.txt
		=> Set the replication factor of a file


	Linux <=> HDFS
        ---------------	
	-copyFromLocal    ( copy from Linux to HDFS )
	-put

	-moveFromLoacl 	  ( move from Linux to HDFS )

	-copyToLocal	  ( copy from HDFS to Linux )
	-get	

	examples:
       
	hadoop fs -copyFromLocal <local-source> <hdfs-destination>
	     	hadoop fs -copyFromLocal datasets/NYSE_dividends.txt
		hadoop fs -copyFromLocal datasets/NYSE_dividends.txt dir1

	hadoop fs -D dfs.replication=5 -copyFromLocal <source> <destination>

	hadoop fs -put <local-source> <hdfs-destination>
		hadoop fs -put datasets/NYSE_dividends.txt
		hadoop fs -put datasets/NYSE_dividends.txt dir1
	
	hadoop fs -copyToLocal <hdfs-source> <local-destination>
	hadoop fs -get <hdfs-source> <local-destination>
		hadoop fs -copyToLocal dir1/datasets/wordcount_input.txt localdir
		hadoop fs -get dir1/datasets localdir

	hadoop fs -moveFromLocal localdir/NYSE_daily.txt dir2
	   
   ============================================
      YARN 
   ============================================

      -> YARN (Yet another resource negotiator)
      -> YARN is the default cluster manager for Hadoop.
      -> Introduced in Hadoop 2.0 as the Cluster Manager.

	
      -> YARN accepts job submissions, schedules them and allocate resource containser to 
	 different jobs


 	Basic Components of YARN :

	1. Resource Manager : One per cluster, master daemon
	2. Node Manager: One per node, slave daemon
	3. Application Master: One per application/job
			      (starts when job starts, and closed when the job is completed)
	4. Containers: Application processes where App Master sends the tasks to be executed.

	Typical application execution with YARN follows this flow:
	   => Client program submits the MR application to the RM, along with information to launch 
	      the application-specific Application Master.
	   => RM negotiates a container for the AM and launches the AM.
	   => AM boots and registers with the RM, allowing the original calling client to interface 
	      directly with the AM.
	   => AM negotiates resources (resource containers) for client application.
	   => AM gives the container launch specification to the NM, which launches a container for 
	      the application.
	   => During execution, client polls AM for application status and progress.
	   => Upon completion, AM deregisters with the RM and shuts down.
  

   ============================================
      MapReduce  (MR)
   ============================================

    -> MR is the default distributed parallel processing framework of Hadoop
 
   
    MapReduce
     
      -> MapReduce Execution Framework (How an MR program is executed on the cluster)

	1. Map Phase		
		-> Several map tasks are launched across the cluster
		-> Each mapper processes one input-split
		-> RecordReader creates (K, V) from each record in the inputsplit and calls map method
		-> Map method processes that record and writes the output as (K, V) pairs to the context
		-> map() methos is called as many times as there are records in the inputsplit.

	2. Reduce Phase
		2.1 Shuffle & Sort phase

		-> Will aggregate all the values of each unique key from across the outputs of all mappers
		-> Will have unique keys and grouped values (ex: ('hadoop', [1,1,1,,1,1,1 ...])
		-> The keys are automatically sorted. 

		2.2 Reducer Phase
		-> The output of the S&S will be divided into as many partitions as there are reducers.
			-> By default, system gives only one reducers
			-> But, we can configure number of reducers in code  (wordcountjob.setNumReduceTasks(2);)
		-> Will process each (K, Iterable[V]) pair and produces final output as (K, V) pairs.

      -> MapReduce Programming Paradigm (How to write a MR program)   

	 MapReduce Application Classes (that a developer has to write)

		1. Driver Class (required)
		     => Where you define all the application configurations.

		2. Mapper Class (required)
		     => Extends "Mapper" base class and implements some life cycle methods:
		     => Implements "map" method, where you write you application logic.		     

		3. Reducer Class (required only if aggregation is needed)
		     => Extends "Reducer" base class and implements some life cycle methods.
		     => Implements "reduce" method, where you write you application logic.

		4. ?
		     

		5. ?


    Hadoop Data Types
    -----------------
   
      => To have better serialization & deserialization of data, Hadoop uses its own custom 
	 data types. These are called Writable data-types. 

	    Java	    Hadoop
	   -------         --------------
	    String	   Text
	    Integer	   IntWritable
	    Long	   LongWritable
	    Double	   DoubleWritable
	    Bool	   BoolWritable
	    null	   NullWritable
	
	
   InputSplits
   -----------
	=> InputSplits are created by the framework as an integral number of records
	=> InputSplits are logical entities (pointer to the byte-positions)
	=> Created by the Framework.

	=> InputSplits  (represented by an "InputSplit") are created by "InputFormat" class.

	
   RecordReader
   ------------
	=> Creates (K, V) pairs from each record of the input-split and calls the "map" method
	   of the mapper program. 


 input-split (InputSplit)			  RecordReader
 -------------------------                         ------------

     hadoop hive mapreduce hdfs hadoop mapreduce  -> (0,"hadoop hive mapreduce hdfs hadoop mapreduce")      
						  ==>  map() -> (hadoop, 1) (hive, 1) (mapreduce, 1) (hdfs, 1) ..
		
    mapper reducer sqoop flume hadoop mapreduce   -> (51, "mapper reducer sqoop flume hadoop mapreduce")
						  ==>  map() -> (mapper, 1) (reducer, 1) (sqoop, 1) (flume, 1) ..

    mapreduce hive flume kafka hadoop hive	  -> (102, "mapreduce hive flume kafka hadoop hive")
						  ==>  map() -> (mapreduce, 1) (hive, 1) (flume, 1) (kafka, 1) ..



  Executing the MapReduce Program
  -------------------------------
	
      -> Export MR application as a Java Jar file

      -> hadoop jar <jar-file-path> <Driver-Class-Name> <Command-line-args>
	 hadoop jar /home/cloudera/wordcount.jar com.cts.WordCountJob wordcount_input.txt wcoutwordcount_input.txt



   Creating a MR program
   ---------------------

	-> Create a Java project

	-> Add the following JAR file to the Referenced Libraries:
		-> Right click on  Referenced Libraries -> Build Path -> Configure Build Path
		-> Click on "Add External JARS" and add the following:
			/usr/lib/hadoop/hadoop-common.jar
			/usr/lib/hadoop-0.20-mapreduce/hadoop-core.jar

	-> Create packages and start write MR programs. 


  Mapper Life Cycle Methods
  -------------------------

    1. setup
	 -> Executed once at the beginning (even before map method is called)
	 -> Used for initialization, creating resources (like DB connections etc).

    2. map
	-> Executed once (after setup method) for every input (K, V) pair in the input-split
	-> Implement the buisiness logic to be executed per record.

    3. cleanup
	-> Executed once after map is called for all records
	-> Write any consolidated output if you have.
	-> Close the connections, clean the resources etc. 

    4. run
	-> Generally not implemented.	


  Reducer Life Cycle Methods
  -------------------------

    1. setup
	 -> Executed once at the beginning (even before reduce method is called)
	 -> Used for initialization, creating resources (like DB connections etc).

    2. reduce
	-> Executed once (after setup method) for every input (K, Iterable[V]) pair. 
	-> Implement the buisiness logic to be executed per record.

    3. cleanup
	-> Executed once after reduce methods is called for all records
	-> Write any consolidated output if you have.
	-> Close the connections, clean the resources etc. 

    4. run
	-> Generally not implemented.	

  
   Use-Case: 
   ---------
	From pseudo_facebook.tsv file find out the record with maximum value of 'likes_received'
	from among peoplw who are born between 1990 and 2000.

	Two MR programs:

	MR 1. Fetch the output for people born between 1990 and 2000
	MR 2. Fetch the record with highest 'likes_received' value from the output of MR 1


   ==================================
      Hive
   ==================================	
  
       


   
  



 




















