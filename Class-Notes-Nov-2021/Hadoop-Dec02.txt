 
  Agenda (20 session * 4 hrs) 
  ---------------------------
    -> Big Data - Concepts
    -> Hadoop
	 -> HDFS
	 -> MapReduce
	 -> YARN
    -> Hive (Data warehousing platform)
    -> Impala (introduction)
    -> Sqoop (export & import tool)
    -> HBase (NoSQL database)
    -> Kafka (PubSub messaging engine)
    -> NiFi (dataflow manager)
    -> Spark ( PySpark - Spark Core & Spark SQL )

 ==========================================================
    Materials:

	-> Presentations (PDFs)
	-> Code - Command sets & Programs
        -> Class Notes

        -> https://github.com/ykanakaraju/Hadoop-Basic
 ==========================================================      

    Big Data 
    --------

       Big Data is data that contains greater variety (unstructured & semu structured) arriving at
       increasing volumes and with velocity. (3 Vs of Big Data)   

	-> Volume
		-> Amount of data. We will have to process huge volumes of low-density & 
		   unstructured data. 
	-> Velocity
		-> Velocity is the fast rate at which data is received and act upon it. 

	-> Variety
 		-> Unstructured & Semi-structured data such as text, audio, video require 
		   additional preprocessing to derive meaning and support metatdata.


   Computing Cluster
   -----------------
        -> Is a unified entity containing a group of nodes whose cumulative resources can be used
	   to store and process huge amounts of data.	
    
   Hadoop
   ------
	-> Is an open source framework to store and process bigdata on a cluster 
	   made of commodity hardware. 

	-> Distributed Storage    : HDFS (Hadoop Distributed File System)
	-> Distributed Processing : MapReduce
	-> Cluster Manager	  : YARN  (added to Hadoop from Hadoop 2.0 onwards)

	=> Written in Java


   HDFS (Hadoop Distributed File System)
   -------------------------------------
	
      => Distributed Storage Framework

      => Data is split into blocks of 128 MB each and are distributed across many machines.
	-> The blocks are distributed across many slave nodes
	-> Each blocks is replicated 3 times (default) on three different node for providing fail-safety.


      HDFS Architecture
      -----------------

	-> Name Node (NN)
		-> Runs on the master machine

		-> Maintains all the meta data in two files:
			-> EditLog (in-memory)
			-> FSImage (persistent file, stored on the disk)

	-> Data Node (DN)
		-> Runs on all the slave nodes.
		-> Send heart-beats to Name node once in 3 sec.
		    -> If three successive heartbeats are missed, then the datanode is considered as 
		       unavailable. 
		-> Data Nodes send block-reports to NN

			-> Block Report:
				-> Info about all the files and directories
				-> Available Disk Space, Total disk space, reserved disk space

	-> Secondary Name Node (SNN)
		-> Responsible for check Poinitng


        -> Balancer
		-> Balancer maintains the replication factor of file.
		-> Is a background process the periodically checks for under & over repicated blocks
		   and maintains the replication.

 
    Hadoop EcoSystem
    ----------------
	=> Various different components that provide different services and run on
	   top of Hadoop make up Hadoop-Ecosystem. 

		
		Execution Engines	: Mapreduce, Spark, Tez
		Analytical Process	: Hive, Drill, Impala
		ML Libraries		: Spark MLlib, Mahout
		Web Indexing & Search	: Lucene, Solr
		Messaging Engines & Streaming : Kafka, Storm
		Coordination Mgmt	: Zookeeper, Ambari
		Data Ingestion		: Sqoop, Flume


    Hadoop Distributions
    --------------------

	Distributions package lot of Hadoop Components as one unified distribution and provide
	management tools to make managing hadoop clusters easy and convinient.

	On-Premisis Clusters
	---------------------
	-> Cloudera 
	-> HortonWorks (merged with Cloudera)
	-> MapR

	Cloud Services
	--------------
	-> Amazon's AWS EMR (Elastic Map Reduce - managed Hadoop service)
	-> Microsoft Azure HDInsights (managed)
	-> Google's GCP DataProc (managed)


   Hadoop Config Files
   --------------------
	-> core-site.xml
	-> hdfs-site.xml
	-> mapred-site.xml


   HDFS File Operation - Working with HDFS
   ---------------------------------------
	=> Home Directory on Linux File System : /home/cloudera	
	=> Home directory on HDFS File System  : /user/cloudera

        Commands:  hadoop fs -<command>  OR  
		   hdfs dfs -<command> 
	
      
    Working with HDFS
    -----------------
	hadoop fs -ls
	hadoop fs -ls <dir>
	hadoop fs -ls -R <dir>   => recursive listing

	hadoop fs -mkdir <dir>
	hadoop fs -rmdir <dir>    => remove an empty dir

	hadoop fs -rm <file-Path>
	hadoop fs -rm -R <non-empty-directory>   => deletes the dir with all its content.
		ex: hadoop fs -rm -R dir1/datasets

	hadoop fs -cp <source-file-path> <destination-dir>

	hadoop fs -cat <file-path>

	hadoop fs -chmod 755  <filePath>
	hadoop fs -chmod -R 755 <DirPath>  (you need execute permisiion on the files)

	hadoop fs -getmerge dir1/*.txt localdir/dir1merged.txt
		=> Merged multiple HDFS files and copy that as one local-file

	hadoop fs -stat %o dir1/NYSE_dividends.txt
		=> get the stats of a file  (refer   hadoop fs -help stat)

	hadoop fs -setrep 5 dir1/NYSE_dividends.txt
	hadoop fs -setrep 5 dir1/*.txt
		=> Set the replication factor of a file


	Linux <=> HDFS
        ---------------	
	-copyFromLocal    ( copy from Linux to HDFS )
	-put

	-moveFromLoacl 	  ( move from Linux to HDFS )

	-copyToLocal	  ( copy from HDFS to Linux )
	-get	

	examples:
       
	hadoop fs -copyFromLocal <local-source> <hdfs-destination>
	     	hadoop fs -copyFromLocal datasets/NYSE_dividends.txt
		hadoop fs -copyFromLocal datasets/NYSE_dividends.txt dir1

	hadoop fs -D dfs.replication=5 -copyFromLocal <source> <destination>

	hadoop fs -put <local-source> <hdfs-destination>
		hadoop fs -put datasets/NYSE_dividends.txt
		hadoop fs -put datasets/NYSE_dividends.txt dir1
	
	hadoop fs -copyToLocal <hdfs-source> <local-destination>
	hadoop fs -get <hdfs-source> <local-destination>
		hadoop fs -copyToLocal dir1/datasets/wordcount_input.txt localdir
		hadoop fs -get dir1/datasets localdir

	hadoop fs -moveFromLocal localdir/NYSE_daily.txt dir2
	   
   ============================================
      YARN 
   ============================================

      -> YARN (Yet another resource negotiator)
      -> YARN is the default cluster manager for Hadoop.
      -> Introduced in Hadoop 2.0 as the Cluster Manager.

	
      -> YARN accepts job submissions, schedules them and allocate resource containser to 
	 different jobs


 	Basic Components of YARN :

	1. Resource Manager : One per cluster, master daemon
	2. Node Manager: One per node, slave daemon
	3. Application Master: One per application/job
			      (starts when job starts, and closed when the job is completed)
	4. Containers: Application processes where App Master sends the tasks to be executed.

	Typical application execution with YARN follows this flow:
	   => Client program submits the MR application to the RM, along with information to launch 
	      the application-specific Application Master.
	   => RM negotiates a container for the AM and launches the AM.
	   => AM boots and registers with the RM, allowing the original calling client to interface 
	      directly with the AM.
	   => AM negotiates resources (resource containers) for client application.
	   => AM gives the container launch specification to the NM, which launches a container for 
	      the application.
	   => During execution, client polls AM for application status and progress.
	   => Upon completion, AM deregisters with the RM and shuts down.
  

   ============================================
      MapReduce  (MR)
   ============================================

    -> MR is the default distributed parallel processing framework of Hadoop
 
   
    MapReduce
     
      -> MapReduce Execution Framework (How an MR program is executed on the cluster)

	1. Map Phase		
		-> Several map tasks are launched across the cluster
		-> Each mapper processes one input-split
		-> RecordReader creates (K, V) from each record in the inputsplit and calls map method
		-> Map method processes that record and writes the output as (K, V) pairs to the context
		-> map() methos is called as many times as there are records in the inputsplit.

	2. Reduce Phase
		2.1 Shuffle & Sort phase

		-> Will aggregate all the values of each unique key from across the outputs of all mappers
		-> Will have unique keys and grouped values (ex: ('hadoop', [1,1,1,,1,1,1 ...])
		-> The keys are automatically sorted. 

		2.2 Reducer Phase
		-> The output of the S&S will be divided into as many partitions as there are reducers.
			-> By default, system gives only one reducers
			-> But, we can configure number of reducers in code  (wordcountjob.setNumReduceTasks(2);)
		-> Will process each (K, Iterable[V]) pair and produces final output as (K, V) pairs.

      -> MapReduce Programming Paradigm (How to write a MR program)   

	 MapReduce Application Classes (that a developer has to write)

		1. Driver Class (required)
		     => Where you define all the application configurations.

		2. Mapper Class (required)
		     => Extends "Mapper" base class and implements some life cycle methods:
		     => Implements "map" method, where you write you application logic.		     

		3. Reducer Class (required only if aggregation is needed)
		     => Extends "Reducer" base class and implements some life cycle methods.
		     => Implements "reduce" method, where you write you application logic.

		4. ?
		     

		5. ?


    Hadoop Data Types
    -----------------
   
      => To have better serialization & deserialization of data, Hadoop uses its own custom 
	 data types. These are called Writable data-types. 

	    Java	    Hadoop
	   -------         --------------
	    String	   Text
	    Integer	   IntWritable
	    Long	   LongWritable
	    Double	   DoubleWritable
	    Bool	   BoolWritable
	    null	   NullWritable
	
	
   InputSplits
   -----------
	=> InputSplits are created by the framework as an integral number of records
	=> InputSplits are logical entities (pointer to the byte-positions)
	=> Created by the Framework.

	=> InputSplits  (represented by an "InputSplit") are created by "InputFormat" class.

	
   RecordReader
   ------------
	=> Creates (K, V) pairs from each record of the input-split and calls the "map" method
	   of the mapper program. 


 input-split (InputSplit)			  RecordReader
 -------------------------                         ------------

     hadoop hive mapreduce hdfs hadoop mapreduce  -> (0,"hadoop hive mapreduce hdfs hadoop mapreduce")      
						  ==>  map() -> (hadoop, 1) (hive, 1) (mapreduce, 1) (hdfs, 1) ..
		
    mapper reducer sqoop flume hadoop mapreduce   -> (51, "mapper reducer sqoop flume hadoop mapreduce")
						  ==>  map() -> (mapper, 1) (reducer, 1) (sqoop, 1) (flume, 1) ..

    mapreduce hive flume kafka hadoop hive	  -> (102, "mapreduce hive flume kafka hadoop hive")
						  ==>  map() -> (mapreduce, 1) (hive, 1) (flume, 1) (kafka, 1) ..



  Executing the MapReduce Program
  -------------------------------
	
      -> Export MR application as a Java Jar file

      -> hadoop jar <jar-file-path> <Driver-Class-Name> <Command-line-args>
	 hadoop jar /home/cloudera/wordcount.jar com.cts.WordCountJob wordcount_input.txt wcoutwordcount_input.txt



   Creating a MR program
   ---------------------

	-> Create a Java project

	-> Add the following JAR file to the Referenced Libraries:
		-> Right click on  Referenced Libraries -> Build Path -> Configure Build Path
		-> Click on "Add External JARS" and add the following:
			/usr/lib/hadoop/hadoop-common.jar
			/usr/lib/hadoop-0.20-mapreduce/hadoop-core.jar

	-> Create packages and start to write MR programs. 


  Mapper Life Cycle Methods
  -------------------------

    1. setup
	 -> Executed once at the beginning (even before map method is called)
	 -> Used for initialization, creating resources (like DB connections etc).

    2. map
	-> Executed once (after setup method) for every input (K, V) pair in the input-split
	-> Implement the buisiness logic to be executed per record.

    3. cleanup
	-> Executed once after map is called for all records
	-> Write any consolidated output if you have.
	-> Close the connections, clean the resources etc. 

    4. run
	-> Generally not implemented.	


  Reducer Life Cycle Methods
  -------------------------

    1. setup
	 -> Executed once at the beginning (even before reduce method is called)
	 -> Used for initialization, creating resources (like DB connections etc).

    2. reduce
	-> Executed once (after setup method) for every input (K, Iterable[V]) pair. 
	-> Implement the buisiness logic to be executed per record.

    3. cleanup
	-> Executed once after reduce methods is called for all records
	-> Write any consolidated output if you have.
	-> Close the connections, clean the resources etc. 

    4. run
	-> Generally not implemented.	

  
   Use-Case: 
   ---------
	From pseudo_facebook.tsv file find out the record with maximum value of 'likes_received'
	from among peoplw who are born between 1990 and 2000.

	Two MR programs:

	MR 1. Fetch the output for people born between 1990 and 2000
	MR 2. Fetch the record with highest 'likes_received' value from the output of MR 1

   ==================================
      Hive
   ==================================	
  
    -> Data Warehousing Platform built on top of Hadoop
	-> uses HDFS as storage layer
	-> uses MapReduce as execution engine.

    -> Hive provide a SQL-like wrapper (Hive Query Language) on top of MR, so the developers/analysts
       do not have to write MR programs. 

    -> Hive is NOT an RDBMS nor a NoSQL database.

	  
    Hive Components
    ----------------

	1. Hive Warehouse  ( directory:  /user/hive/warehouse )
		-> Is a directory on HDFS  
		-> There is a default database called "default" 
		    -> 'default' database does not have any separate db directory.
		-> We can create user-defined databases (schemas) 
		    -> Each database is stored as a separate directory in the warehouse directory  
		    -> Each table will have its own directory inside the database directory
		    -> All the data of 'managed tables' are stored as files in the table directory.

	2. Hive Metastore
		-> Hive stores all its meta-data in a Meta store service 
			-> Hive comes with an embedded database called 'Derby' as a metastore
		-> In prodution, usually MySQL database is used as metastore.


	
    How to interact with Hive
    -------------------------

	1. Command Line Interface (Shell)
		-> Hive shell
		-> Beeline shell (JDBC)

	2. Client Applications
		-> Can connect to hive using thrift, JDBC or ODBC

	3. HUE (Hive User Environment) - Beeswax 
	
   	
   Databases
   ---------
	show databases;	
	create database ctsdb1;
	drop database ctsdb1;        // used to drop an empty database
	describe database ctsdb1;

	show schemas;	
	create schema ctsdb1;
	drop schema ctsdb1;
	describe schema ctsdb1;


   Connecting to beeline shell
   ----------------------------

	$ beeline
	beeline> !connect jdbc:hive2://quickstart:10000 cloudera cloudera org.apache.hive.jdbc.HiveDriver

	0:jdbc:hive2://quickstart:10000> show databases;  


   Tables
   ------

CREATE TABLE empinfo (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

	
  Default Values in the CREATE command :
  --------------------------------------

	ROW FORMAT 		=> DELIMITED 
	FIELDS TERMINATED BY	=> Ctrl-A
	LINES TERMINATED BY	=> \n
	STORED AS		=> TEXTFILE	


desc empinfo
desc extended empinfo
desc formatted empinfo

=> from local file system (linux) : COPY operation

// append mode
LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/data/emp.txt' INTO TABLE empinfo;  

// overwrite mode  
LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/data/emp.txt' OVERWRITE INTO TABLE empinfo; 

=> from hdfs  : MOVE operation
// append mode
LOAD DATA INPATH '/user/cloudera/datasets/emp.txt' INTO TABLE empinfo;

// overwrite mode
LOAD DATA INPATH '/user/cloudera/datasets/emp.txt' OVERWRITE INTO TABLE empinfo;


  Schema on Read
  --------------
    => Hive does not validate the data while loading the data.
    => Validation against the schema of table happens while 'reading' (querying) the data
	 -> Hive shows NULL if some column did the not match the schema. 


External Tables
---------------
CREATE EXTERNAL TABLE empinfo_external (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED AS TEXTFILE
LOCATION '/user/cloudera/hivedata/empinfo_external';

LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/data/emp.txt' OVERWRITE INTO TABLE empinfo_external; 

=> External tables are pointed to a user-specified location (outside of hive warehouse) for data files.
=> The data of an external table is not owned by Hive.
=> If you drop an external table, only the schema is deleted. But not the data files.


  Partitioned Tables
  ------------------

	=> In Hive, Partitions provide a way of dividing a table into coarse-grained parts 
   	   based on the value of a partition column, such as a date or country.

        => A separate partition (directory) is create for every unique-value of the partitiond column

	=> The data files does not contain the values of the partition columns. 
		-> The partition column data comes from the directory name itself.

	Based on how we can load/insert data into a partitioned table, we can use two approaches:

        1. Static Partitioning
	    -> When you know exactly into which partition a specific file need to be loaded
	    -> We can use "LOAD DATA .. " command.

	2. Dynamic Partitioning
	   -> When we can not decide into partition a specific file need to be loaded. 
	
	   -> The data file is first loaded into a base table
	   -> From the base table the data is inserted into partitioned table using "INSERT .. SELECT" command
	   -> We can not use "LOAD DATA ..." command.
	
	   -> NOTE: Make sure to set the following config varibale:
		  set hive.exec.dynamic.partition.mode=nonstrict
	


 Partitioned Tables
 -------------------

CREATE TABLE students_partition_static (id INT, name STRING, mobile STRING, email STRING, project STRING)
PARTITIONED BY (country STRING)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/data/students_IN.txt' 
INTO TABLE students_partition_static
PARTITION (country='India');


CREATE TABLE students_partition_dynamic (id INT, name STRING, mobile STRING, email STRING, project STRING)
PARTITIONED BY (country STRING, batch INT)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;


CREATE TABLE students
(id INT, name STRING, mobile STRING, email STRING, project STRING, country STRING, batch INT)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/data/students_country_batch.csv' INTO TABLE students;

set hive.exec.dynamic.partition.mode=nonstrict

INSERT INTO students_partition_dynamic
PARTITION (country, batch)
SELECT id, name, mobile, email, project, country, batch FROM students;


INSERT INTO students_partition_dynamic
PARTITION (country='India', batch)
SELECT id, name, mobile, email, project, batch FROM students
WHERE country='India';


INSERT INTO students_partition_dynamic
PARTITION (country='India', batch=1)
SELECT id, name, mobile, email, project FROM students
WHERE country='India' and batch=1;

// The following command does not work
// Dynamic partition cannot be the parent of a static partition

INSERT INTO students_partition_dynamic
PARTITION (country, batch=1)
SELECT id, name, mobile, email, project, country FROM students
WHERE batch=1;


LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/data/students_IN_csv.txt' 
INTO TABLE students_partition_dynamic
PARTITION (country='India', batch=4);


  What type of colums can we use for partitioning
  ------------------------------------------------
     => We have to use "low-cardinality" columns and are used in queries 
	as partition columns
     => You need to have sufficient big amount of files in each partition.

 
   Bucketting
   -----------
	
	=> In Hive, Buckets provide an extra structure to the data to improve query efficiency

	-> We can buckettize data of a table or a partition into fixed number of buckets based
	   on a cluster column
		-> Data is split across multiple buckets by applying hash function on cluster columns.

CREATE TABLE students_bucket 
(id INT, name STRING, mobile STRING, email STRING, project STRING, country STRING, batch INT)
CLUSTERED BY (id) 
INTO 4 BUCKETS;

set hive.enforce.bucketing = true;

INSERT INTO students_bucket SELECT * from students;

CREATE TABLE students_bucket 
(id INT, name STRING, mobile STRING, email STRING, project STRING, country STRING, batch INT)
CLUSTERED BY (id) INTO 4 BUCKETS
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;

CREATE TABLE students_part_bucket 
(id INT, name STRING, mobile STRING, email STRING, project STRING, country STRING)
PARTITIONED BY (batch INT)
CLUSTERED BY (id) INTO 3 BUCKETS
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;

insert into students_part_bucket 
partition(batch)
select id, name, mobile, email, project, country, batch from students;

select * from students_part_bucket where batch = 1 and id = 9;
--------------------------------------------

 Sampling Queries
 ----------------
   Key Word: TABLESAMPLE

   => Bucket Level Sampling
	select * from students_bucket tablesample (bucket 1 out of 2 on id);


   => Block Level Sampling
	-> some specifies number of blocks are fetched based on query parameter (such as 20 percent, 10M etc)

	select * from students tablesample (50 percent);
	select * from students tablesample (50M);
	
   => Row Level Sampling
	-> some specifies number of rows are fetched from each input split
	
	select * from students tablesample (5 rows);

 --------------------------------------------------

   Managing Outputs  
   -----------------

    -> Store the output of a query in a table

		insert into empinfo_2 select * from empinfo;    	(APPEND MODE)
		insert overwrite table empinfo_2 select * from empinfo; (OVERWRITE MODE)

	
    -> Store the output of a query in a local (linux) directory (in a desired format)

		INSERT OVERWRITE LOCAL DIRECTORY '/home/cloudera/hive_data/students_pipe'
		SELECT * FROM students;

		INSERT OVERWRITE LOCAL DIRECTORY '/home/cloudera/hive_data/students_pipe'
		ROW FORMAT DELIMITED
		FIELDS TERMINATED BY "|" 
		SELECT * FROM students;


    -> Store the output of a query in a HDFS directory (in a desired format)

		INSERT OVERWRITE DIRECTORY '/user/cloudera/hivedata/students_pipe'
		ROW FORMAT DELIMITED
		FIELDS TERMINATED BY "|" 
		SELECT * FROM students;
	  
   
    Hadoop File Formats
    -------------------	 
	
	-> Text File (default)

	-> Sequence File	-> Data is stored in (K, V) pairs

	-> Avro File		-> Supports schema evolution 
				   The schema itself is stored in the data file (as json content)

	-> ORC file		-> Is a columnar format file

	-> Parquet File		-> Is a columnar format file

    
INSERT OVERWRITE LOCAL DIRECTORY '/home/cloudera/hive_data/students_seq'
STORED AS SEQUENCEFILE
SELECT * FROM students;

INSERT OVERWRITE LOCAL DIRECTORY '/home/cloudera/hive_data/students_avro'
STORED AS AVRO
SELECT * FROM students;

INSERT OVERWRITE LOCAL DIRECTORY '/home/cloudera/hive_data/students_parquet'
STORED AS PARQUET
SELECT * FROM students;

INSERT OVERWRITE LOCAL DIRECTORY '/home/cloudera/hive_data/students_orc'
STORED AS ORC
SELECT * FROM students;


 Sorting
 -------
	
	=> ORDER BY col1
	   -> Generates parallel total sort
	   -> select * from students order by name	

	=> SORT BY col1
	    -> Generated a sorted file per reducer
	    -> select * from students sort by name
	
	=> DISTRIBUTE BY col1
	    -> Attached a partitioner to the reducers based on a columns
            -> select * from students distribute by country sort by name		

	=> CLUSTER BY
	    -> Is a special case DISTRIBUTE BY where you are distributing and sorting by the same column.
	    -> select * from students cluster by country
               ( is a shortcut of select * from students distribute by country sort by country )

	 
  Multi-Table Insert Command
  --------------------------

	from students
    insert into students_in
    select * where country = 'India'
    insert into students_usa
    select * where country = 'USA'
    insert into students_uk
    select * where country = 'UK';
	
Alter command
-------------
alter table empinfo_2 rename to empinfo_3;
alter table empinfo_3 add columns (dob string, age int);	
alter table empinfo_3 change age age2 STRING;
alter table students_partition_static PARTITION (country='USA') RENAME TO PARTITION (country='US');

ALTER TABLE students_partition_static ADD PARTITION(country='Japan')

LOCATION '/user/hive/warehouse/ctsdb1.db/students_partition_static/c=Japan';

hadoop fs -put /home/cloudera/hive/Hive/data/students_IN.txt /user/hive/warehouse/ctsdb1.db/students_partition_static/c=Japan

ALTER TABLE emp_tp_immutable SET TBLPROPERTIES ("immutable"="false")



Views
-----

CREATE VIEW students_us_view AS SELECT * FROM students WHERE country='USA';
select * from students_us_view;
drop view students_us_view;


Complex Data Types
------------------

sample complex data
-------------------
Apple/Grapes/Orange,4306:Sneha/4307:Renu,Sneha/90
Apple/Microsoft/Google,4306:Renu/4307:Anita,Raju/100
Apple/Micromax/Samsung,4306:Raju/4307:Harsha,Harsha/80


table to store complex-data
---------------------------
CREATE TABLE complex_data
(col1 array<string>,
 col2 map<int,string>,
 col3 struct<name:string,score:int>)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
COLLECTION ITEMS TERMINATED BY '/'
MAP KEYS TERMINATED BY ':'
STORED AS TEXTFILE;
 
select * from complex_data;
select col1[0], col1[1], col2[4306], col2[4307], col3.name, col3.score from complex_data;

Table Properties
-----------------

CREATE TABLE emp_tp_metadata 
(empid INT, empname STRING, salary DOUBLE, deptid SMALLINT) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ',' 
LINES TERMINATED BY '\n' 
STORED AS TEXTFILE 
TBLPROPERTIES ('creator'='raju', 'created_at'='2021-11-18 10:00:00')

-> SHOW TBLPRPPERTIES emp_tp_metadata 
-> SHOW TBLPRPPERTIES emp_tp_metadata('creator')


Immutable Tables
----------------
As of Hive 0.13.0, a table can be made immutable by creating it with 
TBLPROPERTIES ("immutable"="true"). 

The default is "immutable"="false".

INSERT INTO behavior into an immutable table is disallowed if any data is already present,
although INSERT INTO still works if the immutable table is empty. 

The behavior of INSERT OVERWRITE is not affected by the "immutable" table property.

An immutable table is protected against accidental updates due to a script loading data 
into it being run multiple times by mistake. The first insert into an immutable table 
succeeds and successive inserts fail, resulting in only one set of data in the table, 
instead of silently succeeding with multiple copies of the data in the table. 

CREATE TABLE emp_tp_immutable
(empid INT, empname STRING, salary DOUBLE, deptid SMALLINT) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ',' 
LINES TERMINATED BY '\n' 
STORED AS TEXTFILE 
TBLPROPERTIES ("immutable"="true")


insert into emp_tp_immutable select * from empinfo;  => Allowed if the table is empty
insert into emp_tp_immutable select * from empinfo;  => Not Allowed if the table is non-empty
insert overwrite emp_tp_immutable select * from empinfo;  => Allowed 
LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/data/emp.txt' INTO TABLE emp_tp_immutable  => Allowed

ALTER TABLE emp_tp_immutable SET TBLPROPERTIES ("immutable"="false")


  Transactional Tables
  --------------------

      -> Transactional tables support 'update' and 'delete' operations  
	
      -> Table must use ORC file format
      -> Table must be bucketted
      -> Add TBLPROPERTIES('transactional'='true') to create command
      -> Set the following configurations:	
		set hive.support.concurrency = true;
		set hive.enforce.bucketing = true;
		set hive.exec.dynamic.partition.mode = nonstrict;
		set hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
		set hive.compactor.initiator.on = true;
		set hive.compactor.worker.threads = 1;



CREATE TABLE user_acid (userid int, name string, age int, gender string)
CLUSTERED BY (userid) INTO 5 buckets 
STORED AS ORC
TBLPROPERTIES('transactional'='true');


Run the following commands
--------------------------
set hive.support.concurrency = true;
set hive.enforce.bucketing = true;
set hive.exec.dynamic.partition.mode = nonstrict;
set hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
set hive.compactor.initiator.on = true;
set hive.compactor.worker.threads = 1;


insert data
-------------
INSERT INTO user_acid(userid, name, age, gender) VALUES 
(1, 'Raju', 40, 'male'), 
(2, 'Raghu', 42, 'male'),
(3, 'Vinay', 20, 'male'),
(4, 'Amrita', 14, 'female'),
(5, 'Aditya', 14, 'male'), 
(6, 'Keertana', 12, 'female'),
(7, 'Pranav', 24, 'male'), 
(8, 'Hari', 24, 'male'),
(9, 'Syamala', 23, 'female')

update user_acid set name='ADITYA...' where userid = 5;

delete from user_acid where userid = 5;


 Joins
 -------------

Supported Joins => inner, outer joins (left, right, full), left_semi

CREATE TABLE IF NOT EXISTS stocks 
(stock_exchange STRING, symbol STRING, trxn_date DATE, open DOUBLE, 
high DOUBLE,low DOUBLE, close DOUBLE, volumn INT,adj_close DOUBLE)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n' 
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '/home/cloudera/datasets/stocks' 
OVERWRITE INTO TABLE stocks; 


CREATE TABLE IF NOT EXISTS dividends 
(stock_exchange STRING, symbol STRING, trxn_date DATE, dividend DOUBLE)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n' 
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '/home/cloudera/datasets/dividends' 
OVERWRITE INTO TABLE dividends; 

SELECT s.trxn_date, s.symbol, s.adj_close, d.dividend
FROM stocks s JOIN dividends d
ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol; 

SELECT s.trxn_date, s.symbol, s.adj_close, d.dividend
FROM stocks s LEFT OUTER JOIN dividends d
ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol; 


 Left Semi
 ---------	
      => Is like inner join but the data comes only from the left side table.
      => The behaviour of the left-semi join is as per the following sub-query:

	select * from emp where deptid IN (select deptid from dept)

SELECT * FROM stocks s LEFT SEMI JOIN dividends d
ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol; 


  Bucket Map Joins
 ------------------

CREATE TABLE IF NOT EXISTS stocks_bkt_16 
(stock_exchange STRING, symbol STRING, trxn_date DATE, open DOUBLE, 
high DOUBLE,low DOUBLE, close DOUBLE, volumn INT,adj_close DOUBLE)
CLUSTERED BY (trxn_date, symbol) INTO 16 BUCKETS 
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;


CREATE TABLE IF NOT EXISTS dividends_bkt_8 
(stock_exchange STRING, symbol STRING, trxn_date DATE, dividend DOUBLE)
CLUSTERED BY (trxn_date, symbol) INTO 8 BUCKETS 
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;


set hive.enforce.bucketing = true; 
set hive.optimize.bucketmapjoin = true;


INSERT INTO stocks_bkt_16 SELECT * FROM stocks SORT BY trxn_date, symbol;

INSERT INTO dividends_bkt_8 SELECT * FROM dividends SORT BY trxn_date, symbol;

SELECT /*+ MAPJOIN(dividends_bkt_8) */ s.trxn_date, s.symbol, s.adj_close, d.dividend
FROM stocks_bkt_16 s JOIN dividends_bkt_8 d
ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol;



  Sort Merge Joins
 -------------------

CREATE TABLE IF NOT EXISTS stocks_bkt_8 
(stock_exchange STRING, symbol STRING, trxn_date DATE, open DOUBLE, 
high DOUBLE,low DOUBLE, close DOUBLE, volumn INT,adj_close DOUBLE)
CLUSTERED BY (trxn_date, symbol) INTO 8 BUCKETS 
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;


INSERT INTO stocks_bkt_8 SELECT * FROM stocks SORT BY trxn_date, symbol;

set hive.auto.convert.sortmerge.join=true;
set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.auto.convert.sortmerge.join.nonconditionaltask=true;

SELECT /*+ MAPJOIN(dividends_bkt_8) */ s.trxn_date, s.symbol, s.adj_close, d.dividend
FROM stocks_bkt_8 s JOIN dividends_bkt_8 d
ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol;


 Explode, Lateral Views and Temporary Tables
 --------------------------------------------

1	1111111,11112222	email11@gmail.com,email1122@gmail.com
2	2222222,22223333	email22@gmail.com,email2222@gmail.com
3	3333333,33333444	email33@gmail.com,email3322@gmail.com
4	4444444,44445555	email44@gmail.com,email4422@gmail.com

   
create table tab_lateral (id int, phones array<string>, emails array<string>)
row format delimited 
fields terminated by '\t'
collection items terminated by ','
stored as textfile;

load data local inpath '/home/cloudera/hive/Hive/data/lateral_view_data.txt' 
overwrite into table tab_lateral;


1	1	1111111		email11@gmail.com
2	1	11112222	email1122@gmail.com
3	2	2222222		email22@gmail.com
4	2	22223333	email2222@gmail.com


TABLE 1
1	1111111		1
1	11112222	2
2	2222222		3
2	22223333	4		


TABLE 2
1	email11@gmail.com	1
1	email1122@gmail.com	2
2	email22@gmail.com	3
2	email2222@gmail.com	4



create table tab_lateral (id int, phones array<string>, emails array<string>)
row format delimited 
fields terminated by '\t'
collection items terminated by ','
stored as textfile;

load data local inpath '/home/cloudera/hive/Hive/data/lateral_view_data.txt' overwrite into table tab_lateral;

# note: expode can not be merged with any other columns as is.
# use lateral views if you want to do so.

select explode(phones) from tab_lateral;

select id, phones_l from tab_lateral 
lateral view explode(phones) p as phones_l;

select id, phones_l, emails_l from tab_lateral 
lateral view explode(phones) p as phones_l
lateral view explode(emails) e as emails_l;


-- This results in cross product of array values, which is not a good data representaion
-- Lets breakdown the data properly

Solution:

create temporary table if not exists tab_lateral_tmp_phones
(rowid int, id int, phone string)
row format delimited
fields terminated by '|';

insert overwrite table tab_lateral_tmp_phones
select row_number() over(), id, phones_l from tab_lateral 
lateral view explode(phones) p as phones_l;


create temporary table if not exists tab_lateral_tmp_emails
(rowid int, id int, email string)
row format delimited
fields terminated by '|';

insert overwrite table tab_lateral_tmp_emails
select row_number() over(), id, emails_l from tab_lateral 
lateral view explode(emails) p as emails_l;

create table if not exists tab_lateral_breakdown
(rowid int, id int, phone string, email string)
row format delimited
fields terminated by '|';

insert overwrite table tab_lateral_breakdown
select a.rowid, a.id, a.phone, b.email
from   tab_lateral_tmp_phones a, tab_lateral_tmp_emails b
where  a.rowid = b.rowid and a.id = b.id;

  Window Functions
 ------------------

	id	dept	salary	 row_number() 
	-------------------------------------	    	
	2	IT	50000	1
	8	IT	55000	2
	6	IT	55000	3
	7	IT	60000	4
	1	IT	60000	5
	12	IT	70000	6

	4	HR	40000	7
	10	HR	45000	8
	11	HR	45000	9
	5	HR	45000	10
			
	3	Sales	60000	11
	9	Sales	65000	12
 
SELECT symbol, trxn_date, close, LAG(close,1) OVER(PARTITION BY symbol) as yesterday_price 
FROM stocks LIMIT 40;

SELECT symbol, trxn_date, close, LAG(close,2) OVER(PARTITION BY symbol) as yesterday_price 
FROM stocks LIMIT 200;

SELECT symbol, trxn_date, close, LEAD(close,3) OVER(PARTITION BY symbol)  
FROM stocks LIMIT 400;

SELECT symbol, trxn_date, close, row_number() OVER(PARTITION BY symbol ORDER BY close)  
FROM stocks LIMIT 400;

SELECT symbol, trxn_date, close, 
sum(close) OVER(PARTITION BY symbol ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as cum_sum 
FROM stocks LIMIT 400;

select symbol, count(symbol) over(partition by symbol) as cnt from stocks;

SELECT symbol, trxn_date, close, 
row_number() OVER(PARTITION BY symbol ORDER BY close) as row_num,
rank() OVER(PARTITION BY symbol ORDER BY close) as rank,
dense_rank() OVER(PARTITION BY symbol ORDER BY close) as dense_rank
FROM stocks LIMIT 400;

   
   How to run a Hive script file ?
   -----------------------------
   $ hive -f '/home/cloudera/hive/Hive/churn_modelling.hql' 
   NOTE: Run the above command from the command line, not from shell.

 
   How to execute a HQL statement in interactive mode
   -------------------------------------------------
   hive -e 'select * from ctsdb1.empinfo'
   NOTE: Run the above command from the command line, not from shell.


  =============================================================================
	IMPALA
  =============================================================================
  
   -> A sql query engine built using massivly parallel processing (MPP) query engine
   -> For processing huge amount of data (PetaByte scale) stored in Hadoop clusters (HDFS)
   -> Uses the same metastore as well as the warehouse as that of Hive
   -> Uses the same query language as that of Hive (Hive QL)
   -> Written in C++ & Java
   -> High performance and Low Latency.
   -> Supports in-memory data processing.  
   -> Developed by Cloudera
	-> Shipped with all major Hadoop managed cloud service providers.


   => Impala does not support loading data from local file system
	-> LOAD DATA LOCAL INPATH ..  command is not supported.

   => Any metadata changes happened outside of impala such as creating databases, tables etc
      will not be reflected automatically in impala. You have to run the 'invalidate metadata'
      command to refresh metadata changes.

		=> invalidate metadata;

   => Whenever data is loaded into the tables from outside of impala, the data changes are not
      automatically reflected in impala. You have to run 'refresh <table-name>' command

		=> refresh empinfo2;

    When to use Impala ?
    --------------------

    => Because Impala uses in-memory computations, it does not provie run-time fail-safety.
        -> If any of the servers/nodes running the impala queries fail, the job will be aborted.

    => Use Hive for long-running batch queries
       Use impala for short-running interactive/batch queries

    => Hive is more suited for long running batch jobs, such those involving ETL.
    => Impala is good for real-time queries while Hive is recommended for large batch jobs.
 
 =============================================================
  Sqoop  
 =============================================================

      SQL-Hadoop  => Tool that moves data back and forth between SQL databases and Hadoop
       
      Sqoop is structured data ingestion tool 

      Sqoop used MR as execution engine
	-> Sqoop launches map-only jobs. 



     sqoop import --connect jdbc:mysql://localhost:3306/ctsdb 
	--username root 
	-P 
	--table users 
	--num-mappers 1 
	--target-dir /user/cloudera/sqoop1/users1  


sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users --num-mappers 1 --target-dir /user/cloudera/sqoop1/users1 

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users -m 1 --target-dir /user/cloudera/sqoop1/users2 --fields-terminated-by '\t'

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users --columns 'id,name,age' -m 1 --target-dir /user/cloudera/sqoop1/users3 --fields-terminated-by '|'

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users --columns 'phone,age,name' -m 1 --target-dir /user/cloudera/sqoop1/users4 --fields-terminated-by ','

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'select * from users where 1=1 and $CONDITIONS' -m 1 --target-dir /user/cloudera/sqoop1/users5 --append

NOTES: While using query:
	- where clause is a must in a query and it should have $CONDITIONS in it.
	- if you are using " (double-quotes) to enclose the query use AND \$CONDITIONS 
	  (eg:  --query "select * from users where 1=1 AND \$CONDITIONS")
	- if you are using ' (single-quotes) to enclose the query use AND $CONDITIONS 
	  (eg:  --query 'select * from users where 1=1 AND $CONDITIONS')

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'select country, count(1) as total from students where 1=1 and $CONDITIONS group by country' -m 1 --target-dir /user/cloudera/sqoop1/students3

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students -m 1 --target-dir /user/cloudera/sqoop1/students4 --enclosed-by '"'
	
-- Storing and AVRO & SEQUENCE file formats
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users --num-mappers 1 --target-dir /user/cloudera/sqoop1/users_avro --as-avrodatafile 	

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users --num-mappers 1 --target-dir /user/cloudera/sqoop1/users_seq --as-sequencefile

-- Using multiple mappers

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students -m 3 --target-dir /user/cloudera/sqoop1/students5 --split-by 'id' 

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students -m 3 --target-dir /user/cloudera/sqoop1/students6 --split-by 'country' 

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students -m 2 --target-dir /user/cloudera/sqoop1/students7 --split-by 'batch' --delete-target-dir

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students -m 3 --target-dir /user/cloudera/sqoop1/students8 --split-by 'batch' --delete-target-dir

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table userspk -m 3 --target-dir /user/cloudera/sqoop1/userspk 


Import Modes
-------------
  
   default: error if target directory exists

   --append: adds new files to the existing directory
   --delete-target-dir : overwrites the content of the existing directory

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students -m 1 --target-dir /user/cloudera/sqoop1/students1 --direct

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students -m 1 --target-dir /user/cloudera/sqoop1/students1 --direct --append

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students -m 1 --target-dir /user/cloudera/sqoop1/students1 --direct --delete-target-dir


----- import directly to hive -------



------ import entire database (import-all-tables) ------
sqoop import-all-tables --connect jdbc:mysql://localhost:3306/ctsdb --username root -P -m 1 --warehouse-dir /user/cloudera/sqoop1/ctsdb 


--------------------
Export
--------------------
sqoop export --connect jdbc:mysql://localhost:3306/ctsdb       
             --table users2
	     --username root 
	     -P                           			
	     -m 1                                       			
             --export-dir /user/cloudera/sqoop/demo    		
             --input-fields-terminated-by ','  

sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop1/users1 

sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop1/users2 --input-fields-terminated-by '\t' 

sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop1/users3/part-m-00000 --input-fields-terminated-by '|' --columns 'id,name,age'

sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop1/users3/part-m-* --input-fields-terminated-by '|' --columns 'id,name,age'

sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop1/users4 --columns 'phone,age,name'

sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop1/users5

sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop1/users_avro

-- does not work - sequence file can not be exported
sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop1/users_seq

sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table students2 --username root -P -m 1 --export-dir /user/cloudera/sqoop1/students4 --input-enclosed-by '"'

-- updating the data in mysql
sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop1/users1 --update-key 'id'

-- this will not work as expected  (because the table has no primary key)
-- this simply adds the records, but not update 
sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir  /user/cloudera/sqoop1/users1 --update-key 'id' --update-mode allowinsert

-- here, --update-key 'id' is a promary-key of userspk table
-- hense it will upsert the records properly.
sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table userspk --username root -P -m 1 --export-dir  /user/cloudera/sqoop1/users1 --update-key 'id' --update-mode allowinsert
 

------ create hive tables -----------
sqoop create-hive-table --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users --fields-terminated-by ',' --hive-table sqoopdb.users
sqoop create-hive-table --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students --fields-terminated-by '|' --hive-table sqoopdb.students

------- eval command ---------------------------
sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'select * from students limit 5'
sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'create table t1 (col1 int, col2 varchar(30))'
sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'insert into t1 (col1, col2) values(1, "raju"), (2, "ramesh"), (3, "ramya")'
sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'update t1 set col2="RAJU" where col1=1'
sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'delete from t1 where col1=1'
sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'drop table t1'
sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'show tables'

------- codegen command --------------
sqoop codegen --connect jdbc:mysql://localhost:3306/ctsdb --table users --class-name Users --username root -P



 ==================================
    HBase
 ==================================
	
   NoSQL (non-relational) databases:
	-> NoSQL databases offer unlimited scalability
	-> NoSQL databases are distributed databases running on clusters	
	   -> Each table is partitioned (sharded) and replicated across many nodes
	   -> Your query load is distributed among many machines
	-> NoSQL Databases does not following struct normalization and any regid schema. 
	-> Non-relational and denormalized (lot of columns are stored in the same table)
  
   Types of NoSQL Databases
	-> Column-family or wide-column oriented databases (Ex: HBase, Cassandra)
	-> Document oriented databases (Ex: MongoDB, CouchDB)
        -> Key-Value Stores (Ex: DynamoDB, Redis)
	-> Graph Databases (Ex: Neo4J)	

   Limitations of Hadoop for transactional processing:
	-> Unstructured Data
	-> No Random Access
	-> High latency
	-> Not ACID complaint

   HBase
	-> Is distributed database management system that is part of Hadoop ecosystem.
	-> Based on "Google Big Table" architecture
	-> Uses HDFS as storage layer
		-> Distributed Storage
		-> Fault Tolerant. 

   HBase Vs RDBMS
   
	-> HBase does not support SQL
	-> HBase only supports a basic set of operations (CRUD) on a single row. 
		-> All operations are applied at "Row-Level"
		-> No Joins, No GroupBy, No Sub-Queries
	-> Denormalized design
	-> Uses column-oriented storage
		(more like a Map than a table)
	-> Limited ACID support
		-> ACID complaint only at row-level
	-> No Data types in HBase
		-> All data is stored a 'bytearrays'


     Apache Phoenix

       => Apache Phoenix is an open source, massively parallel, relational database engine 
	  supporting OLTP for Hadoop using Apache HBase as its backing store


     HBase Data Model

        -> A table consists of column-family
	       -> Columns are not part of table definition
	-> A row is defined by arbitrary number of columns with the same "Row-Id"
		-> Each column belongs to row-id, column-family
		-> A column has key ( cf:col_name ), timestamp, and a value
        -> HBase manitains multiple versions of each column.
	-> Data is stored in sorted order
	      -> order of sorting -> rowid, column-family, column


   Working with HBase
   ------------------

hbase shell   -> run the HBase shell and launches an hbase instance

------------------------------------------

create 'emp', 'contact', 'basic', 'skills', 'salary', 'other' 
create 't1', {NAME => 'c1', VERSIONS => 5, BLOCKCACHE => 'false'}, {NAME => 'c2', VERSIONS => 2}

------------------------------------------
put 'emp', '1', 'basic:empid', '100'
put 'emp', '1', 'basic:name', 'Raju'
put 'emp', '1', 'basic:age', '45'
put 'emp', '1', 'contact:phone', '9246657890'
put 'emp', '1', 'contact:email', 'raju@gmail.com'
put 'emp', '1', 'skills:rdbms', 'mysql,oracle'
put 'emp', '1', 'salary:basic', '40000'
put 'emp', '1', 'salary:tds', '10%'

put 'emp', '2', 'basic:empid', '101'
put 'emp', '2', 'basic:name', 'Ramesh'
put 'emp', '2', 'basic:dob', '2000-01-01'
put 'emp', '2', 'contact:mobile', '9246657890'
put 'emp', '2', 'contact:emailid', 'raju@gmail.com'
put 'emp', '2', 'skills:rdbms', 'mysql,oracle'
put 'emp', '2', 'salary:basic', '40000'
put 'emp', '2', 'salary:hr', '8000'

put 'emp', 'A10', 'basic:empid', '103'
put 'emp', 'A10', 'basic:fname', 'Mahesh'
put 'emp', 'A10', 'basic:dob', '2000-02-01'
put 'emp', 'A10', 'contact:city', 'Hyderabad'
put 'emp', 'A10', 'contact:state', 'TS'
put 'emp', 'A10', 'contact:email', 'ramesh@gmail.com'
put 'emp', 'A10', 'skills:nosql', 'hbase'
put 'emp', 'A10', 'skills:rdbms', 'oracle'
put 'emp', 'A10', 'salary:basic', '40000'
put 'emp', 'A10', 'salary:travel', '5000'
put 'emp', 'A10', 'salary:hra', '10000'

put 'emp', '10', 'basic:empid', '104'
put 'emp', '10', 'basic:firstname', 'Raju'
put 'emp', '10', 'basic:age', '45'
-------------------------------------------------
scan 'emp'    // show all the data
scan 'emp', {COLUMNS => ['basic:empid', 'basic:name']}
scan 'emp', {COLUMNS => ['basic:empid', 'basic:name', 'salary:basic'], TIMERANGE => [1637667918885, 1637668214800]}
--------------------------------------------------

get 'emp', '1'
get 'emp', '1', {COLUMN => ['basic:empid', 'basic:fname', 'basic:dob', 'contact:city']}
get 'emp', '1', {COLUMN => ['basic:empid', 'basic:name', 'basic:dob', 'contact:city'], VERSIONS => 3}
get 't1', '1', {COLUMN => ['f1:name', 'f2:age'], VERSIONS => 5}
get 'emp', '1', {COLUMN => ['basic:empid', 'basic:name', 'basic:age', 'contact:phone'], VERSIONS => 4}

 ---------------------------------------------------
 
 put 't1', '001', 'c1:name', 'Raju~~~~~'
 put 't1', '001', 'c1:age', '45~~~~~'
 put 't1', '001', 'c2:city', 'Hyderabad~~~~~'
 put 't1', '001', 'c2:name', 'Telangana~~~~~'

 ------- delete --------------------
 delete 'emp', '10', 'basic:empid'

##------------ alter an hbase table -------------------

alter 'emp', NAME => 'basic', VERSIONS => 4 
alter 't1', NAME => 'f1', VERSIONS => 3 
alter 't1', NAME => 'f3', VERSIONS => 3
alter 't1', NAME => 'f3', METHOD => 'delete'

----- disable ----------
disable 'emp'

----- enable ----------
enable 'emp'

##------------ alter an hbase table -------------------
alter 'emp', NAME => 'basic', VERSIONS => 4 
alter 't1', NAME => 'f1', VERSIONS => 3 
alter 't1', NAME => 'f3', VERSIONS => 3
alter 't1', NAME => 'f3', METHOD => 'delete'

-------  truncate  ----------------
truncate 't1'

-------- Hive & HBase Integration ------------

# run this from linux while launching hive shell - this adds the required jar files to the hive path for that session
$ hive --auxpath /usr/lib/hive/lib/hive-hbase-handler-0.10.0-cdh4.7.0.jar,/usr/lib/hive/lib/hbase.jar,/usr/lib/hive/lib/zookeeper.jar,/usr/lib/hive/lib/guava-11.0.2.jar

CREATE TABLE hiveempinfo(key int, empname string, salary double, deptid int) 
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' 
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key, emp:empname, emp:salary, dept:deptid") 
TBLPROPERTIES ("hbase.table.name" = "hbaseempinfo");

insert into table hiveempinfo select * from empinfo;
 
Whatever the changes you make to the HBASE table, such as adding, updating, deleting will be
automatically applied to hive table.

If you drop the hive table, hbase table is also dropped.
 

 ==========================================    
       KAFKA
 ==========================================
   
     Messaging Systems
     -----------------
	-> Are used to overcome the complexity and unmanagability of mulitple data pipeline.
	-> Messaging system decouple data pipeline

	Two types:

	-> Point-to-point messaging systems (Queue)
		-> Messages produced by a source (producer application) are intended for a specific 
		   sink (consumer application)
		-> After the message is consumed, it will be deleted from the queue		

	-> Publisher-Subscriber messaging systems
		-> Messages are produced by the publishers (producers) to "topics".
		-> Messages in a topic are not intended for any specific consumer (subscriber).
		-> Subscribers can subscribe to the "topics" and process the data
		-> Messages are retained for a pre-configured period (ex: 7 days)

     What is Kafka?
     --------------	
      -> Is a distributed streaming platform that is used for :

	1. Publish & subscribe streams of records
	2. Store streams of records in fault-tolerant way. 
	3. Process streams of records as they occur (real-time processing)
	
     Why Kafka ?
     -----------
	-> Building real-time data pipelines (decoupled)
	-> Building real-time streaming application

     Benefits of Kafka
     -----------------
	-> Reliability (distributed, partitioned, replicated)
	-> Scalability
	-> Durability (distributed commit-log, intra-cluster replicated)
	-> High performance
	
    Basic Components
    ----------------
	1. Broker		
	   -> Kafka service runing on each node of the cluster is called a broker.
	   -> Brokers are responsible for storing pulished data.
	   -> Each broker in a Kafka cluster must have a unique "broker.id"
	   -> Each broker is stateless, so they use zookeeper for maintaining state.

	2. Zookeeper
	   -> Distributed coordination management service
	   -> All the brokers send heart-beats to zookeeper. 

	3. Cluster
	   -> Multiple brokers communicating with the same zookeeper service forms a cluster
	   -> A cluster can be expanded without any downtime.

	4. Topics
	   -> Is a feed/category to which records/messages/events are published. that can be consumed
	      by one or more consumers. 
	   -> For every topic, Kafka maintains topic partition logs (distributed commit-logs)
	   -> Messages are retained in a topic for a pre-configured amount of time
		-> default is 168 hours (7 days)

	5. Partitions
	   -> A topic is organized as partitions allowing Kafka to handle any arbitrary amount of data. 
	   -> Each partition is an "ordered commi log"  
	   -> Each message in a partition has a unique sequence-id called "offset"
		-> A message is uniquely identified by ( topic, partition-id, offset )		
	
	6. Replicas
	   -> Backups of a partition.
	   -> The "id" of the replica is same as the broker-id
	   -> They are used to prevent data-loss
	   -> Only one replica acts as a 'leader'
		-> All reads and writes are served only by these 'leader' replicas.
		-> If the broker containing 'leader' goes down, an election process is triggered
		   and one the in-sync replicas (ISR) will be elected as the 'leader' 

	7. Producer
	   -> Are applications that produce messages to topic (leader) partitions.
           -> A producer can produce messages to one or more topics

	8. Consumer	
	   -> Are application that subscribe to one or more topics (or topic partitions) and
	      poll messages from the leader replicas of the partitions.

  
   Getting started with Kafka
   --------------------------
    -> Download Kafka binaries from https://kafka.apache.org/downloads
    -> Extract it to a suitable directory.

    Some Important Directories:

	1. bin     => All executables are located here.
	2. libs	   => All libraries (jar files) are located here.
	3. config  => All configuration files are located here.


   Working with Kafka
   -------------------
    
   Note: All the commands are executed from Kafka installation directory path.
   $ cd <Kafka-Installation-Dir>

	1. Start a zookeeper service:
		bin/zookeeper-server-start.sh config/zookeeper.properties

	2. Start the Kafka Broker
		bin/kafka-server-start.sh config/server.properties

	        Important Broker Configurations (server.properties)
                ---------------------------------------------------
		broker.id=0
		listeners=PLAINTEXT://:9092   (port: 9092)
		log.dirs=/tmp/kafka-logs
		log.retention.hours=168
		zookeeper.connect=localhost:2181

	3. Working with Topics

	    3.1 List the existing topics
		bin/kafka-topics.sh --bootstrap-server localhost:9092 --list

	    3.2 Describe a topic
		bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic demotopic

	    3.3 Delete a topic
		bin/kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic demotopic

	    3.4 Create a topic
		bin/kafka-topics.sh --bootstrap-server localhost:9092 
				    --create 
				    --topic ctstopic 
				    --partitions 3 
				    --replication-factor 1

	4. Launching Console Producer
		bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic ctstopic

		bin/kafka-console-producer.sh 
			--bootstrap-server localhost:9092 
			--topic ctstopic 
			--property "parse.key=true" 
			--property "key.separator=:"

	5. Launching Console Consumer
		bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic ctstopic

		bin/kafka-console-consumer.sh 
			--bootstrap-server localhost:9092 
			--topic ctstopic 
			--property print.key=true 
			--property print.value=true 
			--property key.separator=" - "

		bin/kafka-console-consumer.sh 
			--bootstrap-server localhost:9092 
			--topic ctstopic3r 
			--property print.key=true 
			--property print.value=true 
			--property key.separator=" - " 
			--from-beginning	

    What data can a message contain?
    ---------------------------------
	-> topic  	 (required)
	-> value  	 (required) 
	-> key     	 (optional - but mostly used)
	-> partition-id  (optional)
	-> timestamp     (optional)


    Message Distribution Logic
    --------------------------

      1. If the message has the 'partition-id' specified, then the message will be delivered to that partition.

      2. If there is a key specified:

	   2.1 if there is any custom-partitioner specified, then the partitioner logic will appllied
	       to determine to which partition that message the message goes to. 

	   2.2 if no custom partitioner is specified, then the default hash partitioning is used.

     3. If there is no key specified, then messages will randomly distributed, in a load balanced manner.

		

   Single Node - Multiple Broker Cluster Setup
   -------------------------------------------

	server.properties		server1.properties			server2.properties		
	--------------------------------------------------------------------------------------------------
	broker.id=0			broker.id=1				broker.id=2
	#listeners=PLAINTEXT://:9092	listeners=PLAINTEXT://:9093		listeners=PLAINTEXT://:9094
	log.dirs=/tmp/kafka-logs	log.dirs=/tmp/kafka-logs-1		log.dirs=/tmp/kafka-logs-2


   Standalone Consumer
   -------------------
	-> A standalone consumer can subscribe at a topic-level (in which case it will consume 
	  from all partitions of the topic) or at topic-partition level (in which case it will
	  consume from only specified partition of the topic)
	
	Topic-level:
	bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic ctstopic3r  

	Partition-level
	bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic ctstopic3r --partition 2

	NOTE: Having a lot of partitions for a topics allows us to launch multiple instances of
	  standalone consumers, each consuming from 1 partition, there by allowing us to
	  increase consumer side scalability. 

   Consumer Groups 
   ----------------
	-> One or more consumer instances subscribing to the same topic using the same group-id form
	   a 'consumer group'

	-> The partitions of the topics are distributed to the consumers in the consumer group.

  
	Working with Consumer Groups	
	----------------------------
	
	List the Consumer Groups:
		bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list

	Describe a Consumer Group:
		bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group ctstopic3r-group
	 
   Partition Rebalence
   -------------------
	-> rebalance is short window of unavailablilty of entire consumer group, when all the exiting partition
	   assignments are revoked and partitions are reassigned to all the available consumers in the
	   consumer group.

        -> Rebalance is automatically triggered:
		-> When a new consumer joins a group
		-> When an existing consumer leaves a group. 

   Kafka APIs
   -----------

   	1. Producer API   -> Write external applications that produce messages to Kafka topics

	2. Consumer API   -> Write external applications that subscripe and consume messages from Kafka topics
	
	3. Streams API	  -> Write external application that consume from one (one more) topic and write to
			     another kafka topic. 

	4. Connect API	  -> Used to write custome soure connectors and sink connectors to transfer data in real 
			     time from one application to another.
	 

   Creating Kafka Applications from Scratch
   ----------------------------------------

	Two approaches:

	-> Create Kafka project by including Jars from a local installation

	-> Create Kafka project using Maven
		-> Create a simple Java Project
		-> Right-click on the project folder -> Configure -> Convert to Maven
			-> Take all defaults and say OK.
		-> Open pom.xml file
		-> Add the required dependecies in the 
			<dependencies> 
				... add you r dependencies here ... 
			</dependencies>
		-> Dependencies can be addded from 
			https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients		   

   Producer API
   ------------

	-> Write external applications that produce messages to Kafka topics


   Writing a simple producer application:
   --------------------------------------
	
	1. Define the Properties object and specifiy all connection properties:

		Properties kafkaProps = new Properties();
	        kafkaProps.put("bootstrap.servers", "localhost:9092,localhost:9093,localhost:9094");
	      	kafkaProps.put("key.serializer","org.apache.kafka.common.serialization.StringSerializer");         
	      	kafkaProps.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");	
      
        2. Create a 'KafkaProducer' object

		Producer<String, String> producer = new KafkaProducer <String, String>(kafkaProps);

	3. Define the message objects
		
		key = "Key 1";
		value = "Test Java Message 1";
		ProducerRecord<String, String> record = new ProducerRecord<>("demotopic", key, value); 	

		Over-loaded methods:
		-------------------
		ProducerRecord(String topic, V value)
		ProducerRecord(String topic, K key, V value)
		ProducerRecord(String topic, Integer partition, K key, V value)
		ProducerRecord(String topic, Integer partition, Long timestamp, K key, V value)			  		   
	
	4. Send the message to Kafka

		producer.send( record  )	  


     Different ways of sending messages:
     -----------------------------------

	1. Fire and Forget
		-> We send the message, but we do not care about the acknowledgement
		-> We do not anything about if the message is written to topic or not.

		Advantages:
		   -> Very fast

		Disadvantages:
		   -> No delivery guarentee
		   -> Ordering may not be maintained. 

		Code:

		ProducerRecord<String, String> record = new ProducerRecord<>(topicName, key, value); 				  
		producer.send(record);   // this is a non-blocking call returning Future<RecordMetaData>

	2. Synchronous Messaging
		-> Each message is a send and execution is blocked until the ack is received.

		Advantages:
		   -> High delivery guarentee
		   -> Ordering is maintained.

		Disadvantages:
		   -> Slow

		Code:

		ProducerRecord<String, String> record = new ProducerRecord<>(topicName, key, value);
		RecordMetadata metaData = producer.send(record).get();  
		// get() method blocks the call and returns the RecordMetaData object
	
	3. Asynchronous Messaging
		-> The messages are sent along with a callback objects whose onCompletion method
		   is invoked when the Future returns.

 
		  Advantages:
		    -> Very fast
		    -> Good delivery guarentee

		  Disadvantages:
		    -> Ordering may not be maintained. 

		  Code:

		     producer.send( 
		         new ProducerRecord<>(topicName,"Key " + i, "Java KafkaProducerAsync Value " + i), 
		    	 new KafkaProducerAsyncCallback()     // this is a callback object
		      );


   Producer Configurations
   -----------------------

	-> All producer configurations are applied to the KafkaProducer object via the properties
	   object passed as parameter to it.


	Required COonfigurations:	
		-> bootstrap.servers
		-> key.serializer
		-> value.serializer
			-> All Serialization & Deserialization classes are available 
			   @ org.apache.kafka.common.serialization.* package.

	More Configurations: 

	1. acks		: The number of acknowledgements the producer requires the leader to have
			  received before considering a request as complete. (default: 1)

		acks: 0   -> no acks required at all.
		acks: 1   -> (default) acknowledment comes only from leader replica. 
				-> Here, there is a possibility og message loss, in case the leader replica
				   of the partition goes down.
		acks: all ->  All In-Sync replicas should acknowledge the message.
			      (Safe Producer)

	2. buffer.memory  : (bytes) default: 32MB  (in bytes: 32*1024*1024)

	3. max.block.ms   : 60000 (1 min) -> time the send() method & partitionsFor() methods are blocked
			    in case the buffer is full.

	4. batch.size	  : default: 16 KB (16384)

	5. compression.type  : default: none
			       values: none, snappy, gzip, lz4, zstd

	6. linger.ms	  : Max amount of time the delivery thread waits for batch to be filled up.

	7. retries	  : default: Int.Max  (controlled by delivery.timeout.ms)

	8. delivery.timeout.ms  : default: 120000 (2 min)
				  -> Is the upperbound on the time to report success or failure after the 
				     call to send() succeeds. 

	9. client.id	: Used to track the source of the request

	10. partitioner.class : to specify custom partitioner.	


  Consumer API
  -------------
     -> Allows us to write client applications that can subscribe to a topic and process messages
	   (called consumers) 

     Consumer Types :

	1. Standalone Consumers 
        2. Consumer Groups 

     Consumer Groups:
	-> Automatically assigns partitions to the consumers in a consumer group.

        Partition rebalance
	-> Moving partition ownership from one consumer to another consumer

	-> Happens:
		-> when a consumer joins a group
		-> when a consumer leaves a group
		-> when the partitions of a topic are altered

   Creating  a Consumer Application
   ---------------------------------

	1. Define the Properties object and specifiy all connection properties:

		Properties props = new Properties();
		props.put("bootstrap.servers", "localhost:9092,localhost:9093,localhost:9094");		
		props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
		props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");		
		props.put("group.id", "group-demotopic-api");

	2. Create the KafkaConsumer object, which represents a connection to the cluster.

		Consumer<String, String> consumer = new KafkaConsumer<>(props);	

	3. Subscribe to one or more topics (or topic partititons)

		// Subscribing at topic level
		consumer.subscribe(Arrays.asList("ctstopic"));


		// Subscribing to specific partitions only
		List<TopicPartition> partitions = new ArrayList<>(); 
		partitions.add(new TopicPartition(topicName, 0));
		partitions.add(new TopicPartition(topicName, 1));

		consumer.assign(partitions); 

	4. Create a poll loop  (infinite loop)		

        5. Within the pool loop, start receiving messaging by polling from Kafka.

			while (true) { 				  
			    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));	
			    
			    for (ConsumerRecord<String, String> record : records) {					
					// put your business logic here ...					
					System.out.printf(
					  "topic = %s, partition = %s,offset = %d, key = %s, value = %s\n", 
					   record.topic(), 
					   record.partition(), 
					   record.offset(), 
					   record.key(), 
					   record.value());
				}			    
				consumer.commitAsync();   // non-blocking call
			}

		 
  Working with consumer groups
  -----------------------------
 
   bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list

   bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group ctstopic3r-group-api
	
   bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 
				--topic ctstopic3r 
				--group ctstopic3r-group-api 
				--reset-offsets --to-offset 100 --dry-run

   bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 
				--topic ctstopic3r 
				--group ctstopic3r-group-api 
				--reset-offsets --to-offset 100 --execute

   bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 
				--topic ctstopic3r 
				--group ctstopic3r-group-api 
				--reset-offsets --to-earliest --execute

   bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 
				--topic ctstopic3r 
				--group ctstopic3r-group-api 
				--reset-offsets --to-latest --execute

   bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 
				--topic ctstopic3r 
				--group ctstopic3r-group-api 
				--reset-offsets --shift-by 10 --execute

   bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 
				--topic ctstopic3r 
				--group ctstopic3r-group-api 
				--reset-offsets --shift-by -20 --execute

  
  Kafka delivery semantics for Consumers
  --------------------------------------

   1. At-most Once
	-> The messages are received, the offsets are committed, and then messages are processed.
	-> It is possible that some messages may not be processed at all, in case the consumer
	   dies in the middle of processsing the messages. 

   2. At-least Once (default)
	-> The messages are received, processed and then offsets are committed.
	-> If the consumer dies in the middle of processing it is possible that some of the messages
	   that were previously processed, may be processed again (by the next consumer)

   3. Exactly Once
	-> This is not possible in Kafka -> Sink workloads.
	   This is only supported in Kafka -> Kafka workloads.

   Consumer Configurations
   -----------------------
	-> bootstrap.servers   	(required)
	-> key.deserializer  	(required)
	-> value.deserializer  	(required)
	-> group.id

	-> enable.auto.commit    (default: true)
	    -> auto.commit.intervel.ms 

	-> fetch.min.bytes (default: 1024)
		-> controls how much data to pull in each request

	-> max.poll.records (default: 500)
		-> how many max records to receive per poll request (covers multiple partitions)
		-> increase this if messages are small and have lot of RAM

	-> max.partitions.fetch.bytes (def: 1 MB (1*1024*1024))
		-> max data returned per partition

        -> fetch.max.bytes (def: 50 MB)
		-> max data returned for each fetch request (covers multiple partitions)
		
	-> hearbeat.intervel.ms	(def: 3000)

	-> session.timeout.ms (def: 10000)

        -> auto.offset.reset   (def: latest)
		-> possible: latest, earliest, none
		-> What should happen if the current-offset is not set or is invalid.
		
    Streams API
    -----------

	1. Specify the properties:

		Properties props = new Properties();
        	props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-wordcount");
        	props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        	props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);
        	props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());
        	props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());
        	props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "latest");   //earliest

	2. Define a StreamBuilder

		StreamsBuilder builder = new StreamsBuilder();

	3. Setup the topology for the builder and start the stream.

		KStream<String, String> source = builder.stream(
        	    "wordcount-input", 
        	    Consumed.with( Serdes.String(), Serdes.String() )
        	);

        	KTable<String, Long> counts = source
            		.flatMapValues(value -> Arrays.asList(value.split(" ")))
            		.groupBy((key, value) -> value)
            		.count();

		counts.toStream().to("wordcount-output", Produced.with(Serdes.String(), Serdes.Long()));

		final KafkaStreams streams = new KafkaStreams(builder.build(), props);

		stream.start()

   Use-Case:
   --------
      --> refer to the GitHib. assignment_logmessages.


  ===================================
      NiFi
  ================================== 
     
    Apache NiFi is tool that is used to automate the flow of data between different systems.

	Linux File System Directory  ---> HDFS
	JSON -> database
	Kafka -> ElasticSearch

   Apache NiFi supports powerful and scalable directed graphs of data-routing, transformations, and
   system mediation logic. 

   -> Web based Drag & Drop inferface
   -> Focus on configuration of the processors
   -> Scalable across a cluster of machines
   -> Data buffering & back pressure management is supported
   -> Proritizatio of the queue, latency vs. throughput.

 
 Installing Nifi
 ----------------

  Installing NiFi is very straight forward
	
	-> Download NiFi bianaries as a tar or zip file
	   URL: https://nifi.apache.org/download.html
	-> Extract the downloaded archive into a suitable folder
	-> That's it !!


  => Let us assume that NiFi is installed in a directory called "nifi-1.11.4" 

  
 Starting the NiFi
 -----------------

   1. cd nifi-1.11.4

   2. For Windows Users
	-> For Windows users, navigate to the folder where NiFi was installed. 
        -> Within this folder is a subfolder named "bin". 
        -> Navigate to this and double-click the "run-nifi.bat" file.
	-> This will launch NiFi and leave it running in the foreground.
        -> To shut down NiFi, select the window that was launched and Ctrl+C

  3. For Linux/Mac OS X users
	-> $ bin/nifi.sh run    (to start NiFi app in the foreground)
        -> $ bin/nifi.sh status  (to get the status)

   4. Installing NiFi as a service
	-> This is supported only for Linux / Mac

	-> $ bin/nifi.sh install    
             -> This will install with the default service name 'nifi'
        -> $ bin/nifi.sh install dataflow
             -> This will install with the custome service name 'dataflow'

        -> Once installed, the service can be started and stopped as below:
	     -> $ sudo service nifi start    (or sudo service dataflow start)
		$ sudo service nifi stop
		$ sudo service nifi status


   Opening NiFi Web UI
  -------------------

     => Goto:   http://localhost:8080/nifi

     => The port can be changed by editing the nifi.properties file in the NiFi conf directory
     => The default port is 8080.


   Downloading NiFi Templates from Web
   -----------------------------------
   
     URL:  https://cwiki.apache.org/confluence/display/NIFI/Example+Dataflow+Templates
     -> You can download:  CsvToJSON.xml


   Basic Components of NiFi
   ------------------------
	
        1. Flow File
		-> It is basically the data
		-> Comprises of
			1. Content -> the data itself
			2. Attributes -> (K, V) pairs assciated with the data.
		-> Persisted on the disk.

	2. Processor 
		-> There are ~288 processors in version 1.12
		-> Applies a set of transformations and rules to flow-files to generate
	           new flow files.
		-> Any processor can process any flow file.
		-> Processors are passing flowfile references to each other to 
		   advance the data processing.
		-> They are all running in parallel on different threads.

	3. Connector
		-> Its basically a queue of all the flow files that are yet to be processed
		   by the downstream processor.
		-> Defines rules about how flow files are proritized
		-> Can define backpressure to avoid overflow.

	4. Process Groups
		-> A container for defining a group of processors.

	5. Controller Services
		-> Different services provided by NiFi to help with data manipulations
		-> The controlers can be defined at Global scope (NiFi) or at process group 
		   scope
		

   Use-Case-1  ==> CSV files to MySQL  
   ------------------------------------      

       -> GetFile (Source Data - CSV file)
	  -> SplitText (splits single FF into many FFs)
            -> ConvertRecord (Convert CSV into JSON FF)               
	       -> ConvertJSONtoSQL (Convert JSON to SQL statement)
                   -> PutSQL (to run SQL against MySQL)

     Configurations
     --------------

     Processor 1: GetFile
		-> Input Directory: source directory where you put CSV files

	GetFile  -> success -> SplitText

     Processor 2: SplitText
		-> Line Split Count: 1
		   Header Line Count: 1

	SplitText -> splits -> ConvertRecord

     Processor 3: ConvertRecord
		-> RecordReader: CSVReader (Controller Service)
		-> RecordWriter: JSONRecordSetWriter (Controller Service)

	CSVReader Properties
	-> Schema Access Strategy: Use String Fields From Header
	   Treat First Line As Header: true

	JSONRecordSetWriter Properties
	-> No changes
	
	ConvertRecord -> success -> ConvertJSONToSQL
	  
     Processor 4: ConvertJSONToSQL	  
	-> JDBCConnectionPool: DBCPConnectionPool (Controller Service)
	   Statement Type: INSERT
	   Table Name: <MySQL Table Name>
	   Catalog Name: <MYSQL Database Name>

	DBCPConnectionPool properties   
	-> Database Connection URL: jdbc:mysql://localhost:3306/<mysql db name>
	   Database Driver Class Name: com.mysql.jdbc.Driver
           Database Driver Locations: <directory path where you placed the mysql driver>
		    (ex: /home/cloudera/nifi-1.11.4/lib/mysql-connector-java-5.1.44.jar)
	   Database User: <mysql user name>
	   Database password: <mysql user password>

       ConvertJSONToSQL -> sql -> PutSQL 

     Processor 5: PutSQL  
	-> JDBCConnectionPool: DBCPConnectionPool (Controller Service)


   Use-Case-2
   ----------
     ==> Kafka - NiFi Integration
 
       -> GenerateFlowFile (Generates random Flow Files at the desired)
          -> PublishKafka (Publishes Flow Files to a Kafka Topic)

       -> ConsumeKafka (Subscribes to Kafka topic and created flow-file)
          -> LogAttribute (LOg the flow files) 
 

     Configurations
     --------------

     Processor 1: GenerateFlowFile
     -> File Size: 100B
	Batch Size: 5
	Unique Flow Files: true

	GenerateFlowFile -> success -> PublishKafka_0_10 1.11.4

     Processor 2: PublishKafka_0_10 1.11.4
     -> Kafka Brokers: localhost:9092
	Topic Name: <kafka topic name>   ex: nifi-topic


    NOTE: No connection between PublishKafka_0_10 1.11.4 & ConsumeKafka_0_10 1.11.4


    Processor 3: ConsumeKafka_0_10 1.11.4
     -> Kafka Brokers: localhost:9092
	Topic Name: <kafka topic name>   ex: nifi-topic
	
	ConsumeKafka_0_10 1.11.4 -> success -> LogAttribute

    Processor 4: LogAttribute
    -> No changes


   Templates
   ---------


  =====================================================
        Spark  (PySpark)
  =====================================================

   Spark is a unified in-memory distributed computed framework. 

     -> Spark is written in Scala
     -> Spark is polyglot
	   -> Scala, Python, Java, R & SQL
     -> Spark can be run on multiple cluster managers
	  -> local, spark standalone, YARN, Mesos, Kubernetes
	   
     Spark Unified Framework
     ------------------------
	-> Provides a consistent set of API built on the same execution engine to process
           different analytical workloads.    
	
	Hadoop EcoSystem (except Spark)
	--------------------------------------
	Batch Analytics of Unstructured data  	: MapReduce
	Batch Analytics of Structured data	: Hive, Pig, Impala, Drill
	Streaming Analytics			: Kafka, Storm, Samza, Flink
	Predictive Analytics (ML)		: Mahout
	Graph Parallel Computations		: Giraph

	Spark Unified Framework
	----------------------- 
	Batch Analytics of Unstructured data  	: Spark Core (RDDs)
	Batch Analytics of Structured data	: Spark SQL
	Streaming Analytics			: Spark Streaming, Strcutured Streaming
	Predictive Analytics (ML)		: Spark MLlib
	Graph Parallel Computations		: Spark GraphX


     Spark Architecture
    ------------------

	1. Cluster Manager
		-> Jobs are submitted to CM
		-> Spark programs can be submitted to Spark Standalone, YARN, Mesos, Kubernetes
		-> CM will allocate executors to the application. 

	2. Driver
		-> Master Process
		-> Contains the SparkContext
		-> Maintain all the metadata of the user program and of RDD
		-> Send tasks to the executors

		Deploy Modes:
		 client  -> default, the driver process runs on the client machine.
		 cluster -> driver runs in one of the node in the cluster

	3. Executors
		-> Executes the tasks sent by the driver
		-> All tasks do the same processing, but on different partitions of the data
		-> They report the status of the tasks to the driver.

	4. SparkContext
		-> Starting point of execution 
		-> Created inside a driver
		-> Represents an application
		-> Link betweeb the driver and several tasks running in the executors

    
    RDD (Resilient Distributed Dataset)
    -----------------------------------

	=> Is the fundamental data abstraction of Spark

        => Is a collection of ditributed in-memory partitions.
		-> A partition is a collection of objects	

	=> RDDs are immutable (you can not change the content of an RDD)

	=> RDDs are lazily evaluated 
		-> Transformations does not cause execution.
		-> Action commands trigger execution.
		

    What can we do with an RDD ?
    ----------------------------

	Two things:

	1. Transformations
		-> only lineage DAGs are create. does not cause execution.

	2. Actions
		-> cause execution and produces some output


    How to create an RDD ?
    ----------------------

	Three ways:

	1.  Create an RDD from external files.

		rddFile = sc.textFile(filePath, 4)

	2. Create an RDD from programmatic data

		rdd1 = sc.parallelize( range(1, 101) )


	3. By applying transformations on existing RDDs

		rdd2 = rddFile.map(lambda x: x.upper())

 
    RDD Lineage DAG
    ----------------

	-> Is a logical plan that tracks all the dependencies (hierarchy) of RDDs which caused
	   the creation of this RDD all the way from the very first RDD. 

	-> The RDD  lineage DAGs are created at te driver side where transformations are performed.

	rddFile = sc.textFile(filePath, 4)
		Lineage:  (4) rddFile -> sc.textFile

	rdd2 = rddFile.map(lambda x: x.upper())
		Lineage:  (4) rdd2 -> rddFile.map -> sc.textFile

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))
		Lineage:  (4) rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile

	rdd4 = rdd3.filter(lambda x: len(x) > 3)
		Lineage:  (4) rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile
	
        rdd4.collect()
    
		sc.textFile -> map -> flatMap -> filter 


    Working with Spark
    ------------------
   
    1. in your vLab
       
	1.1  Using the PySpark shell

		-> Open a terminal and type the following:
		  $ pyspark

	1.2 Work with Jupyter Notebooks
		-> Open a terminal and type the following:

		$jupyter notebook --allow-root
		$jupyter notebook

   2. Using Databrick Community Edition 

	https://databricks.com/try-databricks


   RDD Transformations
   -------------------

    1. map			P: U -> V 
	
	
		


        










