 
  Agenda (20 session * 4 hrs) 
  ---------------------------
    -> Big Data - Concepts
    -> Hadoop
	 -> HDFS
	 -> MapReduce
	 -> YARN
    -> Hive (Data warehousing platform)
    -> Impala (introduction)
    -> Sqoop (export & import tool)
    -> HBase (NoSQL database)
    -> Kafka (PubSub messaging engine)
    -> NiFi (dataflow manager)
    -> Spark ( PySpark )

 ==========================================================
    Materials:

	-> Presentations (PDFs)
	-> Code - Command sets & Programs
        -> Class Notes

        -> https://github.com/ykanakaraju/Hadoop-Basic
 ==========================================================      

    Big Data 
    --------

       Big Data is data that contains greater variety (unstructured & semu structured) arriving at
       increasing volumes and with velocity. (3 Vs of Big Data)   

	-> Volume
		-> Amount of data. We will have to process huge volumes of low-density & 
		   unstructured data. 
	-> Velocity
		-> Velocity is the fast rate at which data is received and act upon it. 

	-> Variety
 		-> Unstructured & Semi-structured data such as text, audio, video require 
		   additional preprocessing to derive meaning and support metatdata.


   Computing Cluster
   -----------------
        -> Is a unified entity containing a group of nodes whose cumulative resources can be used
	   to store and process huge amounts of data.	
    
   Hadoop
   ------
	-> Is an open source framework to store and process bigdata on a cluster 
	   made of commodity hardware. 

	-> Distributed Storage    : HDFS (Hadoop Distributed File System)
	-> Distributed Processing : MapReduce
	-> Cluster Manager	  : YARN  (added to Hadoop from Hadoop 2.0 onwards)

	=> Written in Java


   HDFS (Hadoop Distributed File System)
   -------------------------------------
	
      => Distributed Storage Framework

      => Data is split into blocks of 128 MB each and are distributed across many machines.
	-> The blocks are distributed across many slave nodes
	-> Each blocks is replicated 3 times (default) on three different node for providing fail-safety.


      HDFS Architecture
      -----------------

	-> Name Node (NN)
		-> Runs on the master machine

		-> Maintains all the meta data in two files:
			-> EditLog (in-memory)
			-> FSImage (persistent file, stored on the disk)

	-> Data Node (DN)
		-> Runs on all the slave nodes.
		-> Send heart-beats to Name node once in 3 sec.
		    -> If three successive heartbeats are missed, then the datanode is considered as 
		       unavailable. 
		-> Data Nodes send block-reports to NN

			-> Block Report:
				-> Info about all the files and directories
				-> Available Disk Space, Total disk space, reserved disk space

	-> Secondary Name Node (SNN)
		-> Responsible for check Poinitng


        -> Balancer
		-> Balancer maintains the replication factor of file.
		-> Is a background process the periodically checks for under & over repicated blocks
		   and maintains the replication.

 
    Hadoop EcoSystem
    ----------------
	=> Various different components that provide different services and run on
	   top of Hadoop make up Hadoop-Ecosystem. 

		
		Execution Engines	: Mapreduce, Spark, Tez
		Analytical Process	: Hive, Drill, Impala
		ML Libraries		: Spark MLlib, Mahout
		Web Indexing & Search	: Lucene, Solr
		Messaging Engines & Streaming : Kafka, Storm
		Coordination Mgmt	: Zookeeper, Ambari
		Data Ingestion		: Sqoop, Flume


    Hadoop Distributions
    --------------------

	Distributions package lot of Hadoop Components as one unified distribution and provide
	management tools to make managing hadoop clusters easy and convinient.

	On-Premisis Clusters
	---------------------
	-> Cloudera 
	-> HortonWorks (merged with Cloudera)
	-> MapR

	Cloud Services
	--------------
	-> Amazon's AWS EMR (Elastic Map Reduce - managed Hadoop service)
	-> Microsoft Azure HDInsights (managed)
	-> Google's GCP DataProc (managed)


   Hadoop Config Files
   --------------------
	-> core-site.xml
	-> hdfs-site.xml
	-> mapred-site.xml


   HDFS File Operation - Working with HDFS
   ---------------------------------------
	=> Home Directory on Linux File System : /home/cloudera	
	=> Home directory on HDFS File System  : /user/cloudera

        Commands:  hadoop fs -<command>  OR  
		   hdfs dfs -<command> 
	
      
    Working with HDFS
    -----------------
	hadoop fs -ls
	hadoop fs -ls <dir>
	hadoop fs -ls -R <dir>   => recursive listing

	hadoop fs -mkdir <dir>
	hadoop fs -rmdir <dir>    => remove an empty dir

	hadoop fs -rm <file-Path>
	hadoop fs -rm -R <non-empty-directory>   => deletes the dir with all its content.
		ex: hadoop fs -rm -R dir1/datasets

	hadoop fs -cp <source-file-path> <destination-dir>

	hadoop fs -cat <file-path>

	hadoop fs -chmod 755  <filePath>
	hadoop fs -chmod -R 755 <DirPath>  (you need execute permisiion on the files)

	hadoop fs -getmerge dir1/*.txt localdir/dir1merged.txt
		=> Merged multiple HDFS files and copy that as one local-file

	hadoop fs -stat %o dir1/NYSE_dividends.txt
		=> get the stats of a file  (refer   hadoop fs -help stat)

	hadoop fs -setrep 5 dir1/NYSE_dividends.txt
	hadoop fs -setrep 5 dir1/*.txt
		=> Set the replication factor of a file


	Linux <=> HDFS
        ---------------	
	-copyFromLocal    ( copy from Linux to HDFS )
	-put

	-moveFromLoacl 	  ( move from Linux to HDFS )

	-copyToLocal	  ( copy from HDFS to Linux )
	-get	

	examples:
       
	hadoop fs -copyFromLocal <local-source> <hdfs-destination>
	     	hadoop fs -copyFromLocal datasets/NYSE_dividends.txt
		hadoop fs -copyFromLocal datasets/NYSE_dividends.txt dir1

	hadoop fs -D dfs.replication=5 -copyFromLocal <source> <destination>

	hadoop fs -put <local-source> <hdfs-destination>
		hadoop fs -put datasets/NYSE_dividends.txt
		hadoop fs -put datasets/NYSE_dividends.txt dir1
	
	hadoop fs -copyToLocal <hdfs-source> <local-destination>
	hadoop fs -get <hdfs-source> <local-destination>
		hadoop fs -copyToLocal dir1/datasets/wordcount_input.txt localdir
		hadoop fs -get dir1/datasets localdir

	hadoop fs -moveFromLocal localdir/NYSE_daily.txt dir2
	   
   ============================================
      YARN 
   ============================================

      -> YARN (Yet another resource negotiator)
      -> YARN is the default cluster manager for Hadoop.
      -> Introduced in Hadoop 2.0 as the Cluster Manager.

	
      -> YARN accepts job submissions, schedules them and allocate resource containser to 
	 different jobs


 	Basic Components of YARN :

	1. Resource Manager : One per cluster, master daemon
	2. Node Manager: One per node, slave daemon
	3. Application Master: One per application/job
			      (starts when job starts, and closed when the job is completed)
	4. Containers: Application processes where App Master sends the tasks to be executed.

	Typical application execution with YARN follows this flow:
	   => Client program submits the MR application to the RM, along with information to launch 
	      the application-specific Application Master.
	   => RM negotiates a container for the AM and launches the AM.
	   => AM boots and registers with the RM, allowing the original calling client to interface 
	      directly with the AM.
	   => AM negotiates resources (resource containers) for client application.
	   => AM gives the container launch specification to the NM, which launches a container for 
	      the application.
	   => During execution, client polls AM for application status and progress.
	   => Upon completion, AM deregisters with the RM and shuts down.
  

   ============================================
      MapReduce  (MR)
   ============================================

    -> MR is the default distributed parallel processing framework of Hadoop
 
   
    MapReduce
     
      -> MapReduce Execution Framework (How an MR program is executed on the cluster)

	1. Map Phase		
		-> Several map tasks are launched across the cluster
		-> Each mapper processes one input-split
		-> RecordReader creates (K, V) from each record in the inputsplit and calls map method
		-> Map method processes that record and writes the output as (K, V) pairs to the context
		-> map() methos is called as many times as there are records in the inputsplit.

	2. Reduce Phase
		2.1 Shuffle & Sort phase

		-> Will aggregate all the values of each unique key from across the outputs of all mappers
		-> Will have unique keys and grouped values (ex: ('hadoop', [1,1,1,,1,1,1 ...])
		-> The keys are automatically sorted. 

		2.2 Reducer Phase
		-> The output of the S&S will be divided into as many partitions as there are reducers.
			-> By default, system gives only one reducers
			-> But, we can configure number of reducers in code  (wordcountjob.setNumReduceTasks(2);)
		-> Will process each (K, Iterable[V]) pair and produces final output as (K, V) pairs.

      -> MapReduce Programming Paradigm (How to write a MR program)   

	 MapReduce Application Classes (that a developer has to write)

		1. Driver Class (required)
		     => Where you define all the application configurations.

		2. Mapper Class (required)
		     => Extends "Mapper" base class and implements some life cycle methods:
		     => Implements "map" method, where you write you application logic.		     

		3. Reducer Class (required only if aggregation is needed)
		     => Extends "Reducer" base class and implements some life cycle methods.
		     => Implements "reduce" method, where you write you application logic.

		4. ?
		     

		5. ?


    Hadoop Data Types
    -----------------
   
      => To have better serialization & deserialization of data, Hadoop uses its own custom 
	 data types. These are called Writable data-types. 

	    Java	    Hadoop
	   -------         --------------
	    String	   Text
	    Integer	   IntWritable
	    Long	   LongWritable
	    Double	   DoubleWritable
	    Bool	   BoolWritable
	    null	   NullWritable
	
	
   InputSplits
   -----------
	=> InputSplits are created by the framework as an integral number of records
	=> InputSplits are logical entities (pointer to the byte-positions)
	=> Created by the Framework.

	=> InputSplits  (represented by an "InputSplit") are created by "InputFormat" class.

	
   RecordReader
   ------------
	=> Creates (K, V) pairs from each record of the input-split and calls the "map" method
	   of the mapper program. 


 input-split (InputSplit)			  RecordReader
 -------------------------                         ------------

     hadoop hive mapreduce hdfs hadoop mapreduce  -> (0,"hadoop hive mapreduce hdfs hadoop mapreduce")      
						  ==>  map() -> (hadoop, 1) (hive, 1) (mapreduce, 1) (hdfs, 1) ..
		
    mapper reducer sqoop flume hadoop mapreduce   -> (51, "mapper reducer sqoop flume hadoop mapreduce")
						  ==>  map() -> (mapper, 1) (reducer, 1) (sqoop, 1) (flume, 1) ..

    mapreduce hive flume kafka hadoop hive	  -> (102, "mapreduce hive flume kafka hadoop hive")
						  ==>  map() -> (mapreduce, 1) (hive, 1) (flume, 1) (kafka, 1) ..



  Executing the MapReduce Program
  -------------------------------
	
      -> Export MR application as a Java Jar file

      -> hadoop jar <jar-file-path> <Driver-Class-Name> <Command-line-args>
	 hadoop jar /home/cloudera/wordcount.jar com.cts.WordCountJob wordcount_input.txt wcoutwordcount_input.txt



   Creating a MR program
   ---------------------

	-> Create a Java project

	-> Add the following JAR file to the Referenced Libraries:
		-> Right click on  Referenced Libraries -> Build Path -> Configure Build Path
		-> Click on "Add External JARS" and add the following:
			/usr/lib/hadoop/hadoop-common.jar
			/usr/lib/hadoop-0.20-mapreduce/hadoop-core.jar

	-> Create packages and start to write MR programs. 


  Mapper Life Cycle Methods
  -------------------------

    1. setup
	 -> Executed once at the beginning (even before map method is called)
	 -> Used for initialization, creating resources (like DB connections etc).

    2. map
	-> Executed once (after setup method) for every input (K, V) pair in the input-split
	-> Implement the buisiness logic to be executed per record.

    3. cleanup
	-> Executed once after map is called for all records
	-> Write any consolidated output if you have.
	-> Close the connections, clean the resources etc. 

    4. run
	-> Generally not implemented.	


  Reducer Life Cycle Methods
  -------------------------

    1. setup
	 -> Executed once at the beginning (even before reduce method is called)
	 -> Used for initialization, creating resources (like DB connections etc).

    2. reduce
	-> Executed once (after setup method) for every input (K, Iterable[V]) pair. 
	-> Implement the buisiness logic to be executed per record.

    3. cleanup
	-> Executed once after reduce methods is called for all records
	-> Write any consolidated output if you have.
	-> Close the connections, clean the resources etc. 

    4. run
	-> Generally not implemented.	

  
   Use-Case: 
   ---------
	From pseudo_facebook.tsv file find out the record with maximum value of 'likes_received'
	from among peoplw who are born between 1990 and 2000.

	Two MR programs:

	MR 1. Fetch the output for people born between 1990 and 2000
	MR 2. Fetch the record with highest 'likes_received' value from the output of MR 1

   ==================================
      Hive
   ==================================	
  
    -> Data Warehousing Platform built on top of Hadoop
	-> uses HDFS as storage layer
	-> uses MapReduce as execution engine.

    -> Hive provide a SQL-like wrapper (Hive Query Language) on top of MR, so the developers/analysts
       do not have to write MR programs. 

    -> Hive is NOT an RDBMS nor a NoSQL database.

	  
    Hive Components
    ----------------

	1. Hive Warehouse  ( directory:  /user/hive/warehouse )
		-> Is a directory on HDFS  
		-> There is a default database called "default" 
		    -> 'default' database does not have any separate db directory.
		-> We can create user-defined databases (schemas) 
		    -> Each database is stored as a separate directory in the warehouse directory  
		    -> Each table will have its own directory inside the database directory
		    -> All the data of 'managed tables' are stored as files in the table directory.

	2. Hive Metastore
		-> Hive stores all its meta-data in a Meta store service 
			-> Hive comes with an embedded database called 'Derby' as a metastore
		-> In prodution, usually MySQL database is used as metastore.


	
    How to interact with Hive
    -------------------------

	1. Command Line Interface (Shell)
		-> Hive shell
		-> Beeline shell (JDBC)

	2. Client Applications
		-> Can connect to hive using thrift, JDBC or ODBC

	3. HUE (Hive User Environment) - Beeswax 
	
   	
   Databases
   ---------
	show databases;	
	create database ctsdb1;
	drop database ctsdb1;        // used to drop an empty database
	describe database ctsdb1;

	show schemas;	
	create schema ctsdb1;
	drop schema ctsdb1;
	describe schema ctsdb1;


   Connecting to beeline shell
   ----------------------------

	$ beeline
	beeline> !connect jdbc:hive2://quickstart:10000 cloudera cloudera org.apache.hive.jdbc.HiveDriver

	0:jdbc:hive2://quickstart:10000> show databases;  


   Tables
   ------

CREATE TABLE empinfo (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

	
  Default Values in the CREATE command :
  --------------------------------------

	ROW FORMAT 		=> DELIMITED 
	FIELDS TERMINATED BY	=> Ctrl-A
	LINES TERMINATED BY	=> \n
	STORED AS		=> TEXTFILE	


desc empinfo
desc extended empinfo
desc formatted empinfo

=> from local file system (linux) : COPY operation

// append mode
LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/data/emp.txt' INTO TABLE empinfo;  

// overwrite mode  
LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/data/emp.txt' OVERWRITE INTO TABLE empinfo; 

=> from hdfs  : MOVE operation
// append mode
LOAD DATA INPATH '/user/cloudera/datasets/emp.txt' INTO TABLE empinfo;

// overwrite mode
LOAD DATA INPATH '/user/cloudera/datasets/emp.txt' OVERWRITE INTO TABLE empinfo;


  Schema on Read
  --------------
    => Hive does not validate the data while loading the data.
    => Validation against the schema of table happens while 'reading' (querying) the data
	 -> Hive shows NULL if some column did the not match the schema. 


External Tables
---------------
CREATE EXTERNAL TABLE empinfo_external (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED AS TEXTFILE
LOCATION '/user/cloudera/hivedata/empinfo_external';

LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/data/emp.txt' OVERWRITE INTO TABLE empinfo_external; 

=> External tables are pointed to a user-specified location (outside of hive warehouse) for data files.
=> The data of an external table is not owned by Hive.
=> If you drop an external table, only the schema is deleted. But not the data files.


  Partitioned Tables
  ------------------

	=> In Hive, Partitions provide a way of dividing a table into coarse-grained parts 
   	   based on the value of a partition column, such as a date or country.

        => A separate partition (directory) is create for every unique-value of the partitiond column

	=> The data files does not contain the values of the partition columns. 
		-> The partition column data comes from the directory name itself.

	Based on how we can load/insert data into a partitioned table, we can use two approaches:

        1. Static Partitioning
	    -> When you know exactly into which partition a specific file need to be loaded
	    -> We can use "LOAD DATA .. " command.

	2. Dynamic Partitioning
	   -> When we can not decide into partition a specific file need to be loaded. 
	
	   -> The data file is first loaded into a base table
	   -> From the base table the data is inserted into partitioned table using "INSERT .. SELECT" command
	   -> We can not use "LOAD DATA ..." command.
	
	   -> NOTE: Make sure to set the following config varibale:
		  set hive.exec.dynamic.partition.mode=nonstrict
	


 Partitioned Tables
 -------------------

CREATE TABLE students_partition_static (id INT, name STRING, mobile STRING, email STRING, project STRING)
PARTITIONED BY (country STRING)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/data/students_IN.txt' 
INTO TABLE students_partition_static
PARTITION (country='India');


CREATE TABLE students_partition_dynamic (id INT, name STRING, mobile STRING, email STRING, project STRING)
PARTITIONED BY (country STRING, batch INT)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;


CREATE TABLE students
(id INT, name STRING, mobile STRING, email STRING, project STRING, country STRING, batch INT)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/data/students_country_batch.csv' INTO TABLE students;

set hive.exec.dynamic.partition.mode=nonstrict

INSERT INTO students_partition_dynamic
PARTITION (country, batch)
SELECT id, name, mobile, email, project, country, batch FROM students;


INSERT INTO students_partition_dynamic
PARTITION (country='India', batch)
SELECT id, name, mobile, email, project, batch FROM students
WHERE country='India';


INSERT INTO students_partition_dynamic
PARTITION (country='India', batch=1)
SELECT id, name, mobile, email, project FROM students
WHERE country='India' and batch=1;

// The following command does not work
// Dynamic partition cannot be the parent of a static partition

INSERT INTO students_partition_dynamic
PARTITION (country, batch=1)
SELECT id, name, mobile, email, project, country FROM students
WHERE batch=1;


LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/data/students_IN_csv.txt' 
INTO TABLE students_partition_dynamic
PARTITION (country='India', batch=4);


  What type of colums can we use for partitioning
  ------------------------------------------------
     => We have to use "low-cardinality" columns and are used in queries 
	as partition columns
     => You need to have sufficient big amount of files in each partition.

 
   Bucketting
   -----------
	
	=> In Hive, Buckets provide an extra structure to the data to improve query efficiency

	-> We can buckettize data of a table or a partition into fixed number of buckets based
	   on a cluster column
		-> Data is split across multiple buckets by applying hash function on cluster columns.

CREATE TABLE students_bucket 
(id INT, name STRING, mobile STRING, email STRING, project STRING, country STRING, batch INT)
CLUSTERED BY (id) 
INTO 4 BUCKETS;

set hive.enforce.bucketing = true;

INSERT INTO students_bucket SELECT * from students;

CREATE TABLE students_bucket 
(id INT, name STRING, mobile STRING, email STRING, project STRING, country STRING, batch INT)
CLUSTERED BY (id) INTO 4 BUCKETS
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;

CREATE TABLE students_part_bucket 
(id INT, name STRING, mobile STRING, email STRING, project STRING, country STRING)
PARTITIONED BY (batch INT)
CLUSTERED BY (id) INTO 3 BUCKETS
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;

insert into students_part_bucket 
partition(batch)
select id, name, mobile, email, project, country, batch from students;

select * from students_part_bucket where batch = 1 and id = 9;
--------------------------------------------

 Sampling Queries
 ----------------
   Key Word: TABLESAMPLE

   => Bucket Level Sampling
	select * from students_bucket tablesample (bucket 1 out of 2 on id);


   => Block Level Sampling
	-> some specifies number of blocks are fetched based on query parameter (such as 20 percent, 10M etc)

	select * from students tablesample (50 percent);
	select * from students tablesample (50M);
	
   => Row Level Sampling
	-> some specifies number of rows are fetched from each input split
	
	select * from students tablesample (5 rows);

 --------------------------------------------------

   Managing Outputs  
   -----------------

    -> Store the output of a query in a table

		insert into empinfo_2 select * from empinfo;    	(APPEND MODE)
		insert overwrite table empinfo_2 select * from empinfo; (OVERWRITE MODE)

	
    -> Store the output of a query in a local (linux) directory (in a desired format)

		INSERT OVERWRITE LOCAL DIRECTORY '/home/cloudera/hive_data/students_pipe'
		SELECT * FROM students;

		INSERT OVERWRITE LOCAL DIRECTORY '/home/cloudera/hive_data/students_pipe'
		ROW FORMAT DELIMITED
		FIELDS TERMINATED BY "|" 
		SELECT * FROM students;


    -> Store the output of a query in a HDFS directory (in a desired format)

		INSERT OVERWRITE DIRECTORY '/user/cloudera/hivedata/students_pipe'
		ROW FORMAT DELIMITED
		FIELDS TERMINATED BY "|" 
		SELECT * FROM students;
	  
   
    Hadoop File Formats
    -------------------	 
	
	-> Text File (default)

	-> Sequence File	-> Data is stored in (K, V) pairs

	-> Avro File		-> Supports schema evolution 
				   The schema itself is stored in the data file (as json content)

	-> ORC file		-> Is a columnar format file

	-> Parquet File		-> Is a columnar format file

    
INSERT OVERWRITE LOCAL DIRECTORY '/home/cloudera/hive_data/students_seq'
STORED AS SEQUENCEFILE
SELECT * FROM students;

INSERT OVERWRITE LOCAL DIRECTORY '/home/cloudera/hive_data/students_avro'
STORED AS AVRO
SELECT * FROM students;

INSERT OVERWRITE LOCAL DIRECTORY '/home/cloudera/hive_data/students_parquet'
STORED AS PARQUET
SELECT * FROM students;

INSERT OVERWRITE LOCAL DIRECTORY '/home/cloudera/hive_data/students_orc'
STORED AS ORC
SELECT * FROM students;


 Sorting
 -------
	
	=> ORDER BY col1
	   -> Generates parallel total sort
	   -> select * from students order by name	

	=> SORT BY col1
	    -> Generated a sorted file per reducer
	    -> select * from students sort by name
	
	=> DISTRIBUTE BY col1
	    -> Attached a partitioner to the reducers based on a columns
            -> select * from students distribute by country sort by name		

	=> CLUSTER BY
	    -> Is a special case DISTRIBUTE BY where you are distributing and sorting by the same column.
	    -> select * from students cluster by country
               ( is a shortcut of select * from students distribute by country sort by country )

	 
  Multi-Table Insert Command
  --------------------------

	from students
    insert into students_in
    select * where country = 'India'
    insert into students_usa
    select * where country = 'USA'
    insert into students_uk
    select * where country = 'UK';
	
Alter command
-------------
alter table empinfo_2 rename to empinfo_3;
alter table empinfo_3 add columns (dob string, age int);	
alter table empinfo_3 change age age2 STRING;
alter table students_partition_static PARTITION (country='USA') RENAME TO PARTITION (country='US');

ALTER TABLE students_partition_static ADD PARTITION(country='Japan')

LOCATION '/user/hive/warehouse/ctsdb1.db/students_partition_static/c=Japan';

hadoop fs -put /home/cloudera/hive/Hive/data/students_IN.txt /user/hive/warehouse/ctsdb1.db/students_partition_static/c=Japan

ALTER TABLE emp_tp_immutable SET TBLPROPERTIES ("immutable"="false")



Views
-----

CREATE VIEW students_us_view AS SELECT * FROM students WHERE country='USA';
select * from students_us_view;
drop view students_us_view;


Complex Data Types
------------------

sample complex data
-------------------
Apple/Grapes/Orange,4306:Sneha/4307:Renu,Sneha/90
Apple/Microsoft/Google,4306:Renu/4307:Anita,Raju/100
Apple/Micromax/Samsung,4306:Raju/4307:Harsha,Harsha/80


table to store complex-data
---------------------------
CREATE TABLE complex_data
(col1 array<string>,
 col2 map<int,string>,
 col3 struct<name:string,score:int>)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
COLLECTION ITEMS TERMINATED BY '/'
MAP KEYS TERMINATED BY ':'
STORED AS TEXTFILE;
 
select * from complex_data;
select col1[0], col1[1], col2[4306], col2[4307], col3.name, col3.score from complex_data;

Table Properties
-----------------

CREATE TABLE emp_tp_metadata 
(empid INT, empname STRING, salary DOUBLE, deptid SMALLINT) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ',' 
LINES TERMINATED BY '\n' 
STORED AS TEXTFILE 
TBLPROPERTIES ('creator'='raju', 'created_at'='2021-11-18 10:00:00')

-> SHOW TBLPRPPERTIES emp_tp_metadata 
-> SHOW TBLPRPPERTIES emp_tp_metadata('creator')


Immutable Tables
----------------
As of Hive 0.13.0, a table can be made immutable by creating it with 
TBLPROPERTIES ("immutable"="true"). 

The default is "immutable"="false".

INSERT INTO behavior into an immutable table is disallowed if any data is already present,
although INSERT INTO still works if the immutable table is empty. 

The behavior of INSERT OVERWRITE is not affected by the "immutable" table property.

An immutable table is protected against accidental updates due to a script loading data 
into it being run multiple times by mistake. The first insert into an immutable table 
succeeds and successive inserts fail, resulting in only one set of data in the table, 
instead of silently succeeding with multiple copies of the data in the table. 

CREATE TABLE emp_tp_immutable
(empid INT, empname STRING, salary DOUBLE, deptid SMALLINT) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ',' 
LINES TERMINATED BY '\n' 
STORED AS TEXTFILE 
TBLPROPERTIES ("immutable"="true")


insert into emp_tp_immutable select * from empinfo;  => Allowed if the table is empty
insert into emp_tp_immutable select * from empinfo;  => Not Allowed if the table is non-empty
insert overwrite emp_tp_immutable select * from empinfo;  => Allowed 
LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/data/emp.txt' INTO TABLE emp_tp_immutable  => Allowed

ALTER TABLE emp_tp_immutable SET TBLPROPERTIES ("immutable"="false")


  Transactional Tables
  --------------------

      -> Transactional tables support 'update' and 'delete' operations  
	
      -> Table must use ORC file format
      -> Table must be bucketted
      -> Add TBLPROPERTIES('transactional'='true') to create command
      -> Set the following configurations:	
		set hive.support.concurrency = true;
		set hive.enforce.bucketing = true;
		set hive.exec.dynamic.partition.mode = nonstrict;
		set hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
		set hive.compactor.initiator.on = true;
		set hive.compactor.worker.threads = 1;



CREATE TABLE user_acid (userid int, name string, age int, gender string)
CLUSTERED BY (userid) INTO 5 buckets 
STORED AS ORC
TBLPROPERTIES('transactional'='true');


Run the following commands
--------------------------
set hive.support.concurrency = true;
set hive.enforce.bucketing = true;
set hive.exec.dynamic.partition.mode = nonstrict;
set hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
set hive.compactor.initiator.on = true;
set hive.compactor.worker.threads = 1;


insert data
-------------
INSERT INTO user_acid(userid, name, age, gender) VALUES 
(1, 'Raju', 40, 'male'), 
(2, 'Raghu', 42, 'male'),
(3, 'Vinay', 20, 'male'),
(4, 'Amrita', 14, 'female'),
(5, 'Aditya', 14, 'male'), 
(6, 'Keertana', 12, 'female'),
(7, 'Pranav', 24, 'male'), 
(8, 'Hari', 24, 'male'),
(9, 'Syamala', 23, 'female')

update user_acid set name='ADITYA...' where userid = 5;

delete from user_acid where userid = 5;


 Joins
 -------------

Supported Joins => inner, outer joins (left, right, full), left_semi

CREATE TABLE IF NOT EXISTS stocks 
(stock_exchange STRING, symbol STRING, trxn_date DATE, open DOUBLE, 
high DOUBLE,low DOUBLE, close DOUBLE, volumn INT,adj_close DOUBLE)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n' 
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '/home/cloudera/datasets/stocks' 
OVERWRITE INTO TABLE stocks; 


CREATE TABLE IF NOT EXISTS dividends 
(stock_exchange STRING, symbol STRING, trxn_date DATE, dividend DOUBLE)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n' 
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '/home/cloudera/datasets/dividends' 
OVERWRITE INTO TABLE dividends; 

SELECT s.trxn_date, s.symbol, s.adj_close, d.dividend
FROM stocks s JOIN dividends d
ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol; 

SELECT s.trxn_date, s.symbol, s.adj_close, d.dividend
FROM stocks s LEFT OUTER JOIN dividends d
ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol; 


 Left Semi
 ---------	
      => Is like inner join but the data comes only from the left side table.
      => The behaviour of the left-semi join is as per the following sub-query:

	select * from emp where deptid IN (select deptid from dept)

SELECT * FROM stocks s LEFT SEMI JOIN dividends d
ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol; 


  Bucket Map Joins
 ------------------

CREATE TABLE IF NOT EXISTS stocks_bkt_16 
(stock_exchange STRING, symbol STRING, trxn_date DATE, open DOUBLE, 
high DOUBLE,low DOUBLE, close DOUBLE, volumn INT,adj_close DOUBLE)
CLUSTERED BY (trxn_date, symbol) INTO 16 BUCKETS 
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;


CREATE TABLE IF NOT EXISTS dividends_bkt_8 
(stock_exchange STRING, symbol STRING, trxn_date DATE, dividend DOUBLE)
CLUSTERED BY (trxn_date, symbol) INTO 8 BUCKETS 
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;


set hive.enforce.bucketing = true; 
set hive.optimize.bucketmapjoin = true;


INSERT INTO stocks_bkt_16 SELECT * FROM stocks SORT BY trxn_date, symbol;

INSERT INTO dividends_bkt_8 SELECT * FROM dividends SORT BY trxn_date, symbol;

SELECT /*+ MAPJOIN(dividends_bkt_8) */ s.trxn_date, s.symbol, s.adj_close, d.dividend
FROM stocks_bkt_16 s JOIN dividends_bkt_8 d
ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol;



  Sort Merge Joins
 -------------------

CREATE TABLE IF NOT EXISTS stocks_bkt_8 
(stock_exchange STRING, symbol STRING, trxn_date DATE, open DOUBLE, 
high DOUBLE,low DOUBLE, close DOUBLE, volumn INT,adj_close DOUBLE)
CLUSTERED BY (trxn_date, symbol) INTO 8 BUCKETS 
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;


INSERT INTO stocks_bkt_8 SELECT * FROM stocks SORT BY trxn_date, symbol;

set hive.auto.convert.sortmerge.join=true;
set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.auto.convert.sortmerge.join.nonconditionaltask=true;

SELECT /*+ MAPJOIN(dividends_bkt_8) */ s.trxn_date, s.symbol, s.adj_close, d.dividend
FROM stocks_bkt_8 s JOIN dividends_bkt_8 d
ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol;


 Explode, Lateral Views and Temporary Tables
 --------------------------------------------

1	1111111,11112222	email11@gmail.com,email1122@gmail.com
2	2222222,22223333	email22@gmail.com,email2222@gmail.com
3	3333333,33333444	email33@gmail.com,email3322@gmail.com
4	4444444,44445555	email44@gmail.com,email4422@gmail.com

   
create table tab_lateral (id int, phones array<string>, emails array<string>)
row format delimited 
fields terminated by '\t'
collection items terminated by ','
stored as textfile;

load data local inpath '/home/cloudera/hive/Hive/data/lateral_view_data.txt' 
overwrite into table tab_lateral;


1	1	1111111		email11@gmail.com
2	1	11112222	email1122@gmail.com
3	2	2222222		email22@gmail.com
4	2	22223333	email2222@gmail.com


TABLE 1
1	1111111		1
1	11112222	2
2	2222222		3
2	22223333	4		


TABLE 2
1	email11@gmail.com	1
1	email1122@gmail.com	2
2	email22@gmail.com	3
2	email2222@gmail.com	4



create table tab_lateral (id int, phones array<string>, emails array<string>)
row format delimited 
fields terminated by '\t'
collection items terminated by ','
stored as textfile;

load data local inpath '/home/cloudera/hive/Hive/data/lateral_view_data.txt' overwrite into table tab_lateral;

# note: expode can not be merged with any other columns as is.
# use lateral views if you want to do so.

select explode(phones) from tab_lateral;

select id, phones_l from tab_lateral 
lateral view explode(phones) p as phones_l;

select id, phones_l, emails_l from tab_lateral 
lateral view explode(phones) p as phones_l
lateral view explode(emails) e as emails_l;


-- This results in cross product of array values, which is not a good data representaion
-- Lets breakdown the data properly

Solution:

create temporary table if not exists tab_lateral_tmp_phones
(rowid int, id int, phone string)
row format delimited
fields terminated by '|';

insert overwrite table tab_lateral_tmp_phones
select row_number() over(), id, phones_l from tab_lateral 
lateral view explode(phones) p as phones_l;


create temporary table if not exists tab_lateral_tmp_emails
(rowid int, id int, email string)
row format delimited
fields terminated by '|';

insert overwrite table tab_lateral_tmp_emails
select row_number() over(), id, emails_l from tab_lateral 
lateral view explode(emails) p as emails_l;

create table if not exists tab_lateral_breakdown
(rowid int, id int, phone string, email string)
row format delimited
fields terminated by '|';

insert overwrite table tab_lateral_breakdown
select a.rowid, a.id, a.phone, b.email
from   tab_lateral_tmp_phones a, tab_lateral_tmp_emails b
where  a.rowid = b.rowid and a.id = b.id;

  Window Functions
 ------------------

	id	dept	salary	 row_number() 
	-------------------------------------	    	
	2	IT	50000	1
	8	IT	55000	2
	6	IT	55000	3
	7	IT	60000	4
	1	IT	60000	5
	12	IT	70000	6

	4	HR	40000	7
	10	HR	45000	8
	11	HR	45000	9
	5	HR	45000	10
			
	3	Sales	60000	11
	9	Sales	65000	12
 
SELECT symbol, trxn_date, close, LAG(close,1) OVER(PARTITION BY symbol) as yesterday_price 
FROM stocks LIMIT 40;

SELECT symbol, trxn_date, close, LAG(close,2) OVER(PARTITION BY symbol) as yesterday_price 
FROM stocks LIMIT 200;

SELECT symbol, trxn_date, close, LEAD(close,3) OVER(PARTITION BY symbol)  
FROM stocks LIMIT 400;

SELECT symbol, trxn_date, close, row_number() OVER(PARTITION BY symbol ORDER BY close)  
FROM stocks LIMIT 400;

SELECT symbol, trxn_date, close, 
sum(close) OVER(PARTITION BY symbol ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as cum_sum 
FROM stocks LIMIT 400;

select symbol, count(symbol) over(partition by symbol) as cnt from stocks;

SELECT symbol, trxn_date, close, 
row_number() OVER(PARTITION BY symbol ORDER BY close) as row_num,
rank() OVER(PARTITION BY symbol ORDER BY close) as rank,
dense_rank() OVER(PARTITION BY symbol ORDER BY close) as dense_rank
FROM stocks LIMIT 400;

   
   How to run a Hive script file ?
   -----------------------------
   $ hive -f '/home/cloudera/hive/Hive/churn_modelling.hql' 
   NOTE: Run the above command from the command line, not from shell.

 
   How to execute a HQL statement in interactive mode
   -------------------------------------------------
   hive -e 'select * from ctsdb1.empinfo'
   NOTE: Run the above command from the command line, not from shell.


  =============================================================================
	IMPALA
  =============================================================================
  
   -> A sql query engine built using massivly parallel processing (MPP) query engine
   -> For processing huge amount of data (PetaByte scale) stored in Hadoop clusters (HDFS)
   -> Uses the same metastore as well as the warehouse as that of Hive
   -> Uses the same query language as that of Hive (Hive QL)
   -> Written in C++ & Java
   -> High performance and Low Latency.
   -> Supports in-memory data processing.  
   -> Developed by Cloudera
	-> Shipped with all major Hadoop managed cloud service providers.


   => Impala does not support loading data from local file system
	-> LOAD DATA LOCAL INPATH ..  command is not supported.

   => Any metadata changes happened outside of impala such as creating databases, tables etc
      will not be reflected automatically in impala. You have to run the 'invalidate metadata'
      command to refresh metadata changes.

		=> invalidate metadata;

   => Whenever data is loaded into the tables from outside of impala, the data changes are not
      automatically reflected in impala. You have to run 'refresh <table-name>' command

		=> refresh empinfo2;

    When to use Impala ?
    --------------------

    => Because Impala uses in-memory computations, it does not provie run-time fail-safety.
        -> If any of the servers/nodes running the impala queries fail, the job will be aborted.

    => Use Hive for long-running batch queries
       Use impala for short-running interactive/batch queries

    => Hive is more suited for long running batch jobs, such those involving ETL.
    => Impala is good for real-time queries while Hive is recommended for large batch jobs.
 
 =============================================================
  Sqoop  
 =============================================================

      SQL-Hadoop  => Tool that moves data back and forth between SQL databases and Hadoop
       
      Sqoop is structured data ingestion tool 

      Sqoop used MR as execution engine
	-> Sqoop launches map-only jobs. 



     sqoop import --connect jdbc:mysql://localhost:3306/ctsdb 
	--username root 
	-P 
	--table users 
	--num-mappers 1 
	--target-dir /user/cloudera/sqoop1/users1  


sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users --num-mappers 1 --target-dir /user/cloudera/sqoop1/users1 

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users -m 1 --target-dir /user/cloudera/sqoop1/users2 --fields-terminated-by '\t'

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users --columns 'id,name,age' -m 1 --target-dir /user/cloudera/sqoop1/users3 --fields-terminated-by '|'

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users --columns 'phone,age,name' -m 1 --target-dir /user/cloudera/sqoop1/users4 --fields-terminated-by ','

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'select * from users where 1=1 and $CONDITIONS' -m 1 --target-dir /user/cloudera/sqoop1/users5 --append

NOTES: While using query:
	- where clause is a must in a query and it should have $CONDITIONS in it.
	- if you are using " (double-quotes) to enclose the query use AND \$CONDITIONS 
	  (eg:  --query "select * from users where 1=1 AND \$CONDITIONS")
	- if you are using ' (single-quotes) to enclose the query use AND $CONDITIONS 
	  (eg:  --query 'select * from users where 1=1 AND $CONDITIONS')

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'select country, count(1) as total from students where 1=1 and $CONDITIONS group by country' -m 1 --target-dir /user/cloudera/sqoop1/students3

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students -m 1 --target-dir /user/cloudera/sqoop1/students4 --enclosed-by '"'
	
-- Storing and AVRO & SEQUENCE file formats
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users --num-mappers 1 --target-dir /user/cloudera/sqoop1/users_avro --as-avrodatafile 	

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users --num-mappers 1 --target-dir /user/cloudera/sqoop1/users_seq --as-sequencefile

-- Using multiple mappers

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students -m 3 --target-dir /user/cloudera/sqoop1/students5 --split-by 'id' 

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students -m 3 --target-dir /user/cloudera/sqoop1/students6 --split-by 'country' 

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students -m 2 --target-dir /user/cloudera/sqoop1/students7 --split-by 'batch' --delete-target-dir

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students -m 3 --target-dir /user/cloudera/sqoop1/students8 --split-by 'batch' --delete-target-dir

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table userspk -m 3 --target-dir /user/cloudera/sqoop1/userspk 


Import Modes
-------------
  
   default: error if target directory exists

   --append: adds new files to the existing directory
   --delete-target-dir : overwrites the content of the existing directory

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students -m 1 --target-dir /user/cloudera/sqoop1/students1 --direct

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students -m 1 --target-dir /user/cloudera/sqoop1/students1 --direct --append

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students -m 1 --target-dir /user/cloudera/sqoop1/students1 --direct --delete-target-dir


----- import directly to hive -------



------ import entire database (import-all-tables) ------
sqoop import-all-tables --connect jdbc:mysql://localhost:3306/ctsdb --username root -P -m 1 --warehouse-dir /user/cloudera/sqoop1/ctsdb 


--------------------
Export
--------------------
sqoop export --connect jdbc:mysql://localhost:3306/ctsdb       
             --table users2
	     --username root 
	     -P                           			
	     -m 1                                       			
             --export-dir /user/cloudera/sqoop/demo    		
             --input-fields-terminated-by ','  

sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop1/users1 

sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop1/users2 --input-fields-terminated-by '\t' 

sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop1/users3/part-m-00000 --input-fields-terminated-by '|' --columns 'id,name,age'

sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop1/users3/part-m-* --input-fields-terminated-by '|' --columns 'id,name,age'

sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop1/users4 --columns 'phone,age,name'

sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop1/users5

sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop1/users_avro

-- does not work - sequence file can not be exported
sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop1/users_seq

sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table students2 --username root -P -m 1 --export-dir /user/cloudera/sqoop1/students4 --input-enclosed-by '"'

-- updating the data in mysql
sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop1/users1 --update-key 'id'

-- this will not work as expected  (because the table has no primary key)
-- this simply adds the records, but not update 
sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir  /user/cloudera/sqoop1/users1 --update-key 'id' --update-mode allowinsert

-- here, --update-key 'id' is a promary-key of userspk table
-- hense it will upsert the records properly.
sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table userspk --username root -P -m 1 --export-dir  /user/cloudera/sqoop1/users1 --update-key 'id' --update-mode allowinsert
 

------ create hive tables -----------
sqoop create-hive-table --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users --fields-terminated-by ',' --hive-table sqoopdb.users
sqoop create-hive-table --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students --fields-terminated-by '|' --hive-table sqoopdb.students

------- eval command ---------------------------
sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'select * from students limit 5'
sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'create table t1 (col1 int, col2 varchar(30))'
sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'insert into t1 (col1, col2) values(1, "raju"), (2, "ramesh"), (3, "ramya")'
sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'update t1 set col2="RAJU" where col1=1'
sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'delete from t1 where col1=1'
sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'drop table t1'
sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'show tables'

------- codegen command --------------
sqoop codegen --connect jdbc:mysql://localhost:3306/ctsdb --table users --class-name Users --username root -P



 ==================================
    HBase
 ==================================
	
   NoSQL (non-relational) databases:
	-> NoSQL databases offer unlimited scalability
	-> NoSQL databases are distributed databases running on clusters	
	   -> Each table is partitioned (sharded) and replicated across many nodes
	   -> Your query load is distributed among many machines
	-> NoSQL Databases does not following struct normalization and any regid schema. 
	-> Non-relational and denormalized (lot of columns are stored in the same table)
  
   Types of NoSQL Databases
	-> Column-family or wide-column oriented databases (Ex: HBase, Cassandra)
	-> Document oriented databases (Ex: MongoDB, CouchDB)
        -> Key-Value Stores (Ex: DynamoDB, Redis)
	-> Graph Databases (Ex: Neo4J)	

   Limitations of Hadoop for transactional processing:
	-> Unstructured Data
	-> No Random Access
	-> High latency
	-> Not ACID complaint

   HBase
	-> Is distributed database management system that is part of Hadoop ecosystem.
	-> Based on "Google Big Table" architecture
	-> Uses HDFS as storage layer
		-> Distributed Storage
		-> Fault Tolerant. 

   HBase Vs RDBMS
   
	-> HBase does not support SQL
	-> HBase only supports a basic set of operations (CRUD) on a single row. 
		-> All operations are applied at "Row-Level"
		-> No Joins, No GroupBy, No Sub-Queries
	-> Denormalized design
	-> Uses column-oriented storage
		(more like a Map than a table)
	-> Limited ACID support
		-> ACID complaint only at row-level
	-> No Data types in HBase
		-> All data is stored a 'bytearrays'


  Apache Phoenix

       => Apache Phoenix is an open source, massively parallel, relational database engine 
	  supporting OLTP for Hadoop using Apache HBase as its backing store


   Working with HBase
   ------------------

hbase shell   -> run the HBase shell and launches an hbase instance
------------------------------------------
create 'emp', 'contact', 'basic', 'skills', 'salary', 'other' 
create 't1', {NAME => 'c1', VERSIONS => 5, BLOCKCACHE => 'false'}, {NAME => 'c2', VERSIONS => 2}
------------------------------------------
put 'emp', '1', 'basic:empid', '100~~~~'
put 'emp', '1', 'basic:name', 'Raju~~~~'
put 'emp', '1', 'basic:age', '45~~~~'
put 'emp', '1', 'contact:phone', '9246657890~~~~'
put 'emp', '1', 'contact:email', 'raju@gmail.com~~~~'
put 'emp', '1', 'skills:rdbms', 'mysql,oracle~~~~'
put 'emp', '1', 'salary:basic', '40000~~~~'
put 'emp', '1', 'salary:tds', '10%~~~~'

put 'emp', '2', 'basic:empid', '101'
put 'emp', '2', 'basic:name', 'Ramesh'
put 'emp', '2', 'basic:dob', '2000-01-01'
put 'emp', '2', 'contact:mobile', '9246657890'
put 'emp', '2', 'contact:emailid', 'raju@gmail.com'
put 'emp', '2', 'skills:rdbms', 'mysql,oracle'
put 'emp', '2', 'salary:basic', '40000'
put 'emp', '2', 'salary:hr', '8000'

put 'emp', 'A10', 'basic:empid', '103'
put 'emp', 'A10', 'basic:fname', 'Mahesh'
put 'emp', 'A10', 'basic:dob', '2000-02-01'
put 'emp', 'A10', 'contact:city', 'Hyderabad'
put 'emp', 'A10', 'contact:state', 'TS'
put 'emp', 'A10', 'contact:email', 'ramesh@gmail.com'
put 'emp', 'A10', 'skills:nosql', 'hbase'
put 'emp', 'A10', 'skills:rdbms', 'oracle'
put 'emp', 'A10', 'salary:basic', '40000'
put 'emp', 'A10', 'salary:travel', '5000'
put 'emp', 'A10', 'salary:hra', '10000'


put 'emp', '10', 'basic:empid', '104'
put 'emp', '10', 'basic:firstname', 'Raju'
put 'emp', '10', 'basic:age', '45'
-------------------------------------------------
scan 'emp'    // show all the data
scan 'emp', {COLUMNS => ['basic:empid', 'basic:name']}
scan 'emp', {COLUMNS => ['basic:empid', 'basic:name', 'salary:basic'], TIMERANGE => [1637667918885, 1637668214800]}
--------------------------------------------------

get 'emp', '1'
get 'emp', '1', {COLUMN => ['basic:empid', 'basic:fname', 'basic:dob', 'contact:city']}
get 'emp', '1', {COLUMN => ['basic:empid', 'basic:name', 'basic:dob', 'contact:city'], VERSIONS => 3}
get 't1', '1', {COLUMN => ['f1:name', 'f2:age'], VERSIONS => 5}
get 'emp', '1', {COLUMN => ['basic:empid', 'basic:name', 'basic:age', 'contact:phone'], VERSIONS => 4}

 ---------------------------------------------------
 
 put 't1', '001', 'c1:name', 'Raju~~~~~'
 put 't1', '001', 'c1:age', '45~~~~~'
 put 't1', '001', 'c2:city', 'Hyderabad~~~~~'
 put 't1', '001', 'c2:name', 'Telangana~~~~~'

 ------- delete --------------------
 delete 'emp', '10', 'basic:empid'

##------------ alter an hbase table -------------------

alter 'emp', NAME => 'basic', VERSIONS => 4 
alter 't1', NAME => 'f1', VERSIONS => 3 
alter 't1', NAME => 'f3', VERSIONS => 3
alter 't1', NAME => 'f3', METHOD => 'delete'

----- disable ----------
disable 'emp'

----- enable ----------
enable 'emp'

##------------ alter an hbase table -------------------
alter 'emp', NAME => 'basic', VERSIONS => 4 
alter 't1', NAME => 'f1', VERSIONS => 3 
alter 't1', NAME => 'f3', VERSIONS => 3
alter 't1', NAME => 'f3', METHOD => 'delete'

-------  truncate  ----------------
truncate 't1'


 => HBase-Hive integration  ==> to be discussed ..



 
 











