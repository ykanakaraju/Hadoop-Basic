
 Curriculum   (20 sessions - 80 hours)
 -------------------------------------
  Introduction to Big Data & Hadoop
  HDFS
  YARN
  MapReduce (using Java)
  Hive 
  Impala - Basics
  Sqoop  
  HBase - Basic Operations
  Kafka (using Java)      
  Spark (PySpark)  


  Training Materials
  ------------------
    -> PDF Presentations
    -> Code modules, command documents, programs
    -> Class Notes
    -> Databricks Notebooks
    -> GitHub: https://github.com/ykanakaraju/Hadoop-Basic


  Pre-requisites
  --------------
     -> SQL
     -> Java 
     -> Python 


  Big Data Introduction
  ---------------------   
     
     Type of data so large and complex that your traditional data processing systems
     can not reasonably store and process. 

   Big Data scenarios
   ------------------
       -> The volume to be handled is very high (much higher than 1 TB)
       -> You want to support real-time analytics (velocity)
       -> You data is growing very fast, so that in no time your system becomes inadeqaute. 
       -> Process unstructured, semi-structured data etc.
    
  Big Data Characteristics
  ------------------------
       -> Volume
       -> Velocity
       -> Variety 
       -> Veracity (correctness of the data)
       -> Value (extracting insights from the data)   
    

  Why your traditional systems have big data problem
  --------------------------------------------------
   -> Because your traditional systems ( that exists in pre-internet era ) are not designed
      for internet scale applications.
 
   -> They are designed to run on a single machine. The single machine has only limited 
      amount of resources.


  Scalability Options
  -------------------
    -> Scale up  (Vertical Scaling)   -> Migrate to a machine with higher configuration
    -> Scala out (Horizontal Scaling) -> Use many machines instead of one. 
                                      -> Use computing cluster

   What is a Computing Cluster?
   ----------------------------
   A unified network of nodes (machines) whose cumulative resources can be used to store
   or process data as if you are working with a single machine. 


   What is a Framework ?
   ---------------------
    A framework is a set of libraries that implement all the basic low-level functionalities
    that the framework is intended for. 

    The users of the framework should focus on the business-logic and implement their solutions 
    as per the guidelines provided by the framework. 


    What is the solution to storage & processing problems associated with Big Data
    ------------------------------------------------------------------------------
    -> Hadoop provides a solution for big-data problems.

 
    What is Hadoop ?
    ----------------     
    Hadoop is a Open Source Framework to Store and Process data in a distributed way 
    using a cluster of commdity harware. 

     -> Components of Hadoop:
	   -> Distributed Storage Solution (HDFS)
	   -> Distributed Processing Solution (MapReduce)
           -> Resource Management (YARN)


   Hadoop Distributions
   --------------------
	Apache Hadoop => Open source
	
	On-Prem & Hybrid Cloud => Cloudera & MapR

	Managed Cloud Solutions 
		-> AWS   : AWS Elastic MapReduce (EMR)
		-> Azure : HDInsignts
	        -> GCP   : DataProc 


    HDFS (Hadoop Distributed File System)
    --------------------------------------

      -> Stores data in the form of "blocks" of 128 MB each
      -> Each block is replicated on other nodes so many times as defined by a 'replication-factor'
           (default rep-factor is 3)      
      
     Q1:  If I want to stote 1024 MB (1 GB) file on HDFS, how much physical storage space 
          is required (using default configs) => 3 GB   

     Q2:  If you want to store 1034 MB file, how many unique blocks are created on HDFS?
	  -> 8 blocks of 128MB each & 1 block of 10 MB
         
	

   Hadoop EcoSystem
   ----------------   
     -> Refers to a set of tools/frameworks/software that run on top of hadoop to solve
        various big data related problems.          

     -> Popular Components in Hadoop EcoSystem             

          -> Resource Management Tool:     YARN
          -> Coordination Management Tool: ZooKeeper
          -> Execution Frameworks: MapReduce, Spark, Tez
          -> SQL-Analytics: Impala, Drill, Hive
	  -> Streaming Data Processing: Kafka, Storm, Spark Streaming
	  -> Machine Learning: Mahout, Spark MLLib
          -> Scripting: Pig
          -> NoSQL database: HBase
          -> Data ingestion tools: Flume, Sqoop

  =======================================================================
     HDFS (Hadoop Distributed File System)
  =======================================================================

   -> Storage Solution
   -> Stores data as blocks of 128 MB 
   -> Each block is replicated.

   HDFS Architecture
   -----------------

   Hadoop uses Master-Slave Architecture


   HDFS Daemons 
   ------------
     -> Daemon: Continuosly running background job. 

     1. Name Node (NN)	
	-> runs on the master machine (master daemon)
        -> recieves heart-beats from DNs.
	   -> if three successive heart-beats are missing from any DN, it is considered as 'dead'
        -> NN maintains two "meta-data" files
	    
           1. FSImage
		-> Persistent disk file
                -> Will contain all the meta-data info from the time that the NN service is started
                   upto the last checkpoint.

	   2. EditLog
		-> Maintained in memory (RAM)
		-> All incrmental changes are recorded in this file.        

     2. Data Node (DN)
	-> runs on every slave machine (salve daemon)
  	-> every DB sends heart-beats to NN once in every 3 seconds
 
        -> In every heart-beat, DN sends "block-report" of that node
           -> A 'block-report' consists of such info as storage data, which files, which blocks, 
              rep-factors, permissions, ...        
    
     3. Secondary Name Node (SNN)
          -> responsible for check-pointing process
              -> check-pointing is a process of merging the edit-log file with fsimage file.

     4. Balancer 
	  -> Ensures that the replication-factor of the files is maintained. 

          -> A file can become "under-replicated" if one nodes that contain the blocks of the file
             goes down, or if the rep-factor of the file is increased. Then balencer creates
             additional replicas of the blocks of that file.  

	  -> A file can become "over-replicated" if one node that contain the blocks that was 
             previously down and comes up again, or if the rep-factor of the file is decreased.
             Then balencer removes the replicas of the blocks of that file beyond the rep-factor.   


   HDFS High Availability
   ----------------------
     
      => Availability : Means that system do not have any down-time. Always available to the client.
    
    
      => Hadoop High Availability is introduced in Hadoop 2.0 onwards. 
	   -> We have "Active NN" and "Stand-by Name Node"

      => Hadoop 1.x is not high availble system.
	   -> In Hadoop 1.x, Name Node is a 'Single Point of Failure'    


   HDFS File Operations
   --------------------

       => HDFS is non-editable file system.

       Default Paths:

        * Cloudera Local Home :  /home/cloudera
        * HDFS Home: 		 /user/cloudera
        
        HDFS FS Command Syntax:  $hadoop fs -<command-name> <parameters>
                                 $hdfs dfs -<command-name> <parameters>



 	Default Paths ( cloudera is the user-name ):

        * Cloudera Local Home :  /home/cloudera
        * HDFS Home: /user/cloudera

 	Configuration Files:
	
	* /etc/hadoop/config/core-site.xml
	* /etc/hadoop/config/hdfs-site.xml
	* /etc/hadoop/config/mapred-site.xml

 	There are individual folders for each node in the cluster in directories as shown below :
  
        * /etc/hadoop/conf.cloudera.hdfs1
	* /etc/hadoop/conf.cloudera.mapred1
	* /etc/hadoop/conf.cloudera.yarn1

 	All daemon log files are written under:

	* /var/log/hadoop/

	
   Common HDFS File Operations
   ---------------------------

	 * hadoop fs -help   				=> lists all the commands with a brief discription
	 * hadoop fs -help <command-name>  
	 * hadoop fs -chmod 755 <file/dir>
	 * hadoop fs -chmod -R 755 <file/dir>		=> change permission for a file/directory
	 * hadoop fs -cat <file-name>
	 * hadoop fs -ls
	 * hadoop fs -ls <dir-path>
	 * hadoop fs -ls -R  <dir-path>			=> Recursive listing of files and directories

	 * hadoop fs -cp <source file> <target dir>     => Copies within HDFS frpm source to target
	 * hadoop fs -mv <source file> <target dir>     => Moves within HDFS frpm source to target
	 * hadoop fs -rm -r <file-name/directory-name>
		 
	 // copy from Linux to HDFS
	 * hadoop -copyFromLocal <local file path>  <hdfs file path> 
	 * hadoop -put <local file path>  <hdfs file path> 

	 // copy from HDFS to Linux
	 * hadoop -copyToLocal <hdfs file path> <local file path>   
	 * hadoop -get <hdfs file path> <local file path>   

	 * hadoop -moveFromLocal <local file path>  <hdfs file path> 
	 * hadoop -moveToLocal <hdfs file path> <local file path> 


  More Commands
  -------------

 	 * du - disk usage  
		  
			ex:  hadoop fs -du /user/cloudera


	 * du -s :   Like -du, but prints a summary of disk usage of all files/directories in the path

			ex: hadoop fs -du -s /user/cloudera

	 * df : Shows the capacity, free and used space of the filesystem
			 -h  Formats the sizes of files in a human-readable fashion.

			ex: hadoop fs -df -h /user/cloudera/
				hadoop fs -df /user/cloudera/

	 * getmerge <src> <localDest>    :  Retrieves all files that match the path src in HDFS,
					and copies them to a single, merged file in the local 										file system identified by localDest. 

			ex: hadoop fs -getmerge test/*.txt test1/merged.txt

	 * Get the replication factor of a file:

			ex: hadoop fs -stat %r test/empinfo.txt

	 * Set the replication factor of a file while copying or moving from linux

			ex: hadoop fs -D dfs.replication=5 -copyFromLocal <source> <destination>
	  
				hadoop fs -D dfs.replication=5 -copyFromLocal datasets/empinfo.txt test2
				hadoop fs -stat %r test2/empinfo.txt

	 * Set the replication factor for an existing file:  -setrep [-R] [-w] <rep-factor> <path>
		   
			ex: hadoop fs -setrep 2 test2/empinfo.txt   => sets the rep-factor of the file to 2
			hadoop fs -stat %r test2/empinfo.txt    => test the rep-factor of the file

	 * appendToFile

		Usage: hdfs dfs -appendToFile <localsrc> ... <dst>

		Append single src, or multiple srcs from local file system to the destination file system. 
		Also reads input from stdin and appends to destination file system.

		hdfs dfs -appendToFile localfile /user/hadoop/hadoopfile
		hdfs dfs -appendToFile localfile1 localfile2 /user/hadoop/hadoopfile
		hdfs dfs -appendToFile localfile hdfs://nn.example.com/hadoop/hadoopfile
		hdfs dfs -appendToFile - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin.


	 * chmod, chgrp, chown  => Change the permissions of files.

 ==========
    YARN
 ==========

   -> Resource Manager

   -> Hadoop 1.0,  MapReduce used to handle resource-management
       -> MR was the only execution process.

   -> Hadoop 2.0
	->  Resource Management:  YARN  (Yet another resource negotiator)
        ->  Execution: MapReduce

        -> Allowed hadoop to support other execution frameworks as-well

      
   YARN Processing Components
   --------------------------
       1. Resource Manager (daemon)  -> one per cluster, runs in a master node
       2. Node Manager (daemon)	     -> one per node, runs in all slave nodes
       3. Application Master         -> one per application, can run in any NN
       4. Set of Containers          -> Allocated to AM where tasks are executed

	

  ==========================================================
     MapReduce
  ==========================================================

   -> Distributed Processing Framework.

    
   Two facets of MapReduce
   ------------------------

    1. MapReduce execution flow (framework process)
	-> How an mR program is executed (what the system does)

	1. Map Phase (developer)  
        2. Combiner
	3. Shuffle & Sort Phase (framework)
        4. Partitioner
	5. Reduce Phase (developer)


    2. MapReduce Programming Structure
	-> How to write MR programs (what developers should do)

	-> An MR Application can have several programs (upto 5 different programs)
   		1. Driver Program 
		       -> master process where configuration details are specified.
		2. Mapper
			-> Logic defined by the developer
			-> has map() method that run once for every input record
		3. Reducer
			-> Logic defined by the developer
			-> has reduce() method that run once for every input record
		4. Combiner
			-> Like a mini-reducer logic applied to the output of each mapper
                        -> Will reduce the amount of data shuffled and improves performance
                        -> Operation has to be commutative.  
		5. Partitioner
			-> Allows the developer to control which keys  goes to which partition
                           based on his own custom implementation.
   
   
   MapReduce Framework Classes
   ---------------------------

   1. InputFormat -> It will identify 'records' from the blocks

   2. InputSplit -> Creates input-split
                    An input-split will have integral number of records.
                    Each input-split can span multiple-blocks.

   3. RecordReader -> Will read input-splits and create (K,V) pairs from the records
                     Call map() method in a mapper with each record.



    MapReduce DataTypes
    -------------------
    -> MR data-types are optimized for serialization & deserialization.
    -> All MR class uses MapReduce Custom Data Types
    
      Java                MapReduce
      ----                ---------
      String		  Text
      Integer		  IntWritable
      Long		  LongWritable
      Boolean		  BoolWritable
      Null		  NullWritable


    Wordcount Program
    -----------------
    Find the word frequency count of all the words in a given text file.
   

    Input-Split
    ------------
InputSplit                          RecordReader			     Mapper -> map(k, v, context) 			    Shuffle & Sort			      Reduce														Shuffle & Sort  			Reduce      

hadoop spark scala flatmap    	-> (1, "hadoop spark scala flatmap")   	    (hadoop, 1) (spark, 1) (scala, 1) (flatmap, 1)   ===>   (hadoop, [1,1,1,1])    		   => (hadoop, 4)                                  
map groupby spark spark		-> (27, "map groupby spark spark")          (map, 1) (groupby, 1) (spark, 1) (spark, 1)       	    (spark, [1,1,1,1,1,1,1,1,1,1,1,1])        (spark, 12)
hadoop spark scala flatmap 	-> (50, "hadoop scala flume oozie")	    (hadoop, 1) (spark, 1) (scala, 1) (flatmap, 1)          (flatmap, [1,1,1,1)                       (flatmap, 4)
map groupby spark spark		-> (74, "sqoop hive hive spark spark")      (map, 1) (groupby, 1) (spark, 1) (spark, 1)		    (scala, [1,1,1,1,1,1])		      (scala, 6)
hadoop spark scala flatmap	-> (102, "mapreduce hadoop hive hadoop")    (hadoop, 1) (spark, 1) (scala, 1) (flatmap, 1) 
map groupby spark spark		-> (135, "hdfs flatmap rdd scala")          (map, 1) (groupby, 1) (spark, 1) (spark, 1) 
hadoop spark scala flatmap 	-> (168, "spark scala transformations")     (hadoop, 1) (spark, 1) (scala, 1) (flatmap, 1)
map groupby spark spark		-> (197, "actions rdd rdd rdd rdd")         (map, 1) (groupby, 1) (spark, 1) (spark, 1)

      
     mapper   																															  
     -------
     input => key type: LongWritable     output =>   key type: Text 
	      value type: Text                       value type: LongWritable


     reducer 																															  
     -------
     input => key type: Text            output =>      key type:  Text
	      value type: LongWritable                 value type: LongWritable

   
   Submitting MR program to the cluster
   ------------------------------------

    $hadoop jar <jar-file-path> <driver-class-qualified-name> <commad-line-args>
    $yarn jar <jar-file-path> <driver-class-qualified-name> <commad-line-args>
   
    $hadoop jar /home/cloudera/mrpractice.jar com.sample.WordCountJob <input-file-path> <output-dir>
    
 
    MapReduce Life Cycle Methods
    ----------------------------


	Mapper Life Cycle methods
	--------------------------

	1. setup	-> Called once at the beginning of the MR process before the map() method
                           is called.
			-> Write initialization code.			
			  
	2. map		-> Executed once for every record in the input-split
			-> Write the logic that you want to apply on the record.

	3. cleanup	-> Called once after the map() method is completed for all records.
			-> Release your resource and other cleanup.

	4. run 		-> Generally not overridden
                        -> Can use this to change the execution flow.


	Reducer Life Cycle methods
	---------------------------

	1. setup	-> Called once at the beginning of the MR process before the reduce() method
                           is called.
			-> Write initialization code.			
			  
	2. reduce       -> Executed once for every record in the input-split
			-> Write the logic that you want to apply on the record.

	3. cleanup	-> Called once after the reduce() method is completed for all records.
			-> Release your resource and other cleanup.

	4. run 		-> Generally not overridden
                        -> Can use this to change the execution flow.
   
  
  Required external jars to add to MR project 
  -------------------------------------------
 
   1. Create a Java Project
	File -> New -> Java Project

   2. Right-click on the project 
	-> Build Path -> Configure Buid Path -> Libraries -> Add External JARS
	
	Add the following jar files
   
   	-> /usr/lib/hadoop/hadoop-common.jar
   	-> /usr/lib/hadoop-0.20-mapreduce/hadoop-core-mr1.jar


   3. Build the Java Project

	Right-click on the project 
	-> Export -> Java -> Jar File -> Give a name and Finish












	 