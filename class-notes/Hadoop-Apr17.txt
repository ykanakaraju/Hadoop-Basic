
 Curriculum   (20 sessions - 80 hours)
 -------------------------------------
  Introduction to Big Data & Hadoop
  HDFS
  YARN
  MapReduce (using Java)
  Hive 
  Impala - Basics
  Sqoop  
  HBase - Basic Operations
  Kafka (using Java)      
  Spark (PySpark)  


  Training Materials
  ------------------
    -> PDF Presentations
    -> Code modules, command documents, programs
    -> Class Notes
    -> Databricks Notebooks
    -> GitHub: https://github.com/ykanakaraju/Hadoop-Basic


  Pre-requisites
  --------------
     -> SQL
     -> Java 
     -> Python 


  Big Data Introduction
  ---------------------   
     
     Type of data so large and complex that your traditional data processing systems
     can not reasonably store and process. 

   Big Data scenarios
   ------------------
       -> The volume to be handled is very high (much higher than 1 TB)
       -> You want to support real-time analytics (velocity)
       -> You data is growing very fast, so that in no time your system becomes inadeqaute. 
       -> Process unstructured, semi-structured data etc.
    
  Big Data Characteristics
  ------------------------
       -> Volume
       -> Velocity
       -> Variety 
       -> Veracity (correctness of the data)
       -> Value (extracting insights from the data)   
    

  Why your traditional systems have big data problem
  --------------------------------------------------
   -> Because your traditional systems ( that exists in pre-internet era ) are not designed
      for internet scale applications.
 
   -> They are designed to run on a single machine. The single machine has only limited 
      amount of resources.


  Scalability Options
  -------------------
    -> Scale up  (Vertical Scaling)   -> Migrate to a machine with higher configuration
    -> Scala out (Horizontal Scaling) -> Use many machines instead of one. 
                                      -> Use computing cluster

   What is a Computing Cluster?
   ----------------------------
   A unified network of nodes (machines) whose cumulative resources can be used to store
   or process data as if you are working with a single machine. 


   What is a Framework ?
   ---------------------
    A framework is a set of libraries that implement all the basic low-level functionalities
    that the framework is intended for. 

    The users of the framework should focus on the business-logic and implement their solutions 
    as per the guidelines provided by the framework. 


    What is the solution to storage & processing problems associated with Big Data
    ------------------------------------------------------------------------------
    -> Hadoop provides a solution for big-data problems.

 
    What is Hadoop ?
    ----------------     
    Hadoop is a Open Source Framework to Store and Process data in a distributed way 
    using a cluster of commdity harware. 

     -> Components of Hadoop:
	   -> Distributed Storage Solution (HDFS)
	   -> Distributed Processing Solution (MapReduce)
           -> Resource Management (YARN)


   Hadoop Distributions
   --------------------
	Apache Hadoop => Open source
	
	On-Prem & Hybrid Cloud => Cloudera & MapR

	Managed Cloud Solutions 
		-> AWS   : AWS Elastic MapReduce (EMR)
		-> Azure : HDInsignts
	        -> GCP   : DataProc 


    HDFS (Hadoop Distributed File System)
    --------------------------------------

      -> Stores data in the form of "blocks" of 128 MB each
      -> Each block is replicated on other nodes so many times as defined by a 'replication-factor'
           (default rep-factor is 3)      
      
     Q1:  If I want to stote 1024 MB (1 GB) file on HDFS, how much physical storage space 
          is required (using default configs) => 3 GB   

     Q2:  If you want to store 1034 MB file, how many unique blocks are created on HDFS?
	  -> 8 blocks of 128MB each & 1 block of 10 MB
         
	

   Hadoop EcoSystem
   ----------------   
     -> Refers to a set of tools/frameworks/software that run on top of hadoop to solve
        various big data related problems.          

     -> Popular Components in Hadoop EcoSystem             

          -> Resource Management Tool:     YARN
          -> Coordination Management Tool: ZooKeeper
          -> Execution Frameworks: MapReduce, Spark, Tez
          -> SQL-Analytics: Impala, Drill, Hive
	  -> Streaming Data Processing: Kafka, Storm, Spark Streaming
	  -> Machine Learning: Mahout, Spark MLLib
          -> Scripting: Pig
          -> NoSQL database: HBase
          -> Data ingestion tools: Flume, Sqoop

  =======================================================================
     HDFS (Hadoop Distributed File System)
  =======================================================================

   -> Storage Solution
   -> Stores data as blocks of 128 MB 
   -> Each block is replicated.

   HDFS Architecture
   -----------------

   Hadoop uses Master-Slave Architecture


   HDFS Daemons 
   ------------
     -> Daemon: Continuosly running background job. 

     1. Name Node (NN)	
	-> runs on the master machine (master daemon)
        -> recieves heart-beats from DNs.
	   -> if three successive heart-beats are missing from any DN, it is considered as 'dead'
        -> NN maintains two "meta-data" files
	    
           1. FSImage
		-> Persistent disk file
                -> Will contain all the meta-data info from the time that the NN service is started
                   upto the last checkpoint.

	   2. EditLog
		-> Maintained in memory (RAM)
		-> All incrmental changes are recorded in this file.        

     2. Data Node (DN)
	-> runs on every slave machine (salve daemon)
  	-> every DB sends heart-beats to NN once in every 3 seconds
 
        -> In every heart-beat, DN sends "block-report" of that node
           -> A 'block-report' consists of such info as storage data, which files, which blocks, 
              rep-factors, permissions, ...        
    
     3. Secondary Name Node (SNN)
          -> responsible for check-pointing process
              -> check-pointing is a process of merging the edit-log file with fsimage file.

     4. Balancer	


































	 