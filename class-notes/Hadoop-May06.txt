
 Curriculum   (20 sessions - 80 hours)
 -------------------------------------
  Introduction to Big Data & Hadoop
  HDFS
  YARN
  MapReduce (using Java)
  Hive 
  Impala - Basics
  Sqoop  
  HBase - Basic Operations
  Kafka (using Java)      
  Spark (PySpark)  


  Training Materials
  ------------------
    -> PDF Presentations
    -> Code modules, command documents, programs
    -> Class Notes
    -> Databricks Notebooks
    -> GitHub: https://github.com/ykanakaraju/Hadoop-Basic


  Pre-requisites
  --------------
     -> SQL
     -> Java 
     -> Python 


  Big Data Introduction
  ---------------------   
     
     Type of data so large and complex that your traditional data processing systems
     can not reasonably store and process. 

   Big Data scenarios
   ------------------
       -> The volume to be handled is very high (much higher than 1 TB)
       -> You want to support real-time analytics (velocity)
       -> You data is growing very fast, so that in no time your system becomes inadeqaute. 
       -> Process unstructured, semi-structured data etc.
    
  Big Data Characteristics
  ------------------------
       -> Volume
       -> Velocity
       -> Variety 
       -> Veracity (correctness of the data)
       -> Value (extracting insights from the data)   
    

  Why your traditional systems have big data problem
  --------------------------------------------------
   -> Because your traditional systems ( that exists in pre-internet era ) are not designed
      for internet scale applications.
 
   -> They are designed to run on a single machine. The single machine has only limited 
      amount of resources.


  Scalability Options
  -------------------
    -> Scale up  (Vertical Scaling)   -> Migrate to a machine with higher configuration
    -> Scala out (Horizontal Scaling) -> Use many machines instead of one. 
                                      -> Use computing cluster

   What is a Computing Cluster?
   ----------------------------
   A unified network of nodes (machines) whose cumulative resources can be used to store
   or process data as if you are working with a single machine. 


   What is a Framework ?
   ---------------------
    A framework is a set of libraries that implement all the basic low-level functionalities
    that the framework is intended for. 

    The users of the framework should focus on the business-logic and implement their solutions 
    as per the guidelines provided by the framework. 


    What is the solution to storage & processing problems associated with Big Data
    ------------------------------------------------------------------------------
    -> Hadoop provides a solution for big-data problems.

 
    What is Hadoop ?
    ----------------     
    Hadoop is a Open Source Framework to Store and Process data in a distributed way 
    using a cluster of commdity harware. 

     -> Components of Hadoop:
	   -> Distributed Storage Solution (HDFS)
	   -> Distributed Processing Solution (MapReduce)
           -> Resource Management (YARN)


   Hadoop Distributions
   --------------------
	Apache Hadoop => Open source
	
	On-Prem & Hybrid Cloud => Cloudera & MapR

	Managed Cloud Solutions 
		-> AWS   : AWS Elastic MapReduce (EMR)
		-> Azure : HDInsignts
	        -> GCP   : DataProc 


    HDFS (Hadoop Distributed File System)
    --------------------------------------

      -> Stores data in the form of "blocks" of 128 MB each
      -> Each block is replicated on other nodes so many times as defined by a 'replication-factor'
           (default rep-factor is 3)      
      
     Q1:  If I want to stote 1024 MB (1 GB) file on HDFS, how much physical storage space 
          is required (using default configs) => 3 GB   

     Q2:  If you want to store 1034 MB file, how many unique blocks are created on HDFS?
	  -> 8 blocks of 128MB each & 1 block of 10 MB
         
	

   Hadoop EcoSystem
   ----------------   
     -> Refers to a set of tools/frameworks/software that run on top of hadoop to solve
        various big data related problems.          

     -> Popular Components in Hadoop EcoSystem             

          -> Resource Management Tool:     YARN
          -> Coordination Management Tool: ZooKeeper
          -> Execution Frameworks: MapReduce, Spark, Tez
          -> SQL-Analytics: Impala, Drill, Hive
	  -> Streaming Data Processing: Kafka, Storm, Spark Streaming
	  -> Machine Learning: Mahout, Spark MLLib
          -> Scripting: Pig
          -> NoSQL database: HBase
          -> Data ingestion tools: Flume, Sqoop

  =======================================================================
     HDFS (Hadoop Distributed File System)
  =======================================================================

   -> Storage Solution
   -> Stores data as blocks of 128 MB 
   -> Each block is replicated.

   HDFS Architecture
   -----------------

   Hadoop uses Master-Slave Architecture


   HDFS Daemons 
   ------------
     -> Daemon: Continuosly running background job. 

     1. Name Node (NN)	
	-> runs on the master machine (master daemon)
        -> recieves heart-beats from DNs.
	   -> if three successive heart-beats are missing from any DN, it is considered as 'dead'
        -> NN maintains two "meta-data" files
	    
           1. FSImage
		-> Persistent disk file
                -> Will contain all the meta-data info from the time that the NN service is started
                   upto the last checkpoint.

	   2. EditLog
		-> Maintained in memory (RAM)
		-> All incrmental changes are recorded in this file.        

     2. Data Node (DN)
	-> runs on every slave machine (salve daemon)
  	-> every DB sends heart-beats to NN once in every 3 seconds
 
        -> In every heart-beat, DN sends "block-report" of that node
           -> A 'block-report' consists of such info as storage data, which files, which blocks, 
              rep-factors, permissions, ...        
    
     3. Secondary Name Node (SNN)
          -> responsible for check-pointing process
              -> check-pointing is a process of merging the edit-log file with fsimage file.

     4. Balancer 
	  -> Ensures that the replication-factor of the files is maintained. 

          -> A file can become "under-replicated" if one nodes that contain the blocks of the file
             goes down, or if the rep-factor of the file is increased. Then balencer creates
             additional replicas of the blocks of that file.  

	  -> A file can become "over-replicated" if one node that contain the blocks that was 
             previously down and comes up again, or if the rep-factor of the file is decreased.
             Then balencer removes the replicas of the blocks of that file beyond the rep-factor.   


   HDFS High Availability
   ----------------------
     
      => Availability : Means that system do not have any down-time. Always available to the client.
    
    
      => Hadoop High Availability is introduced in Hadoop 2.0 onwards. 
	   -> We have "Active NN" and "Stand-by Name Node"

      => Hadoop 1.x is not high availble system.
	   -> In Hadoop 1.x, Name Node is a 'Single Point of Failure'    


   HDFS File Operations
   --------------------

       => HDFS is non-editable file system.

       Default Paths:

        * Cloudera Local Home :  /home/cloudera
        * HDFS Home: 		 /user/cloudera
        
        HDFS FS Command Syntax:  $hadoop fs -<command-name> <parameters>
                                 $hdfs dfs -<command-name> <parameters>



 	Default Paths ( cloudera is the user-name ):

        * Cloudera Local Home :  /home/cloudera
        * HDFS Home: /user/cloudera

 	Configuration Files:
	
	* /etc/hadoop/config/core-site.xml
	* /etc/hadoop/config/hdfs-site.xml
	* /etc/hadoop/config/mapred-site.xml

 	There are individual folders for each node in the cluster in directories as shown below :
  
        * /etc/hadoop/conf.cloudera.hdfs1
	* /etc/hadoop/conf.cloudera.mapred1
	* /etc/hadoop/conf.cloudera.yarn1

 	All daemon log files are written under:

	* /var/log/hadoop/

	
   Common HDFS File Operations
   ---------------------------

	 * hadoop fs -help   				=> lists all the commands with a brief discription
	 * hadoop fs -help <command-name>  
	 * hadoop fs -chmod 755 <file/dir>
	 * hadoop fs -chmod -R 755 <file/dir>		=> change permission for a file/directory
	 * hadoop fs -cat <file-name>
	 * hadoop fs -ls
	 * hadoop fs -ls <dir-path>
	 * hadoop fs -ls -R  <dir-path>			=> Recursive listing of files and directories

	 * hadoop fs -cp <source file> <target dir>     => Copies within HDFS frpm source to target
	 * hadoop fs -mv <source file> <target dir>     => Moves within HDFS frpm source to target
	 * hadoop fs -rm -r <file-name/directory-name>
		 
	 // copy from Linux to HDFS
	 * hadoop -copyFromLocal <local file path>  <hdfs file path> 
	 * hadoop -put <local file path>  <hdfs file path> 

	 // copy from HDFS to Linux
	 * hadoop -copyToLocal <hdfs file path> <local file path>   
	 * hadoop -get <hdfs file path> <local file path>   

	 * hadoop -moveFromLocal <local file path>  <hdfs file path> 
	 * hadoop -moveToLocal <hdfs file path> <local file path> 


  More Commands
  -------------

 	 * du - disk usage  
		  
			ex:  hadoop fs -du /user/cloudera


	 * du -s :   Like -du, but prints a summary of disk usage of all files/directories in the path

			ex: hadoop fs -du -s /user/cloudera

	 * df : Shows the capacity, free and used space of the filesystem
			 -h  Formats the sizes of files in a human-readable fashion.

			ex: hadoop fs -df -h /user/cloudera/
				hadoop fs -df /user/cloudera/

	 * getmerge <src> <localDest>    :  Retrieves all files that match the path src in HDFS,
					and copies them to a single, merged file in the local 										file system identified by localDest. 

			ex: hadoop fs -getmerge test/*.txt test1/merged.txt

	 * Get the replication factor of a file:

			ex: hadoop fs -stat %r test/empinfo.txt

	 * Set the replication factor of a file while copying or moving from linux

			ex: hadoop fs -D dfs.replication=5 -copyFromLocal <source> <destination>
	  
				hadoop fs -D dfs.replication=5 -copyFromLocal datasets/empinfo.txt test2
				hadoop fs -stat %r test2/empinfo.txt

	 * Set the replication factor for an existing file:  -setrep [-R] [-w] <rep-factor> <path>
		   
			ex: hadoop fs -setrep 2 test2/empinfo.txt   => sets the rep-factor of the file to 2
			hadoop fs -stat %r test2/empinfo.txt    => test the rep-factor of the file

	 * appendToFile

		Usage: hdfs dfs -appendToFile <localsrc> ... <dst>

		Append single src, or multiple srcs from local file system to the destination file system. 
		Also reads input from stdin and appends to destination file system.

		hdfs dfs -appendToFile localfile /user/hadoop/hadoopfile
		hdfs dfs -appendToFile localfile1 localfile2 /user/hadoop/hadoopfile
		hdfs dfs -appendToFile localfile hdfs://nn.example.com/hadoop/hadoopfile
		hdfs dfs -appendToFile - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin.


	 * chmod, chgrp, chown  => Change the permissions of files.

 ==========
    YARN
 ==========

   -> Resource Manager

   -> Hadoop 1.0,  MapReduce used to handle resource-management
       -> MR was the only execution process.

   -> Hadoop 2.0
	->  Resource Management:  YARN  (Yet another resource negotiator)
        ->  Execution: MapReduce

        -> Allowed hadoop to support other execution frameworks as-well

      
   YARN Processing Components
   --------------------------
       1. Resource Manager (daemon)  -> one per cluster, runs in a master node
       2. Node Manager (daemon)	     -> one per node, runs in all slave nodes
       3. Application Master         -> one per application, can run in any NN
       4. Set of Containers          -> Allocated to AM where tasks are executed

	

  ==========================================================
     MapReduce
  ==========================================================

   -> Distributed Processing Framework.

    
   Two facets of MapReduce
   ------------------------

    1. MapReduce execution flow (framework process)
	-> How an mR program is executed (what the system does)

	1. Map Phase (developer)  
        2. Combiner
	3. Shuffle & Sort Phase (framework)
        4. Partitioner
	5. Reduce Phase (developer)


    2. MapReduce Programming Structure
	-> How to write MR programs (what developers should do)

	-> An MR Application can have several programs (upto 5 different programs)
   		1. Driver Program 
		       -> master process where configuration details are specified.
		2. Mapper
			-> Logic defined by the developer
			-> has map() method that run once for every input record
		3. Reducer
			-> Logic defined by the developer
			-> has reduce() method that run once for every input record
		4. Combiner
			-> Like a mini-reducer logic applied to the output of each mapper
                        -> Will reduce the amount of data shuffled and improves performance
                        -> Operation has to be commutative.  
		5. Partitioner
			-> Allows the developer to control which keys  goes to which partition
                           based on his own custom implementation.
   
   
   MapReduce Framework Classes
   ---------------------------

   1. InputFormat -> It will identify 'records' from the blocks

   2. InputSplit -> Creates input-split
                    An input-split will have integral number of records.
                    Each input-split can span multiple-blocks.

   3. RecordReader -> Will read input-splits and create (K,V) pairs from the records
                     Call map() method in a mapper with each record.



    MapReduce DataTypes
    -------------------
    -> MR data-types are optimized for serialization & deserialization.
    -> All MR class uses MapReduce Custom Data Types
    
      Java                MapReduce
      ----                ---------
      String		  Text
      Integer		  IntWritable
      Long		  LongWritable
      Boolean		  BoolWritable
      Null		  NullWritable


    Wordcount Program
    -----------------
    Find the word frequency count of all the words in a given text file.
   

    Input-Split
    ------------
InputSplit                          RecordReader			     Mapper -> map(k, v, context) 			    Shuffle & Sort			      Reduce														Shuffle & Sort  			Reduce      

hadoop spark scala flatmap    	-> (1, "hadoop spark scala flatmap")   	    (hadoop, 1) (spark, 1) (scala, 1) (flatmap, 1)   ===>   (hadoop, [1,1,1,1])    		   => (hadoop, 4)                                  
map groupby spark spark		-> (27, "map groupby spark spark")          (map, 1) (groupby, 1) (spark, 1) (spark, 1)       	    (spark, [1,1,1,1,1,1,1,1,1,1,1,1])        (spark, 12)
hadoop spark scala flatmap 	-> (50, "hadoop scala flume oozie")	    (hadoop, 1) (spark, 1) (scala, 1) (flatmap, 1)          (flatmap, [1,1,1,1)                       (flatmap, 4)
map groupby spark spark		-> (74, "sqoop hive hive spark spark")      (map, 1) (groupby, 1) (spark, 1) (spark, 1)		    (scala, [1,1,1,1,1,1])		      (scala, 6)
hadoop spark scala flatmap	-> (102, "mapreduce hadoop hive hadoop")    (hadoop, 1) (spark, 1) (scala, 1) (flatmap, 1) 
map groupby spark spark		-> (135, "hdfs flatmap rdd scala")          (map, 1) (groupby, 1) (spark, 1) (spark, 1) 
hadoop spark scala flatmap 	-> (168, "spark scala transformations")     (hadoop, 1) (spark, 1) (scala, 1) (flatmap, 1)
map groupby spark spark		-> (197, "actions rdd rdd rdd rdd")         (map, 1) (groupby, 1) (spark, 1) (spark, 1)

      
     mapper   																															  
     -------
     input => key type: LongWritable     output =>   key type: Text 
	      value type: Text                       value type: LongWritable


     reducer 																															  
     -------
     input => key type: Text            output =>      key type:  Text
	      value type: LongWritable                 value type: LongWritable

   
   Submitting MR program to the cluster
   ------------------------------------

    $hadoop jar <jar-file-path> <driver-class-qualified-name> <commad-line-args>
    $yarn jar <jar-file-path> <driver-class-qualified-name> <commad-line-args>
   
    $hadoop jar /home/cloudera/mrpractice.jar com.sample.WordCountJob <input-file-path> <output-dir>
    
 
    MapReduce Life Cycle Methods
    ----------------------------


	Mapper Life Cycle methods
	--------------------------

	1. setup	-> Called once at the beginning of the MR process before the map() method
                           is called.
			-> Write initialization code.			
			  
	2. map		-> Executed once for every record in the input-split
			-> Write the logic that you want to apply on the record.

	3. cleanup	-> Called once after the map() method is completed for all records.
			-> Release your resource and other cleanup.

	4. run 		-> Generally not overridden
                        -> Can use this to change the execution flow.


	Reducer Life Cycle methods
	---------------------------

	1. setup	-> Called once at the beginning of the MR process before the reduce() method
                           is called.
			-> Write initialization code.			
			  
	2. reduce       -> Executed once for every record in the input-split
			-> Write the logic that you want to apply on the record.

	3. cleanup	-> Called once after the reduce() method is completed for all records.
			-> Release your resource and other cleanup.

	4. run 		-> Generally not overridden
                        -> Can use this to change the execution flow.
   
  
  Required external jars to add to MR project 
  -------------------------------------------
 
   1. Create a Java Project
	File -> New -> Java Project

   2. Right-click on the project 
	-> Build Path -> Configure Buid Path -> Libraries -> Add External JARS
	
	Add the following jar files   
   	-> /usr/lib/hadoop/hadoop-common.jar
   	-> /usr/lib/hadoop-0.20-mapreduce/hadoop-core-mr1.jar

   3. Build the Java Project

	Right-click on the project 
	-> Export -> Java -> Jar File -> Give a name and Finish


 ====================
    Hive
 ====================

    Hive is Hadoop's Data Warehousing Framework.
    
    Hive is an SQL wrapper on top of map-reduce. So that user can directly work with SQL without
    having to write MR programs. HIve Server can create the MR programs and run them on the cluster
    releaving the analysts from writing MR programs.

    => Hive is NOT an RDBMS. 
    => Hive is NOT a NoSQL database

    Hive provides use a dialect of SQL called "Hive Query Language" (HQL, HiveQL)


    Hive Components
    ----------------
  
    1. Hive Warehouse - An HDFS directory where hive stores all its data files of all its managed tables.
                        In cloudera:  /user/hive/warehouse

    2. Hive MetaStore - MetaStore is typically an RDBMS where hive stores its metadata
                        (Most popular option in production systems is MySQL)
                        -> Hive comes with an embedded metastore database called "derby"
        -> WareHouse  : DATA
	-> Meta Store : METADATA

    3. Hive Server   - Responsible for executing HQL statements (and creating MR programs if required)



    How to Access Hive ?
    --------------------

    1. Command Line Interface (CLI Shells)
	-> hive shell  
		-> Local hive installations with default user.
		-> Less secure

        -> beeline shell (preferred for security reasons)
		-> Connects to Hive using JDBC connection with a specific user credential
                -> More secure.

    2. HUE (Hadoop User Environment)
         -> Provides a Web UI to interact with various Hadoop tools.

    3. Client Applications  ( JDBC, ODBC, Thrift )


   Hive File Structure 
   -------------------
   
    Hive Warehouse:  /user/hive/warehouse

    ->  Hive created separate directories for each database in the Hive warehouse
        (exception: no separate directory for 'default' database)

    -> For every table in the database, a separate directory is created.
       All the data files of that table are saved in that directory.

       ex: db1.emp   ==> /user/hive/warehouse/db1.db/emp
	   db1.emp2  ==> /user/hive/warehouse/db1.db/emp2



   To display current database
   ---------------------------
 	set hive.cli.print.current.db=true;   -> To show db name


   Defaults
   --------
        -> default serde: DELIMITED
        -> default storage format : TEXTFILE
        -> default line terminator: \n
        -> default field terminator: Ctrl-A  ( \0001 )           

   Describe
   ---------
        -> desc empinfo
	-> desc extended empinfo
	-> desc formatted empinfo    


--------------------------------------------------------------------
		  CREATE TABLE  & CTAS Construct
--------------------------------------------------------------------
--- Managed table ---
CREATE TABLE empinfo (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n' 
STORED AS TEXTFILE;

--- External table ---
CREATE EXTERNAL TABLE empexternal(empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION '/user/cloudera/emp_external';

--------------------------------------------------------------------
		DATA LOADING 
--------------------------------------------------------------------

--append data
LOAD DATA LOCAL INPATH '<local linux file path>' INTO TABLE <table name>
LOAD DATA INPATH '<hdfs file path>' INTO TABLE <table name>

-- overwrite data
LOAD DATA LOCAL INPATH '<local linux file path>' OVERWRITE INTO TABLE <table name>
LOAD DATA INPATH '<hdfs file path>' OVERWRITE INTO TABLE <table name>

--loading data into external table --
LOAD DATA LOCAL INPATH '<local linux file path>' INTO TABLE <table name>



    Hive follows Scehma on Read
    ---------------------------
    -> Hive do not control what data goes into the table. 
    -> Schema validation does not happen while "loading" data into the table.
    -> Scahema validation happens while reading the data.
       -> If the data can not be parsed as per schema, Hive shows NULL for that field.


    Managed Tables
    --------------
        -> Managed tables are managed by Hive and are stored in Hive warehouse directory.
        -> When you drop a managed table, Hive drops the schema as well as the data files. 

    
    External Tables
    ---------------
        -> External tables are not managed by Hive.
        -> The data files of external table are not stored in Hive warehouse. 
        -> They are stored in an external location specified while creating the table.
        -> The data loaded into external table either from hive or from any other application/tool
           is not owned/managed by Hive.
        -> When we drop the table, ONLY schema is dropped. The data files are NOT DROPPED.
  

    Use Cases for External tables
    -----------------------------

       -> External tables are used when multiple hive databases are using the same data. 
       -> You can create an external table pointing to existing data, analyze that data and drop the
          table without affecting the actual data.




    Partitioned Tables
    ------------------

     In Hive, Partitions provide a way of dividing a table into coarse-grained 
     parts based on the value of a partition column, such as a date or country.


     Partitioning allows us to create tables partitioned by one or more columns.
     
     Each unique value of the partitioned column will have its own directory 
     within the table's directory.

     Based on the way we load data into the tables we can have static & dynamic partitions

     -> Partition columns should reasonably low-cardinality columns (such as year, country, month, ...)


     Static Partitioning
     -------------------
	-> The data files are loaded by the user using "LOAD DATA .." command
        -> The user has to know precisly to which partition he is loading the file.


     Dynamic Partitioning
     --------------------
	-> We dynamic partitioning when the user do not know to which partition the data goes to.
           This may be because the partition info itself is there in the data or may be we have
           data of many partitions in the same file. 

        -> We can not use "LOAD DATA.." command to load the data into the partitioned table

        -> We have to load the data using dynamic partitioning scheme from a base table using
           "INSERT .. SELECT .." command. 

        -> Data files are created dynamically by hive (hense the name dynamic partitioning)

        -> set hive.exec.dynamic.partition.mode=nonstrict



CREATE TABLE students_part (id INT, name STRING, phone STRING, email STRING, project STRING)
PARTITIONED BY (country STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;


LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/students_IN.txt' INTO TABLE students_part

PARTITION (country='India');


CREATE TABLE students_dynamic_part (id INT, name STRING, phone STRING, email STRING, project STRING)
PARTITIONED BY (country STRING, batch INT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;


// Base Table
CREATE TABLE students (id INT, name STRING, phone STRING, email STRING, project STRING, country STRING, batch INT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;


LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/students_country_batch.csv' INTO TABLE students;


// Loading Data using Dynamic Partitioning

INSERT INTO students_dynamic_part
PARTITION (country, batch)
SELECT id, name, phone, email, project, country, batch FROM students;

INSERT INTO students_dynamic_part
PARTITION (country='India', batch)
SELECT id, name, phone, email, project, batch FROM students
WHERE country = 'India';

INSERT INTO students_dynamic_part
PARTITION (country='Japan', batch=100)
SELECT id, name, phone, email, project FROM students
WHERE country = 'USA' and batch=1;

-- external partitioned table

CREATE EXTERNAL TABLE students_dyn_part_ext
(id int, name string, mobile string, email string, project string)
PARTITIONED BY (country string, batch int)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '|'
LINES TERMINATED BY '\n' 
STORED AS TEXTFILE
LOCATION '/user/cloudera/hive/students_dyn_part_ext';


INSERT OVERWRITE TABLE students_dyn_part_ext
PARTITION (country, batch) 
SELECT id, name, mobile, email, project, country, batch 
FROM students_country_batch;

----------------

LOAD DATA LOCAL INPATH '/home/cloudera/hive-material/data/emp.txt' INTO TABLE empinfo;


-- overwrite
INSERT OVERWRITE TABLE students_country_usa
SELECT * FROM students_country_batch WHERE country='USA'

-- append
INSERT INTO TABLE students_country_usa
SELECT * FROM students_country_batch WHERE country='USA'

--hdfs diretory
INSERT OVERWRITE DIRECTORY '/user/cloudera/hive_data/students_country_batch'
SELECT * FROM students_country_batch WHERE country='USA';

INSERT OVERWRITE DIRECTORY '/user/cloudera/hive_data/students_country_batch'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
SELECT * FROM students_country_batch WHERE country='USA';

INSERT INTO DIRECTORY '/user/cloudera/hive_data/students_country_batch'
SELECT * FROM students_country_batch WHERE country='USA';


--local directory
INSERT OVERWRITE LOCAL DIRECTORY '/home/cloudera/hive_data/students_country_batch'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|'
SELECT * FROM students_country_batch WHERE country='USA';

INSERT OVERWRITE LOCAL DIRECTORY '/home/cloudera/hive_data/students_country_batch_avro'
STORED AS AVRO 
SELECT * FROM students_country_batch WHERE country='USA';

-- creating tables using custom formats
CREATE TABLE empinfo_orc (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
STORED AS ORC;

CREATE TABLE empinfo_parquet (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
STORED AS PARQUET;

CREATE TABLE empinfo_avro (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
STORED AS AVRO;


INSERT OVERWRITE TABLE empinfo_orc
SELECT * FROM empinfo;


-- multi-table insert command

FROM students_country_batch
  INSERT OVERWRITE TABLE students_usa 
    SELECT id, name, mobile, email, project, country, batch WHERE country = 'USA'
  INSERT OVERWRITE TABLE students_uk 
    SELECT id, name, mobile, email, project, country, batch WHERE country = 'UK'
  INSERT OVERWRITE TABLE students_india 
    SELECT id, name, mobile, email, project, country, batch WHERE country = 'India';


-- working with complex data types..
CREATE TABLE complex_data
(col1 array<string>,
 col2 map<int,string>,
 col3 struct<name:string,score:int>)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
COLLECTION ITEMS TERMINATED BY '/'
MAP KEYS TERMINATED BY ':'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '/home/cloudera/hive-material/hive_complex_data' INTO TABLE complex_data;


























	 