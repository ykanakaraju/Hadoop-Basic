
 Curriculum   (40 sessions - 80 hours)
 -------------------------------------
  Introduction to Big Data & Hadoop
  HDFS
  YARN
  MapReduce (using Java)
  Hive 
  Impala - Basics
  Sqoop  
  HBase - Basic Operations
  Kafka (using Java)      
  Spark (PySpark)  


  Training Materials
  ------------------
    -> PDF Presentations
    -> Code modules, command documents, programs
    -> Class Notes
    -> Databricks Notebooks
    -> GitHub:  https://github.com/ykanakaraju/Hadoop-Basic  (class notes)
		https://github.com/ykanakaraju/kafkajava
		https://github.com/ykanakaraju/pyspark

  Pre-requisites
  --------------
     -> SQL
     -> Java 
     -> Python 


  Big Data Introduction
  ---------------------   
     
     Type of data so large and complex that your traditional data processing systems
     can not reasonably store and process. 

   Big Data scenarios
   ------------------
       -> The volume to be handled is very high (much higher than 1 TB)
       -> You want to support real-time analytics (velocity)
       -> You data is growing very fast, so that in no time your system becomes inadeqaute. 
       -> Process unstructured, semi-structured data etc.
    
  Big Data Characteristics
  ------------------------
       -> Volume
       -> Velocity
       -> Variety 
       -> Veracity (correctness of the data)
       -> Value (extracting insights from the data)   
    

  Why your traditional systems have big data problem
  --------------------------------------------------
   -> Because your traditional systems ( that exists in pre-internet era ) are not designed
      for internet scale applications.
 
   -> They are designed to run on a single machine. The single machine has only limited 
      amount of resources.


  Scalability Options
  -------------------
    -> Scale up  (Vertical Scaling)   -> Migrate to a machine with higher configuration
    -> Scala out (Horizontal Scaling) -> Use many machines instead of one. 
                                      -> Use computing cluster

   What is a Computing Cluster?
   ----------------------------
   A unified network of nodes (machines) whose cumulative resources can be used to store
   or process data as if you are working with a single machine. 


   What is a Framework ?
   ---------------------
    A framework is a set of libraries that implement all the basic low-level functionalities
    that the framework is intended for. 

    The users of the framework should focus on the business-logic and implement their solutions 
    as per the guidelines provided by the framework. 


    What is the solution to storage & processing problems associated with Big Data
    ------------------------------------------------------------------------------
    -> Hadoop provides a solution for big-data problems.

 
    What is Hadoop ?
    ----------------     
    Hadoop is a Open Source Framework to Store and Process data in a distributed way 
    using a cluster of commdity harware. 

     -> Components of Hadoop:
	   -> Distributed Storage Solution (HDFS)
	   -> Distributed Processing Solution (MapReduce)
           -> Resource Management (YARN)


   Hadoop Distributions
   --------------------
	Apache Hadoop => Open source
	
	On-Prem & Hybrid Cloud => Cloudera & MapR

	Managed Cloud Solutions 
		-> AWS   : AWS Elastic MapReduce (EMR)
		-> Azure : HDInsignts
	        -> GCP   : DataProc 


    HDFS (Hadoop Distributed File System)
    --------------------------------------

      -> Stores data in the form of "blocks" of 128 MB each
      -> Each block is replicated on other nodes so many times as defined by a 'replication-factor'
           (default rep-factor is 3)      
      
     Q1:  If I want to stote 1024 MB (1 GB) file on HDFS, how much physical storage space 
          is required (using default configs) => 3 GB   

     Q2:  If you want to store 1034 MB file, how many unique blocks are created on HDFS?
	  -> 8 blocks of 128MB each & 1 block of 10 MB
         
	

   Hadoop EcoSystem
   ----------------   
     -> Refers to a set of tools/frameworks/software that run on top of hadoop to solve
        various big data related problems.          

     -> Popular Components in Hadoop EcoSystem             

          -> Resource Management Tool:     YARN
          -> Coordination Management Tool: ZooKeeper
          -> Execution Frameworks: MapReduce, Spark, Tez
          -> SQL-Analytics: Impala, Drill, Hive
	  -> Streaming Data Processing: Kafka, Storm, Spark Streaming
	  -> Machine Learning: Mahout, Spark MLLib
          -> Scripting: Pig
          -> NoSQL database: HBase
          -> Data ingestion tools: Flume, Sqoop

  =======================================================================
     HDFS (Hadoop Distributed File System)
  =======================================================================

   -> Storage Solution
   -> Stores data as blocks of 128 MB 
   -> Each block is replicated.

   HDFS Architecture
   -----------------

   Hadoop uses Master-Slave Architecture


   HDFS Daemons 
   ------------
     -> Daemon: Continuosly running background job. 

     1. Name Node (NN)	
	-> runs on the master machine (master daemon)
        -> recieves heart-beats from DNs.
	   -> if three successive heart-beats are missing from any DN, it is considered as 'dead'
        -> NN maintains two "meta-data" files
	    
           1. FSImage
		-> Persistent disk file
                -> Will contain all the meta-data info from the time that the NN service is started
                   upto the last checkpoint.

	   2. EditLog
		-> Maintained in memory (RAM)
		-> All incrmental changes are recorded in this file.        

     2. Data Node (DN)
	-> runs on every slave machine (salve daemon)
  	-> every DB sends heart-beats to NN once in every 3 seconds
 
        -> In every heart-beat, DN sends "block-report" of that node
           -> A 'block-report' consists of such info as storage data, which files, which blocks, 
              rep-factors, permissions, ...        
    
     3. Secondary Name Node (SNN)
          -> responsible for check-pointing process
              -> check-pointing is a process of merging the edit-log file with fsimage file.

     4. Balancer 
	  -> Ensures that the replication-factor of the files is maintained. 

          -> A file can become "under-replicated" if one nodes that contain the blocks of the file
             goes down, or if the rep-factor of the file is increased. Then balencer creates
             additional replicas of the blocks of that file.  

	  -> A file can become "over-replicated" if one node that contain the blocks that was 
             previously down and comes up again, or if the rep-factor of the file is decreased.
             Then balencer removes the replicas of the blocks of that file beyond the rep-factor.   


   HDFS High Availability
   ----------------------
     
      => Availability : Means that system do not have any down-time. Always available to the client.
    
    
      => Hadoop High Availability is introduced in Hadoop 2.0 onwards. 
	   -> We have "Active NN" and "Stand-by Name Node"

      => Hadoop 1.x is not high availble system.
	   -> In Hadoop 1.x, Name Node is a 'Single Point of Failure'    


   HDFS File Operations
   --------------------

       => HDFS is non-editable file system.

       Default Paths:

        * Cloudera Local Home :  /home/cloudera
        * HDFS Home: 		 /user/cloudera
        
        HDFS FS Command Syntax:  $hadoop fs -<command-name> <parameters>
                                 $hdfs dfs -<command-name> <parameters>



 	Default Paths ( cloudera is the user-name ):

        * Cloudera Local Home :  /home/cloudera
        * HDFS Home: /user/cloudera

 	Configuration Files:
	
	* /etc/hadoop/config/core-site.xml
	* /etc/hadoop/config/hdfs-site.xml
	* /etc/hadoop/config/mapred-site.xml

 	There are individual folders for each node in the cluster in directories as shown below :
  
        * /etc/hadoop/conf.cloudera.hdfs1
	* /etc/hadoop/conf.cloudera.mapred1
	* /etc/hadoop/conf.cloudera.yarn1

 	All daemon log files are written under:

	* /var/log/hadoop/

	
   Common HDFS File Operations
   ---------------------------

	 * hadoop fs -help   				=> lists all the commands with a brief discription
	 * hadoop fs -help <command-name>  
	 * hadoop fs -chmod 755 <file/dir>
	 * hadoop fs -chmod -R 755 <file/dir>		=> change permission for a file/directory
	 * hadoop fs -cat <file-name>
	 * hadoop fs -ls
	 * hadoop fs -ls <dir-path>
	 * hadoop fs -ls -R  <dir-path>			=> Recursive listing of files and directories

	 * hadoop fs -cp <source file> <target dir>     => Copies within HDFS frpm source to target
	 * hadoop fs -mv <source file> <target dir>     => Moves within HDFS frpm source to target
	 * hadoop fs -rm -r <file-name/directory-name>
		 
	 // copy from Linux to HDFS
	 * hadoop -copyFromLocal <local file path>  <hdfs file path> 
	 * hadoop -put <local file path>  <hdfs file path> 

	 // copy from HDFS to Linux
	 * hadoop -copyToLocal <hdfs file path> <local file path>   
	 * hadoop -get <hdfs file path> <local file path>   

	 * hadoop -moveFromLocal <local file path>  <hdfs file path> 
	 * hadoop -moveToLocal <hdfs file path> <local file path> 


  More Commands
  -------------

 	 * du - disk usage  
		  
			ex:  hadoop fs -du /user/cloudera


	 * du -s :   Like -du, but prints a summary of disk usage of all files/directories in the path

			ex: hadoop fs -du -s /user/cloudera

	 * df : Shows the capacity, free and used space of the filesystem
			 -h  Formats the sizes of files in a human-readable fashion.

			ex: hadoop fs -df -h /user/cloudera/
				hadoop fs -df /user/cloudera/

	 * getmerge <src> <localDest>    :  Retrieves all files that match the path src in HDFS,
					and copies them to a single, merged file in the local 										file system identified by localDest. 

			ex: hadoop fs -getmerge test/*.txt test1/merged.txt

	 * Get the replication factor of a file:

			ex: hadoop fs -stat %r test/empinfo.txt

	 * Set the replication factor of a file while copying or moving from linux

			ex: hadoop fs -D dfs.replication=5 -copyFromLocal <source> <destination>
	  
				hadoop fs -D dfs.replication=5 -copyFromLocal datasets/empinfo.txt test2
				hadoop fs -stat %r test2/empinfo.txt

	 * Set the replication factor for an existing file:  -setrep [-R] [-w] <rep-factor> <path>
		   
			ex: hadoop fs -setrep 2 test2/empinfo.txt   => sets the rep-factor of the file to 2
			hadoop fs -stat %r test2/empinfo.txt    => test the rep-factor of the file

	 * appendToFile

		Usage: hdfs dfs -appendToFile <localsrc> ... <dst>

		Append single src, or multiple srcs from local file system to the destination file system. 
		Also reads input from stdin and appends to destination file system.

		hdfs dfs -appendToFile localfile /user/hadoop/hadoopfile
		hdfs dfs -appendToFile localfile1 localfile2 /user/hadoop/hadoopfile
		hdfs dfs -appendToFile localfile hdfs://nn.example.com/hadoop/hadoopfile
		hdfs dfs -appendToFile - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin.


	 * chmod, chgrp, chown  => Change the permissions of files.

 ==========
    YARN
 ==========

   -> Resource Manager

   -> Hadoop 1.0,  MapReduce used to handle resource-management
       -> MR was the only execution process.

   -> Hadoop 2.0
	->  Resource Management:  YARN  (Yet another resource negotiator)
        ->  Execution: MapReduce

        -> Allowed hadoop to support other execution frameworks as-well

      
   YARN Processing Components
   --------------------------
       1. Resource Manager (daemon)  -> one per cluster, runs in a master node
       2. Node Manager (daemon)	     -> one per node, runs in all slave nodes
       3. Application Master         -> one per application, can run in any NN
       4. Set of Containers          -> Allocated to AM where tasks are executed

	

  ==========================================================
     MapReduce
  ==========================================================

   -> Distributed Processing Framework.

    
   Two facets of MapReduce
   ------------------------

    1. MapReduce execution flow (framework process)
	-> How an mR program is executed (what the system does)

	1. Map Phase (developer)  
        2. Combiner
	3. Shuffle & Sort Phase (framework)
        4. Partitioner
	5. Reduce Phase (developer)


    2. MapReduce Programming Structure
	-> How to write MR programs (what developers should do)

	-> An MR Application can have several programs (upto 5 different programs)
   		1. Driver Program 
		       -> master process where configuration details are specified.
		2. Mapper
			-> Logic defined by the developer
			-> has map() method that run once for every input record
		3. Reducer
			-> Logic defined by the developer
			-> has reduce() method that run once for every input record
		4. Combiner
			-> Like a mini-reducer logic applied to the output of each mapper
                        -> Will reduce the amount of data shuffled and improves performance
                        -> Operation has to be commutative.  
		5. Partitioner
			-> Allows the developer to control which keys  goes to which partition
                           based on his own custom implementation.
   
   
   MapReduce Framework Classes
   ---------------------------

   1. InputFormat -> It will identify 'records' from the blocks

   2. InputSplit -> Creates input-split
                    An input-split will have integral number of records.
                    Each input-split can span multiple-blocks.

   3. RecordReader -> Will read input-splits and create (K,V) pairs from the records
                     Call map() method in a mapper with each record.



    MapReduce DataTypes
    -------------------
    -> MR data-types are optimized for serialization & deserialization.
    -> All MR class uses MapReduce Custom Data Types
    
      Java                MapReduce
      ----                ---------
      String		  Text
      Integer		  IntWritable
      Long		  LongWritable
      Boolean		  BoolWritable
      Null		  NullWritable


    Wordcount Program
    -----------------
    Find the word frequency count of all the words in a given text file.
   

    Input-Split
    ------------
InputSplit                          RecordReader			     Mapper -> map(k, v, context) 			    Shuffle & Sort			      Reduce														Shuffle & Sort  			Reduce      

hadoop spark scala flatmap    	-> (1, "hadoop spark scala flatmap")   	    (hadoop, 1) (spark, 1) (scala, 1) (flatmap, 1)   ===>   (hadoop, [1,1,1,1])    		   => (hadoop, 4)                                  
map groupby spark spark		-> (27, "map groupby spark spark")          (map, 1) (groupby, 1) (spark, 1) (spark, 1)       	    (spark, [1,1,1,1,1,1,1,1,1,1,1,1])        (spark, 12)
hadoop spark scala flatmap 	-> (50, "hadoop scala flume oozie")	    (hadoop, 1) (spark, 1) (scala, 1) (flatmap, 1)          (flatmap, [1,1,1,1)                       (flatmap, 4)
map groupby spark spark		-> (74, "sqoop hive hive spark spark")      (map, 1) (groupby, 1) (spark, 1) (spark, 1)		    (scala, [1,1,1,1,1,1])		      (scala, 6)
hadoop spark scala flatmap	-> (102, "mapreduce hadoop hive hadoop")    (hadoop, 1) (spark, 1) (scala, 1) (flatmap, 1) 
map groupby spark spark		-> (135, "hdfs flatmap rdd scala")          (map, 1) (groupby, 1) (spark, 1) (spark, 1) 
hadoop spark scala flatmap 	-> (168, "spark scala transformations")     (hadoop, 1) (spark, 1) (scala, 1) (flatmap, 1)
map groupby spark spark		-> (197, "actions rdd rdd rdd rdd")         (map, 1) (groupby, 1) (spark, 1) (spark, 1)

      
     mapper   																															  
     -------
     input => key type: LongWritable     output =>   key type: Text 
	      value type: Text                       value type: LongWritable


     reducer 																															  
     -------
     input => key type: Text            output =>      key type:  Text
	      value type: LongWritable                 value type: LongWritable

   
   Submitting MR program to the cluster
   ------------------------------------

    $hadoop jar <jar-file-path> <driver-class-qualified-name> <commad-line-args>
    $yarn jar <jar-file-path> <driver-class-qualified-name> <commad-line-args>
   
    $hadoop jar /home/cloudera/mrpractice.jar com.sample.WordCountJob <input-file-path> <output-dir>
    
 
    MapReduce Life Cycle Methods
    ----------------------------


	Mapper Life Cycle methods
	--------------------------

	1. setup	-> Called once at the beginning of the MR process before the map() method
                           is called.
			-> Write initialization code.			
			  
	2. map		-> Executed once for every record in the input-split
			-> Write the logic that you want to apply on the record.

	3. cleanup	-> Called once after the map() method is completed for all records.
			-> Release your resource and other cleanup.

	4. run 		-> Generally not overridden
                        -> Can use this to change the execution flow.


	Reducer Life Cycle methods
	---------------------------

	1. setup	-> Called once at the beginning of the MR process before the reduce() method
                           is called.
			-> Write initialization code.			
			  
	2. reduce       -> Executed once for every record in the input-split
			-> Write the logic that you want to apply on the record.

	3. cleanup	-> Called once after the reduce() method is completed for all records.
			-> Release your resource and other cleanup.

	4. run 		-> Generally not overridden
                        -> Can use this to change the execution flow.
   
  
  Required external jars to add to MR project 
  -------------------------------------------
 
   1. Create a Java Project
	File -> New -> Java Project

   2. Right-click on the project 
	-> Build Path -> Configure Buid Path -> Libraries -> Add External JARS
	
	Add the following jar files   
   	-> /usr/lib/hadoop/hadoop-common.jar
   	-> /usr/lib/hadoop-0.20-mapreduce/hadoop-core-mr1.jar

   3. Build the Java Project

	Right-click on the project 
	-> Export -> Java -> Jar File -> Give a name and Finish


 ====================
    Hive
 ====================

    Hive is Hadoop's Data Warehousing Framework.
    
    Hive is an SQL wrapper on top of map-reduce. So that user can directly work with SQL without
    having to write MR programs. HIve Server can create the MR programs and run them on the cluster
    releaving the analysts from writing MR programs.

    => Hive is NOT an RDBMS. 
    => Hive is NOT a NoSQL database

    Hive provides use a dialect of SQL called "Hive Query Language" (HQL, HiveQL)


    Hive Components
    ----------------
  
    1. Hive Warehouse - An HDFS directory where hive stores all its data files of all its managed tables.
                        In cloudera:  /user/hive/warehouse

    2. Hive MetaStore - MetaStore is typically an RDBMS where hive stores its metadata
                        (Most popular option in production systems is MySQL)
                        -> Hive comes with an embedded metastore database called "derby"
        -> WareHouse  : DATA
	-> Meta Store : METADATA

    3. Hive Server   - Responsible for executing HQL statements (and creating MR programs if required)



    How to Access Hive ?
    --------------------

    1. Command Line Interface (CLI Shells)
	-> hive shell  
		-> Local hive installations with default user.
		-> Less secure

        -> beeline shell (preferred for security reasons)
		-> Connects to Hive using JDBC connection with a specific user credential
                -> More secure.

    2. HUE (Hadoop User Environment)
         -> Provides a Web UI to interact with various Hadoop tools.

    3. Client Applications  ( JDBC, ODBC, Thrift )


   Hive File Structure 
   -------------------
   
    Hive Warehouse:  /user/hive/warehouse

    ->  Hive created separate directories for each database in the Hive warehouse
        (exception: no separate directory for 'default' database)

    -> For every table in the database, a separate directory is created.
       All the data files of that table are saved in that directory.

       ex: db1.emp   ==> /user/hive/warehouse/db1.db/emp
	   db1.emp2  ==> /user/hive/warehouse/db1.db/emp2



   To display current database
   ---------------------------
 	set hive.cli.print.current.db=true;   -> To show db name


   Defaults
   --------
        -> default serde: DELIMITED
        -> default storage format : TEXTFILE
        -> default line terminator: \n
        -> default field terminator: Ctrl-A  ( \0001 )           

   Describe
   ---------
        -> desc empinfo
	-> desc extended empinfo
	-> desc formatted empinfo    


--------------------------------------------------------------------
		  CREATE TABLE  & CTAS Construct
--------------------------------------------------------------------
--- Managed table ---
CREATE TABLE empinfo (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n' 
STORED AS TEXTFILE;

--- External table ---
CREATE EXTERNAL TABLE empexternal(empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION '/user/cloudera/emp_external';

--------------------------------------------------------------------
		DATA LOADING 
--------------------------------------------------------------------

--append data
LOAD DATA LOCAL INPATH '<local linux file path>' INTO TABLE <table name>
LOAD DATA INPATH '<hdfs file path>' INTO TABLE <table name>

-- overwrite data
LOAD DATA LOCAL INPATH '<local linux file path>' OVERWRITE INTO TABLE <table name>
LOAD DATA INPATH '<hdfs file path>' OVERWRITE INTO TABLE <table name>

--loading data into external table --
LOAD DATA LOCAL INPATH '<local linux file path>' INTO TABLE <table name>



    Hive follows Scehma on Read
    ---------------------------
    -> Hive do not control what data goes into the table. 
    -> Schema validation does not happen while "loading" data into the table.
    -> Scahema validation happens while reading the data.
       -> If the data can not be parsed as per schema, Hive shows NULL for that field.


    Managed Tables
    --------------
        -> Managed tables are managed by Hive and are stored in Hive warehouse directory.
        -> When you drop a managed table, Hive drops the schema as well as the data files. 

    
    External Tables
    ---------------
        -> External tables are not managed by Hive.
        -> The data files of external table are not stored in Hive warehouse. 
        -> They are stored in an external location specified while creating the table.
        -> The data loaded into external table either from hive or from any other application/tool
           is not owned/managed by Hive.
        -> When we drop the table, ONLY schema is dropped. The data files are NOT DROPPED.
  

    Use Cases for External tables
    -----------------------------

       -> External tables are used when multiple hive databases are using the same data. 
       -> You can create an external table pointing to existing data, analyze that data and drop the
          table without affecting the actual data.




    Partitioned Tables
    ------------------

     In Hive, Partitions provide a way of dividing a table into coarse-grained 
     parts based on the value of a partition column, such as a date or country.


     Partitioning allows us to create tables partitioned by one or more columns.
     
     Each unique value of the partitioned column will have its own directory 
     within the table's directory.

     Based on the way we load data into the tables we can have static & dynamic partitions

     -> Partition columns should reasonably low-cardinality columns (such as year, country, month, ...)


     Static Partitioning
     -------------------
	-> The data files are loaded by the user using "LOAD DATA .." command
        -> The user has to know precisly to which partition he is loading the file.


     Dynamic Partitioning
     --------------------
	-> We dynamic partitioning when the user do not know to which partition the data goes to.
           This may be because the partition info itself is there in the data or may be we have
           data of many partitions in the same file. 

        -> We can not use "LOAD DATA.." command to load the data into the partitioned table

        -> We have to load the data using dynamic partitioning scheme from a base table using
           "INSERT .. SELECT .." command. 

        -> Data files are created dynamically by hive (hense the name dynamic partitioning)

        -> set hive.exec.dynamic.partition.mode=nonstrict



CREATE TABLE students_part (id INT, name STRING, phone STRING, email STRING, project STRING)
PARTITIONED BY (country STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;


LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/students_IN.txt' INTO TABLE students_part

PARTITION (country='India');


CREATE TABLE students_dynamic_part (id INT, name STRING, phone STRING, email STRING, project STRING)
PARTITIONED BY (country STRING, batch INT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;


// Base Table
CREATE TABLE students (id INT, name STRING, phone STRING, email STRING, project STRING, country STRING, batch INT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;


LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/students_country_batch.csv' INTO TABLE students;


// Loading Data using Dynamic Partitioning

INSERT INTO students_dynamic_part
PARTITION (country, batch)
SELECT id, name, phone, email, project, country, batch FROM students;

INSERT INTO students_dynamic_part
PARTITION (country='India', batch)
SELECT id, name, phone, email, project, batch FROM students
WHERE country = 'India';

INSERT INTO students_dynamic_part
PARTITION (country='Japan', batch=100)
SELECT id, name, phone, email, project FROM students
WHERE country = 'USA' and batch=1;

-- external partitioned table

CREATE EXTERNAL TABLE students_dyn_part_ext
(id int, name string, mobile string, email string, project string)
PARTITIONED BY (country string, batch int)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '|'
LINES TERMINATED BY '\n' 
STORED AS TEXTFILE
LOCATION '/user/cloudera/hive/students_dyn_part_ext';


INSERT OVERWRITE TABLE students_dyn_part_ext
PARTITION (country, batch) 
SELECT id, name, mobile, email, project, country, batch 
FROM students_country_batch;

----------------

LOAD DATA LOCAL INPATH '/home/cloudera/hive-material/data/emp.txt' INTO TABLE empinfo;


-- overwrite
INSERT OVERWRITE TABLE students_country_usa
SELECT * FROM students_country_batch WHERE country='USA'

-- append
INSERT INTO TABLE students_country_usa
SELECT * FROM students_country_batch WHERE country='USA'

--hdfs diretory
INSERT OVERWRITE DIRECTORY '/user/cloudera/hive_data/students_country_batch'
SELECT * FROM students_country_batch WHERE country='USA';

INSERT OVERWRITE DIRECTORY '/user/cloudera/hive_data/students_country_batch'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
SELECT * FROM students_country_batch WHERE country='USA';

INSERT INTO DIRECTORY '/user/cloudera/hive_data/students_country_batch'
SELECT * FROM students_country_batch WHERE country='USA';


--local directory
INSERT OVERWRITE LOCAL DIRECTORY '/home/cloudera/hive_data/students_country_batch'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|'
SELECT * FROM students_country_batch WHERE country='USA';

INSERT OVERWRITE LOCAL DIRECTORY '/home/cloudera/hive_data/students_country_batch_avro'
STORED AS AVRO 
SELECT * FROM students_country_batch WHERE country='USA';

-- creating tables using custom formats
CREATE TABLE empinfo_orc (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
STORED AS ORC;

CREATE TABLE empinfo_parquet (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
STORED AS PARQUET;

CREATE TABLE empinfo_avro (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
STORED AS AVRO;


INSERT OVERWRITE TABLE empinfo_orc
SELECT * FROM empinfo;


Working with different file formats
====================================================

     ORC & Parquet   ==> Columnar File Formats 
                       -> Store all column information together at one physical location.
                       -> This improves performance of aggrgations
                       -> As aggregations are heavily used in analytical warkloads, these formats are very
                          efficient in data-warehouseing like use-cases.

                   => Very good read performance
                   => moderatly good write performance
                   => Splittable
		   => Compressable


     Avro  => Is used where we need schema evolution. 
                 -> The data schema is embedded in the file itself.

     Sequence => Stores data in the form of (K, V) pairs.


SEQUENCE FILE
=============

CREATE TABLE empinfo_seq (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT) 
STORED AS SEQUENCEFILE;

INSERT OVERWRITE LOCAL DIRECTORY '/home/cloudera/hive_data/empinfo_seq'
STORED AS SEQUENCEFILE 
SELECT * FROM empinfo;

LOAD DATA LOCAL INPATH '/home/cloudera/hive_data/empinfo_seq/000000_0' OVERWRITE INTO TABLE empinfo_seq;

INSERT OVERWRITE TABLE empinfo_seq SELECT * FROM empinfo;


ORC FILE
========

CREATE TABLE empinfo_orc (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT) 
STORED AS ORC;

INSERT OVERWRITE LOCAL DIRECTORY '/home/cloudera/hive_data/empinfo_orc'
STORED AS ORC 
SELECT * FROM empinfo;

LOAD DATA LOCAL INPATH '/home/cloudera/hive_data/empinfo_orc/000000_0' OVERWRITE INTO TABLE empinfo_orc;

INSERT OVERWRITE TABLE empinfo_orc SELECT * FROM empinfo;

LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/emp.txt' OVERWRITE INTO TABLE empinfo_orc;
--> The above command leads to IOException.

PARQUET   
=======

CREATE TABLE empinfo_parquet (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT) 
STORED AS PARQUET;

INSERT OVERWRITE TABLE empinfo_parquet SELECT * FROM empinfo;

AVRO
====

CREATE TABLE empinfo_avro (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT) 
STORED AS AVRO;

INSERT OVERWRITE TABLE empinfo_avro SELECT * FROM empinfo;




-- multi-table insert command

FROM students_country_batch
  INSERT OVERWRITE TABLE students_usa 
    SELECT id, name, mobile, email, project, country, batch WHERE country = 'USA'
  INSERT OVERWRITE TABLE students_uk 
    SELECT id, name, mobile, email, project, country, batch WHERE country = 'UK'
  INSERT OVERWRITE TABLE students_india 
    SELECT id, name, mobile, email, project, country, batch WHERE country = 'India';


-- working with complex data types..
CREATE TABLE complex_data
(col1 array<string>,
 col2 map<int,string>,
 col3 struct<name:string,score:int>)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
COLLECTION ITEMS TERMINATED BY '/'
MAP KEYS TERMINATED BY ':'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '/home/cloudera/hive-material/hive_complex_data' INTO TABLE complex_data;




  Joins
  -----

   Hive supports the following joins:

	-> inner join
	-> outer joins (left outer, right outer, full outer)
        -> left semi join


   Left Semi Join:
   ---------------
	-> The data fetched only from left side table for the keys that are there in the joined table.
        -> Very much like inner join, except that the data comes only from left side table.

         -> Left Semi Join is an alias of the following query.
	
		=> select * from emp where deptid in (select deptid from dept)
                => select * from emp left semi join dept on (emp.deptid = dept.deptid);

            (both the above queries are functionally same)


   Map-side Joins
   --------------
       -> map-joins are very efficient and aviods the reduce phase completed.
       -> The join is performed at the map-phase itself. 

        Conditions for performing map-join automatically:

	    -> set hive.auto.convert.join = true   (defaul is true)
            -> All the tables except one must be 'small table'. 
		 -> Small table is defined by the following parameter:
                      hive.mapjoin.smalltable.filesize (default: 25MB)

       If both the tables that are joined are big tables (> hive.mapjoin.smalltable.filesize), then
       hive does not perform map-join.
 
          -> In this case, developers have to apply optimizations to improve join efficiency.

        1. Bucket Map Join	
              -> Both the tables that are joined must be bucketted on the join column
              -> The number of buckets of one table should be a multiple of other.
              -> Set the following config setting: 
			set hive.enforce.bucketing = true; 
			set hive.optimize.bucketmapjoin = true;

        2. Sort Merge Join 
	      -> Both the tables that are joined must be bucketted on the join column
              -> The number of buckets of both tables must be same
              -> Set the following config setting: 
			set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
			set hive.optimize.bucketmapjoin = true;
			set hive.optimize.bucketmapjoin.sortedmerge = true;



CREATE TABLE IF NOT EXISTS stocks 
(stock_exchange STRING, symbol STRING, trxn_date DATE, open DOUBLE, 
high DOUBLE,low DOUBLE, close DOUBLE, volumn INT, adj_close DOUBLE)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n' 
STORED AS TEXTFILE;


LOAD DATA LOCAL INPATH '/home/cloudera/hive-material/data/stocks' 
OVERWRITE INTO TABLE stocks; 


CREATE TABLE IF NOT EXISTS dividends 
(stock_exchange STRING, symbol STRING, trxn_date DATE, dividend DOUBLE)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n' 
STORED AS TEXTFILE;


LOAD DATA LOCAL INPATH '/home/cloudera/hive-material/data/dividends' 
OVERWRITE INTO TABLE dividends; 


=================================================
   JOINS & STREAMING JOINS
=================================================

 ------------------------------------------------
 -- regular join
 ------------------------------------------------
 SELECT s.trxn_date, s.symbol, s.adj_close, d.dividend
 FROM stocks s JOIN dividends d
 ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol; 

 ------------------------------------------------
 -- regular join with better placement of tables
 ------------------------------------------------
 SELECT s.trxn_date, s.symbol, s.adj_close, d.dividend
 FROM dividends d JOIN stocks s
 ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol;

 ------------------------------------------------
 -- stream join hint (stream a table to reducer)
 ------------------------------------------------
 SELECT /*+ STREAMTABLE(s) */
 s.trxn_date, s.symbol, s.adj_close, d.dividend
 FROM stocks s JOIN dividends d
 ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol;
 

=================================================
  Map Joins
=================================================

 ------------------------------------------------
 -- map join hint
 ------------------------------------------------
 SELECT /*+ MAPJOIN(d) */
 s.trxn_date, s.symbol, s.adj_close, d.dividend
 FROM stocks s JOIN dividends d
 ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol;

 ------------------------------------------------
 --property to be set (default is true anyway)
 ------------------------------------------------
 set hive.auto.convert.join=true

 set hive.mapjoin.smalltable.filesize = 25000000

 SELECT s.trxn_date, s.symbol, s.adj_close, d.dividend
 FROM stocks s JOIN dividends d
 ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol; 

=================================================
  Bucket Map Joins
=================================================

CREATE TABLE IF NOT EXISTS stocks_bkt_8 
(stock_exchange STRING, symbol STRING, trxn_date DATE, open DOUBLE, 
high DOUBLE,low DOUBLE, close DOUBLE, volumn INT,adj_close DOUBLE)
CLUSTERED BY (trxn_date, symbol) INTO 8 BUCKETS 
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;

CREATE TABLE IF NOT EXISTS dividends_bkt_4 
(stock_exchange STRING, symbol STRING, trxn_date DATE, dividend DOUBLE)
CLUSTERED BY (trxn_date, symbol) INTO 4 BUCKETS 
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;

set hive.enforce.bucketing = true; 
set hive.optimize.bucketmapjoin = true;


INSERT INTO stocks_bkt_8 SELECT * FROM stocks SORT BY trxn_date, symbol;

INSERT INTO dividends_bkt_4 SELECT * FROM dividends SORT BY trxn_date, symbol;

SELECT /*+ MAPJOIN(dividends_bkt_4) */ s.trxn_date, s.symbol, s.adj_close, d.dividend
FROM stocks_bkt_8 s JOIN dividends_bkt_4 d
ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol;

=================================================
  Sort Merge Joins
=================================================

CREATE TABLE IF NOT EXISTS stocks_bkt_4
(stock_exchange STRING, symbol STRING, trxn_date DATE, open DOUBLE, 
high DOUBLE,low DOUBLE, close DOUBLE, volumn INT,adj_close DOUBLE)
CLUSTERED BY (trxn_date, symbol) INTO 4 BUCKETS 
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;

INSERT INTO stocks_bkt_4 SELECT * FROM stocks SORT BY trxn_date, symbol;

set hive.auto.convert.sortmerge.join=true;
set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.auto.convert.sortmerge.join.nonconditionaltask=true;

SELECT /*+ MAPJOIN(dividends_bkt_4) */ s.trxn_date, s.symbol, s.adj_close, d.dividend
FROM stocks_bkt_4 s JOIN dividends_bkt_4 d
ON s.trxn_date = d.trxn_date AND s.symbol = d.symbol;


Immutable Tables & Transaction Tables
-------------------------------------
  
 Immutable Tables
 ----------------

   -> Protect you from accedentally appending data to a non-empty table using INSERT INTO
      command.
   -> It won't protect from "LOAD DATA ..." command.
   -> The behavior of INSERT OVERWRITE is not affected by the "immutable" table property.

CREATE TABLE emp_immutable
(empid INT, empname STRING, salary DOUBLE, deptid SMALLINT) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ',' 
LINES TERMINATED BY '\n' 
STORED AS TEXTFILE 
TBLPROPERTIES ("immutable"="true")

INSERT INTO TABLE emp_immutable SELECT * FROM empinfo where empid = 101;
INSERT INTO TABLE emp_immutable SELECT * FROM empinfo where empid = 102;   // error
INSERT OVERWRITE TABLE emp_immutable SELECT * FROM empinfo where empid = 101;
LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/emp.txt' INTO TABLE emp_immutable;


 Transactional Tables
 --------------------

    -> Transactional tables allow updates and deletes
    -> Provides full ACID support. 

    Conditions to create transactional tables
   
     -> make the "transactional" table property as "true"
     -> table must be a bucketted table
     -> storage format should be "ORC"
     -> set some configuration options.  
	
 	  set hive.support.concurrency = true;
	  set hive.enforce.bucketing = true;
	  set hive.exec.dynamic.partition.mode = nonstrict;
	  set hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
	  set hive.compactor.initiator.on = true;
	  set hive.compactor.worker.threads = 1;

CREATE TABLE college(clg_id int,clg_name string,clg_loc string) 
CLUSTERED BY (clg_id) INTO 5 buckets 
STORED AS ORC
TBLPROPERTIES('transactional'='true');

INSERT INTO TABLE college values(1,'nec','nlr'),(2,'vit','vlr'),(3,'srm','chen'),(4,'lpu','del'),(5,'stanford','uk'),(6,'JNTUA','atp'),(7,'cambridge','us');
 
UPDATE college set clg_id = 8 where clg_id = 7;
UPDATE college set clg_name = 'IIT' where clg_id = 6;
DELETE FROM college WHERE clg_id=5;


----- Alter--
-- rename a table ---
ALTER TABLE empinfo RENAME TO employee;

--renaming a partition --
ALTER TABLE students_part_batch PARTITION (batch=1) RENAME TO PARTITION (batch=10);

-- add a column to an existing table --- 
ALTER TABLE employee ADD COLUMNS (email STRING, facebook STRING);

-- add a new partition to a partioned table ---
ALTER TABLE emp ADD PARTITION (country='CN') LOCATION '<directory-path>';

-- change a column name and type ---
ALTER TABLE employee CHANGE empname ename STRING;
ALTER TABLE employee CHANGE ename empname STRING AFTER salary;

-- alter table location / partition location --
ALTER TABLE emp SET LOCATION '<directory-path>';
ALTER TABLE emp PARTITION (country = 'IN') SET LOCATION '<directory-path>';


===========================
   Impala
===========================

   -> General purpose SQL engine developed by Cloudera. 
	-> Works for both analytical and transactional loads (interactive queries)
        -> Supports queries that take milliseconds upto hours.

   -> Real time query processing engine for Hadoop
        -> Uses its own processing engine called "Massive Parallel Parocessing (MPP)" database engine.

   -> Runs directly on top of HDFS
	-> Does not use MR
	-> Reads widely used hadoop formats (ORC, Parquet, SequenceFile, Text, AVRO)
	-> Can co-exist with Hadoop cluster 

    -> High Peformance
	-> Written in C++ mostly (some Java)
        -> Used run-time code generation
	-> completly new execution engine.

    -> Impala uses:
	 -> The same metastore as Hive.
	 -> Uses the same SQL (Hive Query Language)
	 -> Uses the hive warehouse to store its tables.
       

   Meta Data and Table Data in Impala
   -----------------------------------
    => Whenever hive metadata changes outside of impala (may be by hive) that metadata is not automatically
       refreshed in Impala Catalog.
          -> Impala maintains a catalog where it caches all the metadata.
       => To refresh impala catalog use the following command:
	   -> "INVALIDATE METADATA"

    => Whenever new data is added to a table outside of impala, we have to run the following command
       to refresh the table in impala:
	   -> REFRESH <table-name>


   Impala Daemons & Impala Architecture
   ------------------------------------

    1. impalad
	-> runs on every node on the impala cluster
	-> accepts client requests / queries
        -> performs planning, coordination and execution of the client queries
        -> distributes the query execution across many nodes and coordinates them
        -> sends the results back to client.

         Three Processes:
		-> Query Planner
		-> Query Coordinator
		-> Query Executors (distributed across many nodes)

    2. statestored
        -> Runs on only one node in cluster
	-> All the impalad nodes communicate with statestored continuosly
        -> if any node is down, then that info is communicated to all the impala nodes

    3. catalogd
	-> Maintains the metadata
	-> Any changes to the metadata are relayed to all impalad nodes
	-> If metadata changes happen outside of the impala cluster (for ex by hive), those
           changes have to refreshed using "invalidate metadata" command. 


 ======================
   Sqoop
 ======================
  -> Sqoop   => SQL on Hadoop  -> (SQl hadOOP)

  -> Structured Data Ingestion Tool
  -> Built on top of MapReduce.

  -> Using Sqoop, you can import data from external SQL datastores (RDBMS) to Hadoop native data stores (HDFS, Hive, HBase). 

  -> Sqoop launches multiple map-only tasks on the cluster to efficiently and parallely transfer bulk data between RDBMS and Hadoop systems. 



sqoop list-databases --connect jdbc:mysql://localhost:3306 --username root -password cloudera

sqoop list-tables --connect jdbc:mysql://localhost:3306/retail_db --username root -password cloudera

====================
  import
====================

sqoop import --connect jdbc:mysql://localhost:3306/retail_db --username root --password cloudera --table categories -m 1 --target-dir /user/cloudera/sqoop/retail_db/categories --fields-terminated-by ',' --lines-terminated-by '\n'

sqoop import --connect jdbc:mysql://localhost:3306/retail_db --username root --password cloudera --table customers -m 1 --target-dir /user/cloudera/sqoop/retail_db/customers  --fields-terminated-by '\t' --delete-target-dir

sqoop import --connect jdbc:mysql://localhost:3306/retail_db --username root --password cloudera --table customers -m 1 --target-dir /user/cloudera/sqoop/retail_db/customers3  --fields-terminated-by '\t' --enclosed-by '"'


-- overwrite mode (delete mode)
sqoop import --connect jdbc:mysql://localhost:3306/retail_db --username root --password cloudera --table categories -m 1 --target-dir /user/cloudera/sqoop/retail_db/categories --fields-terminated-by ',' --lines-terminated-by '\n' --delete-target-dir

-- append mode
sqoop import --connect jdbc:mysql://localhost:3306/retail_db --username root --password cloudera --table categories -m 1 --target-dir /user/cloudera/sqoop/retail_db/categories --fields-terminated-by ',' --lines-terminated-by '\n' --append

--specific columns
sqoop import --connect jdbc:mysql://localhost:3306/retail_db --username root -P --table customers --columns 'customer_id,customer_fname,customer_lname' -m 1 --target-dir /user/cloudera/sqoop/retail_db/customers1 --fields-terminated-by '|'


sqoop import --connect jdbc:mysql://localhost:3306/retail_db --username root -P --table customers --columns 'customer_id,customer_fname,customer_lname' -m 3 --target-dir /user/cloudera/sqoop/retail_db/customers2

sqoop import --connect jdbc:mysql://localhost:3306/retail_db --username root -P --table customers --columns 'customer_id,customer_fname,customer_lname' -m 3 --target-dir /user/cloudera/sqoop/retail_db/customers2

sqoop import --connect jdbc:mysql://localhost:3306/demodb --username root -P --table students -m 3 --target-dir /user/cloudera/sqoop/demodb/students --split-by 'id'

sqoop import --connect jdbc:mysql://localhost:3306/retail_db --username root --password cloudera --query 'select * from customers where customer_id < 100 and $CONDITIONS'  -m 1 --target-dir /user/cloudera/sqoop/retail_db/customer_id 

--as-sequencefile 
sqoop import --connect jdbc:mysql://localhost:3306/retail_db --username root -P --table customers -m 1 --target-dir /user/cloudera/sqoop/retail_db/customers_seq --as-sequencefile 

--as-sequencefile 
sqoop import --connect jdbc:mysql://localhost:3306/retail_db --username root -P --table customers -m 1 --target-dir /user/cloudera/sqoop/retail_db/customers_avro --as-avrodatafile



===  importing all tables in the database ===

sqoop import-all-tables --connect jdbc:mysql://localhost:3306/demodb --username root --password cloudera -m 1 --warehouse-dir /user/cloudera/sqoop/demodb


===== Importing data into Hive =====

sqoop create-hive-table --connect jdbc:mysql://localhost:3306/retail_db --username root --password cloudera --table categories --fields-terminated-by ',' --hive-table testdb.categories



sqoop import --connect jdbc:mysql://localhost:3306/retail_db --table categories --username root --password cloudera --hive-table testdb.categories -m 1 --fields-terminated-by ',' --lines-terminated-by '\n' --hive-import 


  export
====================

sqoop export --connect jdbc:mysql://localhost:3306/exportdb --table employee --username root --password cloudera -m 1  --export-dir /user/cloudera/sqoop/upsert/emp.txt --input-fields-terminated-by ','



sqoop export --connect jdbc:mysql://localhost:3306/exportdb --table students --username root --password cloudera -m 1  --export-dir /user/cloudera/sqoop/demodb/students/part-m-* --input-fields-terminated-by ','  

sqoop export --connect jdbc:mysql://localhost:3306/exportdb --table customers --username root --password cloudera -m 1  --export-dir /user/cloudera/sqoop/retail_db/customers/* --input-fields-terminated-by '\t'  

sqoop export --connect jdbc:mysql://localhost:3306/exportdb --table students --username root --password cloudera -m 1  --export-dir /user/cloudera/sqoop/demodb/students/part-m-* --input-fields-terminated-by ',' --columns 'id,name,mobile,email'

sqoop export --connect jdbc:mysql://localhost:3306/exportdb --table customers --username root --password cloudera -m 1  --export-dir /user/cloudera/sqoop/retail_db/customers_avro 

sqoop export --connect jdbc:mysql://localhost:3306/exportdb --table customers --username root --password cloudera -m 1  --export-dir /user/cloudera/sqoop/retail_db/customers_seq

sqoop export --connect jdbc:mysql://localhost:3306/exportdb --table employee --username root --password cloudera -m 1 --export-dir /user/cloudera/sqoop/upsert/emp2.txt --update-key 'empid' --update-mode 'allowinsert'

NOTE: --update-mode has two possible values: 'updateonly', 'allowinsert'


  eval
================

sqoop eval --connect jdbc:mysql://localhost:3306/demodb --username root --password cloudera --query 'show tables'

sqoop eval --connect jdbc:mysql://localhost:3306/demodb --username root -P --query 'select * from students'

sqoop eval --connect jdbc:mysql://localhost:3306/demodb --username root --password cloudera --query 'create table tab1(c1 int, c2 varchar(50))'

sqoop eval --connect jdbc:mysql://localhost:3306/demodb --username root --password cloudera --query 'insert into tab1 (c1, c2) values (1, "raju"), (2, "ramesh")'

sqoop eval --connect jdbc:mysql://localhost:3306/demodb --username root --password cloudera --query 'update tab1 set c2 = "kanakaraju" where c1=1'

sqoop eval --connect jdbc:mysql://localhost:3306/demodb --username root --password cloudera --query 'delete from tab1 where c1=1'

sqoop eval --connect jdbc:mysql://localhost:3306/demodb --username root --password cloudera --query 'drop table tab1'

  codegen
================

sqoop codegen --connect jdbc:mysql://localhost:3306/demodb --table emp --username root --password cloudera --class-name EmpTable


================
  HBase
================

   RDBMS Database Limitations:
   ---------------------------

      -> Scalability limitations
	  -> They run on a single machine
	  -> Can not support unlimited scalability.
          -> Not designed as distributed databases.

      -> Does not support dynamic schemas
	  -> RDBMSs have fixed schema
          -> All rows of a table have same number of columns.
	  -> Does not support schema evolution. 
   

   NoSQL Databases
   ---------------
      -> Designed as distributed databases.
	  -> All NoSQL databases run on a cluster.
      -> Uses denormalized data models
	  -> Non-relational
	  -> All the data of a single entity is stored in the same table
          -> No joins
      -> Schema-less (flexible schema)
	  -> Each row need not have same number of columns
          -> Columns need not be part of the table structure.  
          -> Supports applications with schema evolution


   Types of NoSQL databases
   ------------------------
	
        1. Column (column-family) orieted databases (Ex: HBase, Cassandra)
	2. Document oriented databases (ex: MongoDB, CouchDB, DynamoDB)
	3. Key-value Stores (ex: Redis)
	4. Graph Databases (ex: Neo4J)
  
     
   Limitations of Hadoop
   ---------------------         
      -> Unstructured Data
            -> HDFS does not have any schema
	    -> They are just files
	
      -> No Random Access supported
	    -> Even to read a single record, you have process the entire file.
         
      -> High Latency
	    -> MR does not have throughput to support transactional databases.

       -> Not ACID complaint
	    -> Databases are reliable source of data because of ACID guarantees. 

    HBase   
    -----
        -> Is a distributed database management system that provides transactional processing
	    -> Part of Hadoop ecosystem.
	
        -> Three seminal papers of Google (published bw 2003 & 2006)
	     -> Google File System	: HDFS
	     -> MapReduce		: MapReduce
	     -> Google Big Table	: HBase


	-> HDFS uses HDFC to store persistent data
		-> distributed storage
		-> fault tolerant.


   Features of HBase
   -----------------
	-> Is a column-family oriented database
	-> HBase does support SQL
	-> Denormalized data design
	    -> We don't normalize table
	    -> Store all data in a single table 
		-> organize data as column-families
	-> Uses column-oriented storage
	    -> more like a map than a table (in RDBMS sense)
	-> HBase only supports a basic set of operations (CRUD) on a "single-row"
	    -> All operations are applied at "row-level"
	    -> No Joins
	    -> No Group By (aggregartions)
	    -> No Sub-Queries
	-> Very limited ACID complience
	    -> ACID complience is at row-level.


   Working with HBase
   ------------------

    1. Client Application (using Java API)
    2. HBase Shell
    3. Web Clients (using thrift service) - HUE


    HBase Data Model
    ----------------

	-> The basic unit of data is a "column" (or cell)

	-> A row is identified by an "row-id"
	-> Every column belongs a column-family and has a row-id attached to it
		-> all the columns with the same row-id make up a row
	-> Each row can have its own set of columns.
	-> Each column will have 
		-> A row-id
		-> A column-family
		-> A value
		-> A timestamp (generated by Hbase)

		-> A cell is a key-value pair
			Key: (Row-Id, Col-Identifie)   col-id => column-family:column-name
			Value: Value, TimeStamp

			Key: (1, 'basic:name')
			     ("raju", 16020456345237)

	-> Each cell is versioned.
		-> A cell added with the same row-id and same column-identifier with 
		   a different will result in a new version of the cell being created with
		   a different (latest) timestamp.
		-> version is identified by a timestamp

	-> The number of versions you want to maintain can be configured for each column-family
		-> default is 1.

   	-> Basic unit of data is a cell
	    -> We can insert one cell
	    -> We can update one cell
	    -> We can delete one cell
                  -> UNlike RDBMS, where we work with rows.

  NOTE:  Apache Hbase is usually used along with Apache Phoenix, which provides an SQL-like 
         interface for developers. 


  

create 'emp', 'basic', 'contact', 'salary', 'skills', 'other'

create 'emp2', {NAME => 'basic', VERSIONS => 3}, {NAME => 'contacts', VERSIONS => 2}, {NAME => 'skills', VERSIONS => 2}, {NAME => 'other', VERSIONS => 2}

describe 'emp'

-----------------------------------------------------
put 'emp', '1', 'basic:empid','101' 
put 'emp', '1', 'basic:name','Raju'
put 'emp', '1', 'basic:age','45'
put 'emp', '1', 'basic:email','raju@gmail.com'
put 'emp', '1', 'contact:mobile','8501044567'
put 'emp', '1', 'contact:address','blah blah'
put 'emp', '1', 'salary:basic','40000'
put 'emp', '1', 'salary:hr','10000'
put 'emp', '1', 'skills:rdbms','mysql,oracle'
put 'emp', '1', 'skills:proglang','python,java'

put 'emp', '2', 'basic:empid','102' 
put 'emp', '2', 'basic:fname','Kanaka Raju'
put 'emp', '2', 'basic:lname','Y'
put 'emp', '2', 'basic:dob','2000-02-01'
put 'emp', '2', 'basic:email','kraju@gmail.com'
put 'emp', '2', 'contact:mobile','8501044567'
put 'emp', '2', 'contact:address','blah blah'
put 'emp', '2', 'salary:basic','40000'
put 'emp', '2', 'salary:hr','10000'
put 'emp', '2', 'skills:rdbms','mysql,oracle'
put 'emp', '2', 'skills:nosql','hbase,mongdb'


put 'emp', '5', 'basic:empid','105' 
put 'emp', '5', 'basic:name','Aditya'
put 'emp', '5', 'basic:dob','2020-02-02'
put 'emp', '5', 'basic:email','ramesh@gmail.com'
put 'emp', '5', 'contact:mobile','8501044567'
put 'emp', '5', 'contact:mobile2','8501044566'
put 'emp', '5', 'skills:databases','oracle,hbase'
put 'emp', '5', 'other:hobbies','movies,reading'


put 'emp', '10', 'basic:empid','110' 
put 'emp', '10', 'basic:name','Aditya'
put 'emp', '10', 'basic:dob','2020-02-02'
put 'emp', '10', 'basic:email','ramesh@gmail.com'
put 'emp', '10', 'contact:mobile','8501044567'
put 'emp', '10', 'contact:mobile2','8501044566'
put 'emp', '10', 'skills:databases','oracle,hbase'
put 'emp', '10', 'other:hobbies','movies,reading'


put 'emp', '3', 'basic:empid','103' 
put 'emp', '3', 'basic:name','Aditya'
put 'emp', '3', 'basic:dob','2020-02-02'
put 'emp', '3', 'basic:email','ramesh@gmail.com'
put 'emp', '3', 'contact:mobile','8501044567'
put 'emp', '3', 'contact:mobile2','8501044566'
put 'emp', '3', 'skills:databases','oracle,hbase'
put 'emp', '3', 'other:hobbies','movies,reading'
------------------------------------------------------------


put 'emp2', '1', 'basic:empid','101' 
put 'emp2', '1', 'basic:name','Raju'
put 'emp2', '1', 'basic:age','45'
put 'emp2', '1', 'basic:email','raju@gmail.com'
put 'emp2', '1', 'contacts:mobile','8501044567'
put 'emp2', '1', 'contacts:address','blah blah'

put 'emp2', '2', 'basic:empid','102' 
put 'emp2', '2', 'basic:fname','Kanaka Raju'
put 'emp2', '2', 'basic:lname','Y'
put 'emp2', '2', 'basic:dob','2000-02-01'
put 'emp2', '2', 'basic:email','kraju@gmail.com'
put 'emp2', '2', 'contacts:mobile','8501044567'
put 'emp2', '2', 'contacts:address','blah blah'


put 'emp2', '1', 'basic:empid','10000' 
put 'emp2', '1', 'contacts:mobile','8501066666'

-----------------------------------------------------------

scan 'emp'

-----------------------------------------------------------
get 'emp', '1'

get 'emp', '1', {COLUMN => ['basic:empid', 'basic:name'], VERSIONS => 2}

get 'emp2', '1', {COLUMN => ['basic:empid', 'contacts:mobile'], VERSIONS => 2}

-------------------------------------------------------------

alter 'emp', NAME => 'other', METHOD => 'delete'  // delete a column family

alter 'emp', NAME => 'other', VERSIONS => 3  // add a column family
alter 'emp', NAME => 'basic', VERSIONS => 3  // modify the properties of existing column family

--------------------------------------------------------------

truncate 'emp2'

--------------------------------------------------------------

disable 'emp2'
drop 'emp2'

---------------------------------------------------------------

  Hive - HBase Integration
  ------------------------

   -> We can create a  table in Hive using Hbase as the storoge Handler (HbaseStorageHandler)
   -> We can map the columns of the hive table with a hbase table
   -> We can perform update, delete and random data access etc on hive table using hbase.

$ hive --auxpath /usr/lib/hive/lib/hive-hbase-handler-0.10.0-cdh4.7.0.jar,/usr/lib/hive/lib/hbase.jar,/usr/lib/hive/lib/zookeeper.jar,/usr/lib/hive/lib/guava-11.0.2.jar


   CREATE TABLE hiveempinfo (key int, empname string, salary double) 
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' 
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,emp:empname,emp:salary") 
TBLPROPERTIES ("hbase.table.name" = "hbaseempinfo");


=> In the above command:
   -> The first column (key) is mapped to hbase row-key identified by :key
   -> The second column (empname) is mapped to hbase column emp:empname, 
      where 'emp' is the CF and empname is the column.
   -> The third column (salary) is mapped to hbase column emp:salary, where 'emp' is the CF and salary is the column.
   -> The hbaseempinfo table is automatically created. 

# load data into hive (deserialized) table just created
hive> INSERT OVERWRITE TABLE hiveempinfo SELECT empid, empname, salary FROM empinfo;

NOTE: when you load data into the hive table the corresponding HBase table is also populated 
with the same data. Now, you can do DML operations in HBase and see the changes reflected in Hive.

-------------------

  
  HBase Architecture
  ------------------

   Google published three seminal papers between 2003 and 2006

    1. Google File System	: HDFS
    2. MapReduce			: MapReduce
    3. Google Big Table		: HBase   


  => HBase is a "sorted nested map": <Row-ID, <Column-Family, <Column, <TimeStamp Value>>>
  
  => When you read data from hbase, it performs a random-access lookup on the row-id
  => When you write data to hbase, hbase inserts the row in the 'right-place'
	-> These are possible with "regions"
  
  Region => Is a contiguous range of row-ids.
	    Each region is stored in a region-server.
            Serve as an index to perform fast lookup for where a row belongs to. 
	    Regions are distributed across the cluster among many region-servers
           
  Region-Server => run on all the slave nodes of hbase cluster
		   host zero or more regions
 

  With in each region-server:

      1. WAL (write-ahead-log)  - hdfs file which records all incoming data changes (writes/updates)
      2. MemStore  - one per column-family
		     in-memory buffer where all data changed are recorded
		     write-buffer
      3. HFile - periodically, memstore contents are flushed to HFiles (stored in HDFS)
      4. Block Cache - maintains an index of row-keys to HFile blocks
		       used for fast-random access.



 =====================
     Kafka
 =====================

  -> Distributed Publisher-Subscriber Messaging Engine and Streaming Data Platform.
  -> Kafka runs on a cluster and hense provide very high scalability
  -> Kafka is mainly writen in Scala (programming language)
   
   Kafka can be seen from 3 perspectives:
	-> distributed commit-log  (intermediate storage layer)
        -> pub/sub messaging system (producers can produce and consumers can consume) 
        -> distributed streaming platform (you can write streaming applications)


  Installing Kafka
  -----------------
  
	-> Download kafka binaries from the following location.
		https://kafka.apache.org/downloads

        -> Extract the tar ball to a suitbale location.

	-> Both windows and linux binaries are available in the same package.
	
    
  Kafka Building Blocks
  ---------------------

   1. Broker
	-> Each kafka service running on a node in the kafka cluster is a broker. 
	-> Every broker in a cluster has a unique broket-id
	-> Brokers communicate with a central Zookeeper cluster to maintain their state
	-> Broker store data as 'topics' (stored on disk) as commit-logs.
	
	-> Runs by default on port 9092 (configurable)

  2. Zookeeper
	-> Acts as central coordination service
        -> All brokers communicate (heartbeats) with zookeeper.
	-> A cluster is formed by having multiple brokers communicating with the same zookeeper.

  3. Topic
	-> Is a distributed sequential commit-log that stores messages of same type (like a table)
	-> Every brokers stores its topic on the disk at a disignated location
		-> log.dirs=/tmp/kafka-logs
	-> Each topic is organized as multiple partitions
        -> A message is uniquely identified by (topic, partition, offset-id)

   4. Topic Partitions
	-> Each topic is organized as multiple partitions
 		-> Partitions are distribited across multiple brokers
		-> Each broker can store 0 or more partitions
	-> Every partition has replicas (to provide fail-safety)
	-> Only one replica of each partition is designated as "leader" partition
	-> All reads/writes go only to the leader partitions
	-> If the broker containing the leader partition goes down, Zookeeper triggers an
	   election, and one of the (in-sync) replicas is elected as "leader" replica.

  5. Producer
	-> Is a client that writes data to a topic
	-> The messages produced by producer are distributed among all the partitions of the topic
	    -> The distribution logic can be controlled using custom partition.
	    -> Kafka applies its default partitioning logic in the absense of a custom partitioner.

  6. Consumer
	-> Is a client application that subscribes to a topic and polls data from the topic
	-> The consumer consumes the messages and write the offsets that are consumed to a 
	   __consumers_offsets topic
		-> This is consumer tracks what messages have been consumed.

  => Bootstrap Server: 
	Is one of the active brokers in the cluster through which a client can establish a conncetion to the entire cluster.
	NOTE: You are not connecting to any specific broker. You are connecting to cluster as a whole.
 


  Working with Kafka
  ------------------

   # Start Zookeeper
   ~~~~~~~~~~~~~~~~~~~~~~~~
   $bin/zookeeper-server-start.sh config/zookeeper.properties


   # Start a Kafka Broker
   ~~~~~~~~~~~~~~~~~~~~~~~~
   $bin/kafka-server-start.sh config/server.properties 


   # Kafka Topic Operations
   ~~~~~~~~~~~~~~~~~~~~~~~~
   $bin/kafka-topics.sh --list --bootstrap-server localhost:9092
   $bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --topic topic1 --partitions 1 --replication-factor 1
   $bin/kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic topic1
   $bin/kafka-topics.sh --delete --bootstrap-server localhost:9092 --topic topic1



  # Kafka Console Producer
  ~~~~~~~~~~~~~~~~~~~~~~~~
  $bin/kafka-console-producer.sh --topic topic1 --bootstrap-server localhost:9092

  

  $bin/kafka-console-producer.sh \
  	--bootstrap-server localhost:9092 \
  	--topic my-topic \
  	--property "parse.key=true" \
  	--property "key.separator=:"

 
  # Kafka Console Consumer
  ~~~~~~~~~~~~~~~~~~~~~~~~
  $bin/kafka-console-consumer.sh --topic topic1 --bootstrap-server localhost:9092

  # to read from the beginning of the topic 
  $bin/kafka-console-consumer.sh --topic topic1 --bootstrap-server localhost:9092 --from-beginning

  #create a consumer group using console comsumer
  $bin/kafka-console-consumer.sh --topic topic1 --bootstrap-server localhost:9092 --consumer.config config/consumer.properties



  Kafka APIs
  ----------

   1. Producer API 
	-> Used to write producer applications

   2. Consumer API
	-> Used to write consumer application

   3. Streams API
	-> Used to write streaming application that can enrich/modify events in existing topics 
           in real-time and write modified events to other topics.
        -> Kafka-to-Kafka workloads

   4. Connect API   
	-> Move data in real time between two desperate data systems in real time   



   Message distribution protocol
   -----------------------------     
    A message can have :

	-> topic   (mandatory, not data)
	-> value   (mandatory, the content of the message)
	-> key	   (optional, but mostly we have a key)
	-> partition-id  (optional)
	-> timestamp (optional, event-time)

    1. If the message contains a partition-id, then the message will be delivered to that partition.
    2. If we define a custom-partitioning logic (based on the key) then the message will be sent to 
       the partition as per the output of the custom partitioner (for that key)
    3. If you do not define a custom partitioner, then the partition will be decided based on the key
       of the message using the default hash partitioning logic.
    4. If the message has no key at all, the messages are distributed randomly in a load-balanced manner.



    Single-node Multiple Broker Kafka Setup
    ---------------------------------------   
       
			broker.id	port		log.dir
    	Broker: 0	0		9092		/tmp/kafka-logs
	Broker: 1	1		9093		/tmp/kafka-logs-1			
	Broker: 2	2		9094		/tmp/kafka-logs-2
   
	=> define the protocol, ip and port:   		listeners=PLAINTEXT://:9092
   	=> defines where kafka broker saves its logs:   log.dirs=/tmp/kafka-logs


    Consumer Groups
    ---------------

	$bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list
 	$bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-group
    	 --reset offsets for a consumer group

 	$bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group my-group --reset-offsets --to-earliest --execute --topic topic1
     		 NOTE: The possible values for --reset-offsets are:
	 	--to-datetime, --by-period, --to-earliest, --to-latest, --shift-by, --from-file & --to-current

 	$bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group my-group --reset-offsets --shift-by 2 --execute --topic topic1
 	$bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group my-group --reset-offsets --shift-by -2 --execute --topic topic1




  Create Applications usig Kafka API
  -----------------------------------

     To write external applications using Kafka API, you can use two approaches.

	1. If you have Kafka binaries downloaded, then you can include Kafka library jar files into 
	   the build path of you application.	 

	2. You can use maven projects and add the required dependencies from the central maven
           reporsitory.

		Maven:
		  -> build tool (you can add your build configurations and maven can build based
                     on your build configurations)
		  -> packgage manager (allows to you add required dependecies of your project and
                     adds them to your project by downloading them from MVNRepository)

   Kafka Producer API
   ------------------

    1. KafkaProducer -> reprsents a connection to kafka with the specified configuration.

    2. ProducerRecord -> represents one message/record
	   producer.send( new ProducerRecord(......) )

	 ProducerRecord(String topic, V value)
	 ProducerRecord(String topic, K key, V value)
	 ProducerRecord(String topic, Integer partition, K key, V value)
	 ProducerRecord(String topic, Integer partition, Long timestamp, K key, V value)


   Different ways of sending messages to Kafka
   -------------------------------------------

    1. Fire and Forget

	-> The producer does not care if the messages or sent or not successfully.
        -> The producer does not receive the acknowledgement.

        Advantages:
	  -> Very fast and can give very high producer throughput. 

        Disadvantage:
	  -> No message delivery guarantee / knowledge.

        Use Cases:
	   -> Click Stream analysis, web-log analysis.


   2. Synchronous messaging
	
	-> The producer sends a message and waits (get call) for the acknowledgement
	-> The next message will be sent only after it receives the ack for the privious message

        Advantage:
	  -> Very good message delivery guarantee
	  -> Message order can be maintained

        Disadvantages:
	   -> Slow

        Use Cases:                         
	    -> Finacial domain where each message matters and order also matters.


  3. Asynchronous messaging

	-> The producer send a message along with a callback method.
		      producer.send( 
		         new ProducerRecord<>(topicName,"Key " + i, "Java KafkaProducerAsync Value " + i), 
		    	 new KafkaProducerAsyncCallback()
		      );
	-> The messages are sent asynchronously with out blocking the send call
        -> The acknowlegments (Future<RecordMetaData>) are processed by the callback methods

         Advantages:	 
           -> Very fast as we are not blocking the send calls
           -> We do know about the status of the message - good delivery guarantee.

        Disadvantage
           -> messages can be produced out of order
   

  Important Producer Configurations
  ---------------------------------

    Mandatory Configuration Properties:
	-> bootstrap.servers
	-> key.serializer
	-> value.serializer

   1. acks :   

	The number of acknowledgements the producer requires the leader to have received before
        the request is considered complete.   (default: 1)
	
	i. acks = 0
	     -> producer will not wait for the ack. from the server at all.
             -> 'retries' config. will not take effect.		 

	ii. acks = 1 (default)
	     -> the leader writes the message to its local log, and acks without waiting for
                acks from the follower replicas.

	iii. acks = "all" (-1)
	     -> the leader will wait for all the ISRs (In-sync replicas) to ack. then only it
                send success to the producer.


   2. buffer.memory (bytes - 32 MB)
	The total memory the producer can use to buffer records waiting to sent to the server.

   3. max.block.ms (60,000)
	The max time the send() requests can be blocked, if sent speed > delivery speed, so as to
        freeup the buffer. 

   4. batch.size (16KB - 16*1024  in bytes)

   5. compression.type (default: none)
	 values: none (default), snappy, gzip, lz4

   6. retries  (default: unlimited,  Int.Max)
	 -> number fo times the client resends the message whose send fails with transient error. 
	 -> "delivery.timeout.ms" (120000 - 2 min)	

   7. linger.ms 
	 -> The number of millisecond the delivery process waits until messages are accumulated in
            the batches
         -> This applies only in those cases where the delivery speed is faster than the sent speed.     
   

   
    Kafka Consumer API
    ------------------

     -> Allows you write consumer applications that can subscribe to one or more topics or 
	one or more TopicPartitions and consume the messages. 
       
     -> A consumer can subscribe at topic-level or at topic-partition level.  


     Consumer Groups
     ---------------
	-> Allow you to distribute partitions among various consumer instances in the consumer group
           there by helps you to increase consumer side scalability.


     Consumer Rebalancing
     ---------------------

        -> Moving the ownership of a partition from one consumer to another consumer in a consumer
           group is called "rebalance"

        -> 'rebalance' is a short window of unavailability where all consumers stop consuming until 
            partition reallocation happens

  
        Rebalancing happens:
	
	1. when a new consumer joins a consumer group
	2. when an existing consumer leaves a consumer group (may be its killed, session expires)
	3. when a new partition is added to the topic (which is being consumed from)
    

      Consumer API
      ------------

	1. Create the properties objects	
	       Mandatory properties:
		  -> bootstrap.servers
		  -> key.deserializer
		  -> value.deserializer
		  -> group.id

	2. Create a KafkaConsumer object, which represents a cluster connection with the properties (defined above)
	3. Subscribe to one or more topics or topic-partitions
	4. Creata the poll loop ( infinite loop )
	5. Within the poll loop, poll the messages from the topic. 
		This returns a ConsumerRecords (collection) object with a bunch of records
	6. Loop through the ConsumerRecords and process each ConsumerRecord.
			

      Offset Commit behaviour
      -----------------------

       -> A consumer in a group commits the offset of the last message in each partition that is 
          has received to a topic called __consumer_offset	


 Important Producer Configurations
  ---------------------------------

    Mandatory Configuration Properties:
	-> bootstrap.servers
	-> key.serializer
	-> value.serializer

   1. acks :   

	The number of acknowledgements the producer requires the leader to have received before
        the request is considered complete.   (default: 1)
	
	i. acks = 0
	     -> producer will not wait for the ack. from the server at all.
             -> 'retries' config. will not take effect.		 

	ii. acks = 1 (default)
	     -> the leader writes the message to its local log, and acks without waiting for
                acks from the follower replicas.

	iii. acks = "all" (-1)
	     -> the leader will wait for all the ISRs (In-sync replicas) to ack. then only it
                send success to the producer.


   2. buffer.memory (bytes - 32 MB)
	The total memory the producer can use to buffer records waiting to sent to the server.

   3. max.block.ms (60,000)
	The max time the send() requests can be blocked, if sent speed > delivery speed, so as to
        freeup the buffer. 

   4. batch.size (16KB - 16*1024  in bytes)

   5. compression.type (default: none)
	 values: none (default), snappy, gzip, lz4

   6. retries  (default: unlimited,  Int.Max)
	 -> number fo times the client resends the message whose send fails with transient error. 
	 -> "delivery.timeout.ms" (120000 - 2 min)	

   7. linger.ms 
	 -> The number of millisecond the delivery process waits until messages are accumulated in
            the batches
         -> This applies only in those cases where the delivery speed is faster than the sent speed.     
   

   
    Kafka Consumer API
    ------------------

     -> Allows you write consumer applications that can subscribe to one or more topics or 
	one or more TopicPartitions and consume the messages. 
       
     -> A consumer can subscribe at topic-level or at topic-partition level.  


     Consumer Groups
     ---------------
	-> Allow you to distribute partitions among various consumer instances in the consumer group
           there by helps you to increase consumer side scalability.


     Consumer Rebalancing
     ---------------------

        -> Moving the ownership of a partition from one consumer to another consumer in a consumer
           group is called "rebalance"

        -> 'rebalance' is a short window of unavailability where all consumers stop consuming until 
            partition reallocation happens

  
        Rebalancing happens:
	
	1. when a new consumer joins a consumer group
	2. when an existing consumer leaves a consumer group (may be its killed, session expires)
	3. when a new partition is added to the topic (which is being consumed from)
    

      Consumer API
      ------------

	1. Create the properties objects	
	       Mandatory properties:
		  -> bootstrap.servers
		  -> key.deserializer
		  -> value.deserializer
		  -> group.id

	2. Create a KafkaConsumer object, which represents a cluster connection with the properties (defined above)
	3. Subscribe to one or more topics or topic-partitions
	4. Creata the poll loop ( infinite loop )
	5. Within the poll loop, poll the messages from the topic. 
		This returns a ConsumerRecords (collection) object with a bunch of records
	6. Loop through the ConsumerRecords and process each ConsumerRecord.
			

      Offset Commit behaviour
      -----------------------

       -> A consumer in a group commits the offset of the last message in each partition that is 
          has received to a topic called __consumer_offset	


      Kafka delivery semantics for consumers
      --------------------------------------

       1. At-most-once    
	 
   	    -> every message is processed maximum once. some messages may not be processed at all.

	   -> The consumer (in the group) immediatly commits the offset after it receives the messages
              (even before the messages are processed).


      2. At-least-once (default)
	
	   -> every message is processed at least once. Some messages may be processed more than
	      once resulting in duplicate message processing. Here you have to ensure that your
	      consumer is "idempotent".
	
	   -> The consumer (in the group) commits the offset after all the messages are processed 
              and before fetching the next batch of records.
                	

      3. Exactly-once
	
	   -> Not supported in Kafka => Sink workflows (using Consumer API)
	   -> This is only supported in Kafka => Kafka workflows using Streams API.


   Consumer Configurations
   -----------------------

	Mandatory Properties:
	
	1. bootstrap.servers
	2. key.deserializer
	3. value.deserializer
	4. group.id


      Other Properties
      ----------------
	
	1. fetch.min.bytes   	(def: 1 byte)

	2. fetch.max.wait.ms 	(def: 500)
 
	3. fetch.max.bytes 	(def: 50MB,  50*1024*1024)
		-> max data returned for each fetch request (from all partitions)
		   in each poll request.

	4. max.partitions.fetch.bytes (def: 1 MB)
		-> max data to be returned per partition

	5. max.poll.records (def: 500)
		-> In each poll req. how many max messages you want to receive

	6. enable.auto.commit  (def: true)
		-> Kafka automatically commits the starting offset of the messages polled
		   by a consumer
		-> Follows 'at-least-once' deliver semantics.

	7. auto.offset.reset   (def: latest)
		-> possible values: latest, earliest, none
		-> Controls from which offset the consumer should read in the event that the
		   'current-offset' is not set or the available 'current-offset' is invalid. 

	8. heartbeat.intervel.ms (def: 3000,  3 sec)

	9. session.timeout.ms  (def: 10000, 10 sec)

       10. allow.auto.create.topics (def: true)


  =========================================
     Spark (PySpark)
  =========================================

     Spark is a unified in-memory distributed (run on a cluster) computing framework.    

	-> Spark is 100x faster than MR if you use full in-memory RDD persistence
	-> Spark is 6-7x faster than MR if you use disk based computation.

     -> Spark is written in Scala.
     -> Spark supports Scala, Java, Python & R programming languages. 
     -> Spark APIs
		-> Spark Low level API (Spark Core API)
		-> Spark SQL
		-> Spark Streaming
		-> Spark MLlib
		-> Spark Graphx
     -> Spark programs can be executed on various cluster managers
		-> Spark Standalone Scheduler (Spark's own cluster manager)
		-> YARN
		-> Mesos
		-> Kubernetesis
     
    Spark's Unified Framework
    -------------------------

     -> Spark provides a consistent set of APIs built on core data abstractions to process
	different analytics workloads using the same execution engine.
	
       Hadoop EcoSystem (excluding Spark)
       ----------------------------------
	Batch processing of unstructured data	 => MapReduce
	Batch processing of structured data	 => Hive, Impala, Drill, Hbase	
	Streaming data analytics		 => Kafka, Storm
	Machine Learning (Predictive Analytics)	 => Mahout
	Graph Parallel Computations		 => Giraph

       Spark
       ----------------------------------
	Batch processing of unstructured data	 => Spark Core (RDDs)
	Batch processing of structured data	 => Spark SQL (DataFrames)
	Streaming data analytics		 => Spark Streaming, Structured Streaming
	Machine Learning (Predictive Analytics)	 => Spark MLlib
	Graph Parallel Computations		 => Spark GraphX



   Getting started with Spark on Databricks
   -----------------------------------------   
   ** Databricks Community Edition (free edition)
 		
	Signup: https://www.databricks.com/try-databricks
		Screen 1: Fill up the details with valid email address.
		Screen 2: Click on "Get started with Community Edition" link (Not the 'Continue' button).

	Login: https://community.cloud.databricks.com/login.html

	Downloading a file from Databricks
	----------------------------------
		/FileStore/<FILEPATH>
		https://community.cloud.databricks.com/files/<FILEPATH>?o=1072576993312365

		Example:
		dbfs:/FileStore/output/wc/part-00000
		https://community.cloud.databricks.com/files/output/wc/part-00000?o=1072576993312365


	Enabling DBFS File browser
	--------------------------
	<your account (top-right)> -> Admin settings -> workspace settings -> advanced -> DBFS File Browser (enable it)


   Spark Architecture
   ------------------

    	1. Cluster Manager (CM)
		-> Applications are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the 'SparkContext' object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		-> Where to run the driver process
		1. Client : default, driver runs on the client. 
		2. Cluster: driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks run the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver. 
 

  RDD (Resilient Distributed Dataset)
   -----------------------------------
	-> RDD is the fundamental data abstraction of Spark

	-> RDD is a collection of distributed in-memory partitions.
	    -> Each partition is a collection of objects of some type.

	-> RDDs are immutable

	-> RDDs are lazily evaluated
	   -> Transformations does not cause execution.
	   -> Action commands trigger execution.

   Creating RDDs
   -------------
	Three ways:

	1. Creating an RDD from external data file

		rdd1 = sc.textFile(<dataPath>, 4)

		default Number of partitions: sc.defaultMinPartition
		  sc.defaultMinPartition = 2, if number of cores >= 2
					   1, otherwise

	2. Creating an RDD from programmatic data

		rdd1 = sc.parallelize([2,1,3,2,4,3,5,4,6,7,5,6,7,6,8,8,9,0], 2)

		default Number of partitions: sc.defaultParallelism
		sc.defaultParallelism = number of CPU cores allocated.


	3. By applying transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x*10)


   RDD Operations
   --------------
    Two types of operations

	1. Transformations
		-> Transformations return RDDs
		-> Transformations does not cause execution of RDDs
		-> Cause lineage DAGs to be created

	2. Actions
		-> Triggers execution of RDDs
		-> Produces output by sending a Job to the cluster


   RDD Lineage DAG
   ---------------  
    Driver maintains a Lineage DAG for every RDD.
    Lineage DAG is a heirarchy of dependencies of RDDs all the way starting from the very first RDD.	
    Lineage DAG is a logical plan on how to create the RDD.

	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	rddFile Lineage DAG: (4) rddFile -> sc.textFile on E:\\Spark\\wordcount.txt
	
	rddWords = rddFile.flatMap(lambda x: x.split())
	rddWords Lineage DAG: (4) rddWords -> rddFile.flatMap -> sc.textFile on E:\\Spark\\wordcount.txt

	rddPairs = rddWords.map(lambda x: (x,1))
	rddPairs Lineage DAG: (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	rddWc Lineage DAG: (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile


  Types of RDD Transformations
  ----------------------------

    Two types:

	 1. Narrow Transformations
           -> Narrow transformations are those, where the computation of each partition depends ONLY
              on its input partition.
           -> There is no shuffling of data.
           -> Simple and efficient

      	2. Wide Transformations
           -> In wide transformations, the computation of a single partition depends on many
              partitions of its input RDD.
           -> Data shuffle across partitions will happen.
           -> Complex and expensive


  RDD DAG Scheduling
  ------------------
	Application (represented by a Spark context)
	|
	|=> Jobs (each action command launches a job)
	     |
	     |=> Stages (each wide transformations causes a new stage)
		  |
		  |=> Tasks (set of transformations that can run in parallel)
			|
			|=> Transformations


  RDD Persistence
  ---------------

      	rdd1 = sc.textFile(<file>, 4)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )   -------> instruction to spark to save rdd6 partitions
	rdd7 = rdd6.t7(...)

	rdd6.collect() 
	DAG of rdd6:  (4) rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		tasks: [sc.textFile, t3, t5, t6] -> collected

	rdd7.collect()
	DAG of rdd7:  (4) rdd7 -> rdd6.t7
		tasks: [t7] -> collected

	rdd6.unpersist()


	Storage Levels
        --------------
	MEMORY_ONLY		=> default, Memory Serialized 1x replicated
	MEMORY_AND_DISK		=> Disk Memory Serialized 1x replicated
	DISK_ONLY		=> Disk Serialized 1x replicated
	MEMORY_ONLY_2		=> Memory Serialized 2x replicated	
	MEMORY_AND_DISK_2	=> Disk Memory Serialized 2x replicated	

	Commands
	--------
	rdd1.cache()   -> in-memory persistence
	rdd1.persist() -> in-memory persistence
	rdd1.persist( StorageLevel.DISK_ONLY)
	
	rdd1.unpersist()

	
   Spark Executor Memory Structure
   -------------------------------     

      Let us say we are requesting an executor with 10 GB RAM. 
      The spark job will be allocated executors with 10.3 GB (10GB + 300MB) RAM. 
      
      1. Reserved Memory (300 MB)
           -> Spark's internal usage
      
      2. Spark Memory (spark.memory.fraction: 0.6)   => 6 GB (Unified Memory)
          
         2.1  Execution Memory
                 -> Used for RDD partition creation and transformation
                 -> Can forcibly evict RDD partitions from storage memory if it requires
                    additional memory upto the quota allocated for it.

         2.2  Storage Memory (spark.memory.storageFraction: 0.5) => 3 GB
                 -> The RDD partitions and broadcast variables are persisted
                    in this memory.

      3. User Memory  => 4 GB
         -> Running non-spark related code execution. 
         -> Related to running python methods, storing python collection.



  RDD Transformations
  -------------------

  1. map		P: U -> V
			Object to object transformation
			Input RDD: N objects, Output RDD: N objects					

  		rdd2 = rddFile.map(lambda x : x.split())



  2. filter		P: U -> Boolean
			Filters the objects based on the function
			Input RDD: N objects, Output RDD: <= N objects	

		rddFile.filter(lambda x: len(x.split()) > 8 ).collect()


  3. glom		P: None
			Return one list object per partition with all the objects of the partition
			Input RDD: N objects, Output RDD: number of objects = number of partitions

		rdd1	    	rdd2 = rdd1.glom()

		P0: 3,4,2,4,6 -> glom -> P0: [3,4,2,4,6]
		P1: 4,6,7,8,6 -> glom -> P1: [4,6,7,8,6]
		P2: 5,0,7,9,1 -> glom -> P2: [5,0,7,9,1]

		rdd1.count() = 15 (int)   rdd2.count() = 3 (list)



  4. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation

		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p) ).collect()
		rdd1.mapPartitions(lambda p: [sum(p)] ).collect()


  5. flatMap		P: U -> Iterable[V]
			flattens the iterables generated by the function
 			Input RDD: N objects, Output RDD: >= N objects	

		rddWords = rddFile.flatMap(lambda x: x.split())


  6. mapValues		P: U => V
			Applied only on Pair RDDs
			Applies the function only to the 'value' part of the key-value pairs

		rddPairs.mapValues(lambda x: x*10).collect()
		-> In the above fn, x represents only the 'value' part of key-value pairs


   7. distinct		P: None, Optional: numPartitions
			Returns distinct objects of the RDD.

			rddWords.distinct().collect()


    8. sortBy		P: U -> V, Optional: ascending (True/False), numPartitions
			Sorts the RDDs based on the function output.

		rdd1.sortBy(lambda x: x%3).glom().collect()
		rdd1.sortBy(lambda x: x%3, False).glom().collect()
		rdd1.sortBy(lambda x: x%3, True, 2).glom().collect()	 


    9. groupBy		P: U -> V, Optional: numPartitions

			Returns a Pair-RDD where:
			    key: Each unique value of the function output
		            value: ResultIterable. Grouped objects of the RDD that produced the key 
  

    10. partitionBy	P: numPartitions, Optional: partition-function (default: hash)
			Applied only on Pair RDDs
			Controls which keys go to which partition based on partition-function applied to keys.
           
  		rdd4 = rdd1.map(lambda x: (x, 1)).partitionBy(3).map(lambda x: x[0])


  ..ByKey Transformations
  -----------------------
	=> Are wide transformations
	=> Applied on Pair RDDs only


    11. sortByKey	P: None, Optional: ascending (True/False), numPartitions
			Sorts the data based on the keys.

			rddPairs.sortByKey().glom().collect()
			rddPairs.sortByKey(False).glom().collect()
			rddPairs.sortByKey(False, 3).glom().collect()	


    12. groupByKey	P: None, Optional: numPartitions
			Returns a Pair RDD where:
				key: Unique keys of the RDD
				value: ResultIterable. Grouped values

			CAUTION: avoid groupByKey if possible. 

		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 1) \
        		.flatMap(lambda x: x.split()) \
        		.map(lambda x: (x, 1)) \
        		.groupByKey() \
        		.mapValues(sum) \
        		.sortBy(lambda x: x[1], False) 


  13. reduceByKey	P: (U, U) -> U,  Optional: numPartitions
			Reduce all the values of each unique key by iterativly applying the reduce function - first, on
			each partition, and then, across the reduced values of all partitions. 
			
		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 1) \
        		.flatMap(lambda x: x.split()) \
        		.map(lambda x: (x, 1)) \
        		.reduceByKey(lambda x, y: x + y) \
        		.sortBy(lambda x: x[1], False)


  14. repartition	P: numPartitions
			Is used to increase or decrease the number of output partitions	
			Global shuffle	
			
		rdd2 = rdd1.repartition(5)


  15. coalesce		P: numPartitions
			Is used to only decrease the number of output partitions	
			Partition merging

		rdd2 = rdd1.coalesce(5)

		Recommendations
		---------------
		-> The size of each partition should be between 100 MB to 1 GB
		   (Ideally 128 MB if you are running on Hadoop)
		-> The number of partitions should be a multiple of number of cores
		-> The number of cores per executor should be 5


  RDD Actions
  ------------

  1. collect

  2. count

  3. saveAsTextFile

  4. reduce		P: (U, U) -> U
			Reduces an entire RDD to one object by iterativly applying the function - first, on
			each partition, and then, across the reduced values of all partitions. 

		rdd1		
		P0: 3,2,1,7,5 -> reduce -> -12 -> reduce -> 6
		P1: 6,8,4,2,1 -> reduce -> -9
		P2: 9,14,3,1,0 -> reduce -> -9

		rdd1.reduce( lambda x,y:x-y )  -> 


  5. take(n)

		rdd1.take(10)  -> returns a list of first 10 objects


  6. takeOrdered(n, [fn])

		rddWords.takeOrdered(20)
		rddWords.takeOrdered(20, len)

  7. takeSample(withReplacement, n, [seed])

		with-replacement sampling
			rdd1.takeSample(True, 10)
			rdd1.takeSample(True, 10, 45645)    # 45645 is a seed

		without-replacement sampling
			rdd1.takeSample(False, 10)
			rdd1.takeSample(False, 10, 45645)   # 45645 is a seed

  8. countByValue


  9. countByKey


  10. foreach => P: function;
		  Returns nothing. 
		  Executes the function on all objects of the RDD.


  11. saveAsSequenceFile

	rddWc.saveAsSequenceFile("/FileStore/output/seq")



  Spark Closures
  --------------

	In Spark, a closure constitutes all the variables and methods which must be visible 
	for the executor to perform its computations on the RDD. 

	=> This closure is serialized and a separate copy is sent to each executor (by the driver).


	c = 0

	def isPrime(n):
	   return True if n is Prime
	   else return False

	def f1(n):
	   global c
	   if (isPrime(n)) c += 1
	   return n*2   

	rdd1 = sc.parallelize( range(1, 4001), 4 )

	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(c)      // 0   

	
	Limitation: We can not use local variables to implement global counters. 
	Solution: Use 'Accumulator' variables


  Shared Variables
  ----------------

  1. Accumulator variable

	-> Is a shared variable, not part of a closure
	-> Not a local copy
	-> One variable maintained by the driver
	-> All tasks can add to this variable (the driver copy is updated)
	-> Is used to implement "global counter"	


	c = sc.accumulator(0)

	def isPrime(n):
	   return True if n is Prime
	   else return False

	def f1(n):
	   global c
	   if (isPrime(n)) c.add(1)
	   return n*2   

	rdd1 = sc.parallelize( range(1, 4001), 4 )

	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(c)  


  2. Broadcast variable

	-> Is a shared variable that is not part of the closure
	-> Variable is broadcasted to each executor node.
	-> All tasks in that executor can read from that copy.	

	d = sc.broadcast({ 1:a, 2:b, 3:c, 4:d, 5:e, 6:f, 7:g, ..... })   # 100 MB
	
	def f1(k):
	   global d
	   return d.value[k]

	rdd1 = sc.parallelize( range(0, 4001), 4 )	
	rdd2 = rdd1.map( f1 )
	rdd2.collect() 



  spark-submit
  ============

	Is a single command to submit any spark application (scala, java, python, R) to any cluster
	manager (local, YARN, Mesos, Kubernetes)

       spark-submit [options] <app jar | python file | R file> [app arguments]
	
       spark-submit --master yarn
		--deploy-mode cluster
		--driver-memory 2G
		--driver-cores 2
		--executory-memory 5G
	        --executor-cores 5
		--num-executors 10
		E:\\Spark\\wordcount.py [app args]


   Case Study
   ===========		
	dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

	From cars.tsv dataset, get the average weight of all the models of each make of American origin cars. 
	-> Arrange in the DESC order of average weight. 
        -> Save the output as a single text file. 
		
	=> Try it yourself	


  ================================
     Spark SQL (pyspark.sql)
  ================================ 

    -> High Level API built on top of Spark Core

	File Formats : Parquet (default), ORC, JSON, CSV (delimited text), Text
	JDBC Format  : RDBMS, NoSQL
	Hive Format  : Hive Warehouse

   SparkSession
   ------------

	-> Starting point of execution
	-> Represents a user session (SparkSession) running inside an application (SparkContext)
	-> Each SparkSession can have its own configuration

	spark = SparkSession \
    		.builder \
    		.appName("Basic Dataframe Operations") \
    		.config("spark.master", "local") \
    		.getOrCreate()   


   DataFrame (DF)
   --------------
	-> Main data abstraction of Spark SQL
	-> Is a collection of distributed in-memory partitions
	-> Immutable
	-> Lazily evaluated

	-> DataFrame is a collection of "Row" objects.

	-> DataFrame contains two components:
		-> Data    : Collection of 'Row' objects
		-> Schema  : StructType object

			StructType(
			   [
				StructField('age', LongType(), True), 
				StructField('gender', StringType(), True), 
				StructField('name', StringType(), True), 
				StructField('phone', StringType(), True), 
				StructField('userid', LongType(), True)
			   ]
			)

   Basic steps in creating a Spark SQL Application
   -----------------------------------------------

	1. Read/load data from some data-source into a DataFrame 		

		#df1 = spark.read.format("json").load(inputData)
		#df1 = spark.read.load(inputData, format="json")
		df1 = spark.read.json(inputData)

	2. Transform the DF using DF transformation methods or using SQL


	        Using DF transformation methods
		-------------------------------	
		 df2 = df1.select("userid", "name", "age", "gender", "phone") \
        		.where("age is not null") \
        		.orderBy("gender", "age") \
        		.groupBy("age").count() \
        		.limit(4)


		Using SQL
		---------		
		df1.createOrReplaceTempView("users")
		spark.catalog.listTables()

		qry = """select age, count(*) as count
			from users
			where age is not null
			group by age
			order by age
			limit 4"""
					
		df3 = spark.sql(qry)
		df3.show()

		NOTE: You can drop temp-view:  
		      -> spark.catalog.dropTempView("users")


	3. Save the dataframe into some external destination (such as files/databases/hive etc)

		df2.write.format("json").save("/FileStore/output/json")
		df2.write.save("/FileStore/output/json", format="json")
		df2.write.json("/FileStore/output/json")


  Save Modes
  ----------
    -> Control the behaviour when saving a DF into an existing directory.

	1. errorIfExists (default)
	2. ignore
	3. append
	4. overwrite

	df3.write.mode("overwrite").json(outputPath)
	df3.write.json(outputPath, mode="overwrite")


  LocalTempView & GlobalTempView
  -------------------------------	
	LocalTempView 
	   -> Local to a specific SparkSession
	   -> Created using createOrReplaceTempView command

		df1.createOrReplaceTempView("users")

	GlobalTempView
	   -> Can be accessed from multiple SparkSessions within the application
	   -> Tied to "global_temp" database
	   -> Created using createOrReplaceGlobalTempView command

		df1.createOrReplaceGlobalTempView ("gusers")


	select age, count(1) as count
	from global_temp.gusers
	where age is not null
	group by age
	order by age
	limit 4


  DataFrame Transformations
  -------------------------

  1. select

	df2 = df1.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME", "count")


	df2 = df1.select( col("ORIGIN_COUNTRY_NAME").alias("origin"),
                  col("DEST_COUNTRY_NAME").alias("destination"),
                  expr("count").cast("int"),
                  expr("count + 10 as newCount"),
                  expr("count > 200 as highFrequecy"),
                  expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic"))


  2. where / filter

	df3 = df2.where("domestic = false and count > 1000")
	df3 = df2.filter("domestic = false and count > 1000")

	df3 = df2.filter( col("count") > 1000 )


  3. orderBy / sort

	df3 = df2.orderBy("count", "origin")
	df3 = df2.orderBy(desc("count"), asc("origin"))
	df3 = df2.sort(desc("count"), asc("origin"))

  

  4. groupBy  => returns a pyspark.sql.group.GroupedData object. Use an aggregation function to return a DataFrame.
	
	df3 = df2.groupBy("highFrequecy", "domestic").count()
	df3 = df2.groupBy("highFrequecy", "domestic").sum("count")
	df3 = df2.groupBy("highFrequecy", "domestic").max("count")
	df3 = df2.groupBy("highFrequecy", "domestic").avg("count")

	df3 = df2.groupBy("highFrequecy", "domestic") \
        	.agg( count("count").alias("count"),
              		sum("count").alias("sum"),
              		max("count").alias("max"),
              		round(avg("count"), 2).alias("avg")
            	  )

   5. limit	

	df2 = df1.limit(10)


   6. selectExpr

	df2 = df1.selectExpr("DEST_COUNTRY_NAME as destination", 
                     "ORIGIN_COUNTRY_NAME as origin",
                     "count",
                     "count + 10 as newCount",
                     "count > 200 as highFrequency",
                     "DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic")

	df2.show()
	df2.printSchema()


  7. withColumn & withColumnRenamed

	df3 = df1.withColumn("newCount", expr("count + 10")) \
         	.withColumn("highFrequency", expr("count > 200")) \
         	.withColumn("domestic", expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME")) \
         	.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
         	.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin") \
            	.withColumn("countryCode", lit(91))

	df3.show(5)
        ---------------------

	df4 = df3.withColumn("ageGroup", when(expr("age < 13"), "child")
                                .when(expr("age < 20"), "teenager")
                                .when(expr("age < 60"), "adult")
                                .otherwise("senior") )

	df4.show()


  8.  udf (user defined function)

		def getAgeGroup( age ):
			if (age <= 12):
				return "child"
			elif (age >= 13 and age <= 19):
				return "teenager"
			elif (age >= 20 and age < 60):
				return "adult"
			else:
				return "senior"

		get_age_group = udf(getAgeGroup, StringType())

		df4 = df3.withColumn("ageGroup", get_age_group(col("age")) )

		df4.show()

		----------------------------------------

		@udf (returnType = StringType())
		def getAgeGroup( age ):
			if (age <= 12):
				return "child"
			elif (age >= 13 and age <= 19):
				return "teenager"
			elif (age >= 20 and age < 60):
				return "adult"
			else:
				return "senior"

		df4 = df3.withColumn("ageGroup", getAgeGroup(col("age")) )
	
		--------------------------------------------
		# applying UDF in SQL

		def getAgeGroup( age ):
			if (age <= 12):
				return "child"
			elif (age >= 13 and age <= 19):
				return "teenager"
			elif (age >= 20 and age < 60):
				return "adult"
			else:
				return "senior"


		spark.udf.register("get_age_group", getAgeGroup, StringType())
		
		spark.catalog.listFunctions()

		qry = "select id, name, age, get_age_group(age) as ageGroup from users"

		df5 = spark.sql(qry)
		df5.show()

   9. drop	=> drops/excludes the specified columns


		df3 = df2.drop("newCount", "highFrequency")

		df3.printSchema()
		df3.show(4)


   10.  dropna	=> drops the Rows that has null values in any column or specified columns
	fillna  => fills with specified values of Rows that has null values in any column or specified columns

		usersDf = spark.read.json("E:\\PySpark\\data\\users.json")
		usersDf.show()

		df3 = usersDf.dropna()  # drop rows if there is null in any column
		df3 = usersDf.dropna(subset=["phone", "age"])  # drop rows if there is null in phone or age columns

		df3.show()

		-------------------------
		userDf2 = usersDf \
    		.fillna("0000000000", subset=["phone"]) \
    		.fillna("ANONYMOUS", subset=["name"]) \
    		.fillna(0)


   11. dropDuplicates => drop duplicates based on any column or specified columns


		listUsers = [(1, "Raju", 5),
					 (1, "Raju", 5),
					 (3, "Raju", 5),
					 (4, "Raghu", 35),
					 (4, "Raghu", 35),
					 (6, "Raghu", 35),
					 (7, "Ravi", 70)]

		df3 = spark.createDataFrame(listUsers, ["id", "name", "age"])
		df3.show()

		df4 = df3.dropDuplicates()
		df4 = df3.dropDuplicates(["name","age"])

		df4.show()


   12. distinct => returns distinct rows

		listUsers = [(1, "Raju", 5),
			(1, "Raju", 5),
			(3, "Raju", 5),
			(4, "Raghu", 35),
			(4, "Raghu", 35),
			(6, "Raghu", 35),
			(7, "Ravi", 35)]


		df3 = spark.createDataFrame(listUsers, ["id", "name", "age"])
		df3.show()

		df4 = df3.distinct()
		df4.show() 


      Q. How many unique DEST_COUNTRY_NAME values are there in df1??	

		df1.select("DEST_COUNTRY_NAME").distinct().count()
		df1.dropDuplicates(["DEST_COUNTRY_NAME"]).count()


   13. randomSplit

		dfList = df1.randomSplit([0.5, 0.5], 464)
		print( dfList[0].count(), dfList[1].count() )

   14. sample
   
		df2 = df1.sample(True, 0.65)		# with-replacement sampling
		df2 = df1.sample(True, 1.65)		# fraction > 1 is allowed
		df2 = df1.sample(True, 0.65, 42342)	# 42342 is a seed

		df2 = df1.sample(False, 0.65)		# without-replacement sampling
		df2 = df1.sample(False, 1.65)		# ERROR: fraction > 1 is NOT allowed
		df2 = df1.sample(False, 0.65, 42342)    # 42342 is a seed


   15. union, intersect, subtract

		df4 = df2.union(df3)
		df4.show()
		df4.count()
		df4.rdd.getNumPartitions()
		df4.write.mode("overwrite").csv("E:\\PySpark\\output\\union")

		df5 = df4.intersect(df3)
		df5.show()
		df5.rdd.getNumPartitions()

		df6 = df4.subtract(df3)
		df6.show()
		df6.rdd.getNumPartitions()

  16. repartition

		df2 = df1.repartition(6)
		df2.rdd.getNumPartitions()

		df3 = df2.repartition(3)
		df3.rdd.getNumPartitions()

		df4 = df2.repartition(4, col("DEST_COUNTRY_NAME"))
		df4.rdd.getNumPartitions()

		df5 = df2.repartition(col("DEST_COUNTRY_NAME"))
		df5.rdd.getNumPartitions()

  17. coalesce

		df6 = df2.coalesce(10)  # df2 - 6 partitions
		df6.rdd.getNumPartitions()


  18. join   --> discussed as a separated topic.



  Working with different file formats
  -----------------------------------

  JSON
	read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)
		df1 = spark.read.json(inputPath, multiLine=True)

	write
		df3.write.format("json").save(outputPath)
		df3.write.save(outputPath, format="json")
		df3.write.json(outputPath)	

  Parquet (default)
	read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.load(inputPath, format="parquet")
		df1 = spark.read.parquet(inputPath)

	write
		df3.write.format("parquet").save(outputPath)
		df3.write.save(outputPath, format="parquet")
		df3.write.parquet(outputPath)


  ORC
	read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.load(inputPath, format="orc")
		df1 = spark.read.orc(inputPath)

	write
		df3.write.format("orc").save(outputPath)
		df3.write.save(outputPath, format="orc")
		df3.write.orc(outputPath)	


  CSV (delimited text)

	read
		df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputPath)
		df1 = spark.read.format("csv").load(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")

		my_schema = StructType(
    			[
        			StructField('ORIGIN_COUNTRY_NAME', StringType(), True), 
        			StructField('DEST_COUNTRY_NAME', StringType(), True), 
        			StructField('count', IntegerType(), True)
    			]
		)

		# use Apache Hive types
		my_schema = "ORIGIN_COUNTRY_NAME STRING, DEST_COUNTRY_NAME STRING, count INT"

		// using programmatic schema
		df1 = spark.read.schema(my_schema).csv(inputPath, header=True)

	write
		df3.write.format("csv").save(outputPath, header=True)
		df2.write.csv(outputPath, header=True)
		df2.write.csv(outputPath, header=True, sep="|", mode="overwrite")


  Text
	read
		df1 = spark.read.text(inputPath)

	write
		df1.write.text(outputPath)






  Creating an RDD from DataFrame
  ------------------------------
	rdd1 = df1.rdd


  Creating a DataFrame from an RDD
  --------------------------------
	listUsers = [(1, "Raju", 5),
		(2, "Ramesh", 15),
		(3, "Rajesh", 18),
		(4, "Raghu", 35),
		(5, "Ramya", 25),
		(6, "Radhika", 35),
		(7, "Ravi", 70)]

	rdd1 = spark.sparkContext.parallelize(listUsers)

	df1 = rdd1.toDF(["id", "name", "age"])

	df1.show()
	df1.printSchema()


  Creating a DataFrame from programmatic data
  --------------------------------------------
	listUsers = [(1, "Raju", 5),
		(2, "Ramesh", 15),
		(3, "Rajesh", 18),
		(4, "Raghu", 35),
		(5, "Ramya", 25),
		(6, "Radhika", 35),
		(7, "Ravi", 70)]

	df1 = spark.createDataFrame(listUsers, ["id", "name", "age"])
	df1 = spark.createDataFrame(listUsers).toDF("id", "name", "age")

  
  Create a DataFrame with programmatic schema
  -------------------------------------------

	mySchema = "id INT, name STRING, age INT"
	df1 = spark.createDataFrame(listUsers, schema=mySchema)

	------------------------

	mySchema = StructType([
            StructField("id", IntegerType(), True),
            StructField("name", StringType(), True),
            StructField("age", IntegerType(), True)
        ])
	df1 = spark.createDataFrame(listUsers, schema=mySchema)
   
	--------------------

	mySchema = StructType([
            StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
            StructField("DEST_COUNTRY_NAME", StringType(), True),
            StructField("count", IntegerType(), True)
        ])

	df1 = spark.read.schema(mySchema).json(inputPath)	


	 