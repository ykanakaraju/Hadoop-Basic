
  Agenda  (20 sessions of 4 hours each)
  -------------------------------------
    	Understanding Big data
    	Hadoop - basics
    	HDFS - Architecture & File Operations
	YARN - Cluster Manager
	MapReduce 
	Hive - Data Warehousing
	Impala (introduction)	
	Sqoop (structured data ingestion tool)
	HBase ( NoSQL database )
	Kafka (pubsub messaging engine)	
	NiFi (Workflow Scheduler)
	Spark (Python) - PySpark   
 ----------------------------------------------------------

   What is Big Data ?
   ------------------
  
    Refers to data that is large and complex data that is difficult to store and process
    using traditional data management tools or data processing technologies.

    Big Data Challenges:
	-> Storing huge amount of data
	-> Processing data faster
	-> Processing complex data

    What qualities of data make some data as Big Data ?   
	-> Volume  (10 TBs or PB scala data stores)
	-> Velocity (real time data processing)
	-> Variety (unstructured & semi-structured data)
	-> Veracity (data in doubt)
	-> Value (deep analytics)

   => Hadoop provides a solution to big data.


   What is a Cluster?
   ------------------		
        -> Is a unified entity comprising og many nodes whise cumulative resources (disk space, RAM, 
           CPU cores etc) can be used to distribute your storage and processing. 

  
   What is Hadoop ?
   ----------------
	Is an opensource framework for "storing" and "processing" big data on 
	"computing clusters" made of commodity machines. 

         1. HDFS   -> Hadoop Distributed File System
		   -> Distributed Storage Solution
		   -> Allows data of a single file to be distributed across many machines.
	
	 2. MapReduce -> Distributed Processing Framework
		    -> Used to distribute processing across many nodes in a cluster

	 3. YARN    -> Introduced in Hadoop 2 as a dedicated Resource Management Framework.


   
   HDFS (Hadoop Distributed File System)
   -------------------------------------	

     -> Is a distributed storage framework

     -> Stores the data as blocks of 128 MB each.
 
      1. Name Node (NN)
		-> Is a master daemon (continuously running background job)
		-> Is a Java background process
		-> Receives "Block reports" from all the DNs
		-> The meta-data of the entire file system name space is maintained using 2 files.

		   1. FSImage
			-> Contains the state of the entire cluster from the boot time upto
			   the last check point

		   2. EditLog
			-> Contains current data (on going changes to the cluster) from the last check
			   point upto the current moment. 

      2. Data Node (DN)
		-> Is a slave daemon
		-> Runs on all the slave machines
		-> DNs send heart-beats to NNs once every 3 sec.
		   -> if three successive heart-beats are missed, NN marks the DN as dead. 
		-> DNs send "block report" to NN
		   -> 'Block Report' contains such information as:
			-> Total Disk Space
			-> Available Space
			-> Reserved Space
			-> List of all the files, blocks of the file, owner of the file, permissions,
			   Replication-factor, etc
			-> If it is a dircetory, owner, timestamp etc.
	
      3. Secondary Name Node (SNN)
		-> Is a master daemon	
	
      4. Balancer
	       -> Is a daemon that runs on the master (same machine as NN)
	       -> Maintains the replication-factor of the HDFS files.	


   Hadoop EcoSystem
   ----------------
	-> A set of tools that are commonly used for various data analytics, data flow,
           data ingestion use-cases on top of Hadoop.	   

    
   Hadoop Distributions
   --------------------
	-> Third-party Hadoop Distributions help you to install many of the commonly used
	   components of Hadoop very easily.

	-> They will provide cluster-access or image-copies (pre-intslled hard-disk files)
	   containing all the hadoop components pre-installed.

	Hardware Requirements:	
	  -> Minimum 8 GB for CDH 4.7 (old and light-weight)
	  -> Minimum 12 GB to 16 GB RAM is required for CDH 5.13 and above.
	
	On-Premesis Installation
	------------------------
	-> Cloudera
	-> HortonWorks (merged with cloudera)
	-> MapR

	Cloud Hadoop Clusters
	---------------------
	-> Amazon: EMR (Elastic MapReduce - AWS)
	-> Microsoft: HDInsights (Azure)
	-> Google: DataProc (GCP)
		

   Setup Hadoop On your Local Machine
   ----------------------------------
     -> You can use Cloudera "ImageCopy" (CDH) as a virtual machine.
	
	https://docs.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh_download.html

      -> Get any one CDH Quickstrat version.

      -> Download Oracle VirtualBox from https://www.virtualbox.org/wiki/Downloads
	 (click on Windows Hosts)
	
      -> Follow the steps mentoned in the document shared with you.


   HDFS File Operations
   --------------------
     
      Linux File System  (local file system)
	-> Home Directory: /home/cloudera

      HDFS File System   (distributed file system)
	-> Home Directory: /user/cloudera  (hdfs://localhost.localdomain:8020/user/cloudera)

      Two command formats:
	
	-> hadoop fs -<command>
	-> hdfs dfs -<command>


      Common File Operations on HDFS
      ------------------------------

	List the files & directories:   
		
		hadoop fs -ls
		hdfs dfs -ls

		hdfs dfs -ls datasets
		hadoop fs -ls -R datasets    -> recursive listing

	Help
		hadoop fs -help
		hadoop fs -help stat

	Remove a files/directories:

		hadoop fs -rm <file-name>
		hadoop fs -rm dir1/*.txt
		hadoop fs -rm -R practice1    // remove non-empty directory		

	Create a directory:

		hadoop fs -mkdir demodir

	Delete an empty directory:

		hadoop fs -rmdir demodir

	Copy from one directory to other

		hadoop fs -cp <source> <destination>
		hadoop fs -cp practice1/NYSE_*.txt practice2

	To print the content of a file on stdout

		hadoop fs -cat wordcount_input.txt

	Move from one directory to other

		hadoop fs -mv practice1/NYSE_*.txt practice2/dir1

		
	Changing permissions on an HDFS file

		hadoop fs -chmod 777 practice1
		hadoop fs -chmod -R 777 practice1    // recursive permission


       Copy/Move files between Local File System and HDFS
       ---------------------------------------------------
	
	Copy from Linux to HDFS

	    -> hadoop fs -copyFromLocal datasets/NYSE_dividends.txt practice1
	    -> hadoop fs -put datasets/NYSE_daily.txt practice1

	    -> hadoop fs -put datasets practice1

       Copy from HDFS to Linux

	   -> hadoop fs -copyToLocal <hdfs-file-path> <linux-directory>
	   -> hadoop fs -get  <hdfs-file-path> <linux-directory>

       Move from Linux to HDFS
	
	   -> hadoop fs -moveFromLocal <linux-file-path> <hdfs-directory>
	      hadoop fs -moveFromLocal test1/baseball.txt practice2

       
	

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                  Some important file locations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 Default Paths:

        * Cloudera Local Home :  /home/cloudera
        * HDFS Home: /user/cloudera

 Configuration Files:
	
	* /etc/hadoop/conf/core-site.xml
	* /etc/hadoop/conf/hdfs-site.xml
	* /etc/hadoop/conf/mapred-site.xml

 There are individual folders for each node in the cluster in directories as shown below :
  
        * /etc/hadoop/conf.cloudera.hdfs1
	* /etc/hadoop/conf.cloudera.mapred1
	* /etc/hadoop/conf.cloudera.yarn1

 All daemon log files are written under:

	* /var/log/hadoop/


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                  HDFS Commands
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        
 * hadoop fs -help   				=> lists all the commands with a brief discription
 * hadoop fs -help <command-name>  
 * hadoop fs -chmod -R 755 <file/dir>		=> change permission for a file/directory
 * hadoop fs -cat <file-name>
 * hadoop fs -ls
 * hadoop fs -ls <dir-path>
 * hadoop fs -ls -R  <dir-path>

 * hadoop fs -cp <source file> <target dir>     => Copies within HDFS frpm source to target
 * hadoop fs -mv <source file> <target dir>     => Moves within HDFS frpm source to target
 * hadoop fs -rm -r <file-name/directory-name>
 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 Moving files to/from local file system to/from hdfs 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 * hadoop -copyFromLocal <local file path>  <hdfs file path> 
 * hadoop -put <local file path>  <hdfs file path> 

 * hadoop -copyToLocal <hdfs file path> <local file path>   
 * hadoop -get <hdfs file path> <local file path>   

 * hadoop -moveFromLocal <local file path>  <hdfs file path> 
 * hadoop -moveToLocal <hdfs file path> <local file path> 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 Admin Commands
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 * sudo -u hdfs hdfs dfsadmin -report   => reports the usage stats of the cluster
 * sudo -u hdfs hdfs fsck / -files -blocks

Safe Mode:

* sudo -u hdfs hdfs dfsadmin -safemode leave
  sudo -u hdfs hdfs dfsadmin -safemode enter
  sudo -u hdfs hdfs dfsadmin -safemode get
  sudo -u hdfs hdfs dfsadmin -safemode wait

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 More Commands
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 * du - disk usage  
      
        ex:  hadoop fs -du /user/cloudera


 * du -s : Like -du, but prints a summary of disk usage of all files/directories in the path

        ex: hadoop fs -du -s /user/cloudera


 * df : Shows the capacity, free and used space of the filesystem
         -h  Formats the sizes of files in a human-readable fashion.

        ex: hadoop fs -df -h /user/cloudera/
            hadoop fs -df /user/cloudera/


 * getmerge <src> <localDest>    :  Retrieves all files that match the path src in HDFS, 
                                    and copies them to a single, merged file in the local 
                                    file system identified by localDest. 

        ex: hadoop fs -getmerge test/*.txt test1/merged.txt


 * Get the replication factor of a file:

        ex: hadoop fs -stat %r test/empinfo.txt


 * Set the replication factor of a file while copying or moving from linux

        ex: hadoop fs -D dfs.replication=5 -copyFromLocal <sourc e> <destination>
  
            hadoop fs -D dfs.replication=5 -copyFromLocal datasets/empinfo.txt test2
            hadoop fs -stat %r test2/empinfo.txt


 * Set the replication factor for an existing file:  -setrep [-R] [-w] <rep-factor> <path>
       
        ex: hadoop fs -setrep 2 test2/empinfo.txt   => sets the rep-factor of the file to 2
	    hadoop fs -stat %r test2/empinfo.txt    => test the rep-factor of the file


 * appendToFile

	Usage: hdfs dfs -appendToFile <localsrc> ... <dst>

	Append single src, or multiple srcs from local file system to the destination file system. 
	Also reads input from stdin and appends to destination file system.

	hdfs dfs -appendToFile localfile /user/hadoop/hadoopfile
	hdfs dfs -appendToFile localfile1 localfile2 /user/hadoop/hadoopfile
	hdfs dfs -appendToFile localfile hdfs://nn.example.com/hadoop/hadoopfile
	hdfs dfs -appendToFile - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin.


 * chmod, chgrp, chown  => Change the permissions of files.


====================================================
   YARN  -- (Yet another resource negotiator)
====================================================

   1. Resource Manager
	-> Master daemon
	-> One per cluster
	-> receives job submissions and schedules jobs
	-> Allocates containers for Application masters
	-> Dynamically manages the resource allocation to all the jobs

   2. Node Manager
	-> Slave daemon
	-> One per node
	-> Sends heart-beats to RM
	-> reports the status of all the jobs/tasks running on that node to RM
	-> Launches App.Master for a job

   3. Application Master
	-> One instance per application
	-> manages the tasks related to a job on the cluster
	-> requests for resources required for the job

   4. Container    
	-> Multiple containers are allocated for a single application
	-> The actual tasks of a job are executed in these containers
	-> These containers communicates with the application master.

	
  Documentation  -> https://hadoop.apache.org/docs/r3.3.1/

  Book -> https://www.isical.ac.in/~acmsc/WBDA2015/slides/hg/Oreilly.Hadoop.The.Definitive.Guide.3rd.Edition.Jan.2012.pdf

  ====================================================
      MapReduce
  ====================================================

   -> Hadoop's distributed processing framework. 
	
    MapReduce can be seen from two perspectives

    1. MapReduce Programming Model	
	-> What kind of code the developer has to write.

	Three important classes:

	1. Driver Class 
		-> All the configurations required to run the program
		-> Has the main() method

	2. Mapper Class
		-> Execution logic is defined in this class
		-> Input comes in the form of (K,V) pairs
		-> We define logic to process the key and value as per our need
		   and write the output as (K, V) into a context object.
		-> hundreds of Mapperer will be launched on the cluster


	3. Reducer Class



    2. MapReduce Framework Execution Flow
	-> How an MR program is executed by the framework.

	1. Mapper Phase    -> Will run multiple instances of user defined code written 
			      in the mapper class

	2. Shuffle & Sort   -> Completly framework driven
			       Collect all the values of each unique key across all the outputs of mappers
			       Launches the reduce phase

	3. Reduce Phase     -> Will run one or more instances of user defined code written 
			      in the reduce class


      Submit an MR Program to the Cluster (YARN Resource Manager)
      -----------------------------------------------------------
	  $hadoop jar /home/cloudera/mrpractice.jar com.sample.WordCountJob wordcount_input.txt wcout

      Input Split
      -----------
	-> Always has whole number of records
		-> record in atext file is a line (ending with \n)

	-> InputSplits are created from the blocks where where InputSplit
	   starts at a new record and ends at a record boundary

	-> InputSplits are given as input data to the mappers.

	InputFormat  -> Identified the records and creates InputSpits
	InpuSplit    -> One intergral number of records that could span multple blocks
     	RecordReader -> Creates a (K, V) pair from each record of the inputsplit
	             -> RecordReader calles the map() method with each (K, V) pair

InputSplit						   RecordReader
=========== 						    =========== 
hadoop hive mapreduce hdfs hadoop mapreduce     	=>  (0, hadoop hive mapreduce hdfs hadoop mapreduce)        ---> map
mapper reducer sqoop flume hadoop mapreduce		=>  (40, mapper reducer sqoop flume hadoop mapreduce)	    ---> map	
mapreduce hive flume kafka hadoop hive			=>  (74, mapreduce hive flume kafka hadoop hive)            ---> map
hadoop hadoop mapreduce mapreduce hive hive
sqoop flume kafka mapper reducer partitioner combiner
driver hadoop mapreduce driver mapper combiner
reducer partitioner hadoop sqoop mapreduce


   public class WordCountMapper extends Mapper<LongWritable, Text, Text, LongWritable>

	LongWritable, Text   -> Key data type and Value datatype of input (K,V) pair
				(0, hadoop hive mapreduce hdfs hadoop mapreduce)

	Text, LongWritable   -> Key data type and Value datatype of output (K,V) pair
			        (hadoop, 1), (hive, 1), (mapreduce, 1), (hdfs, 1), (hadoop, 1), (mapreduce,1)


  Input-Split		RecordReader
  -----------          --------------
  hadoop hive mapreduce -> (0, hadoop hive mapreduce) --> map(....) --> (hadoop, 1) (hive, 1) (mapreduce, 1)
  mapper reducer sqoop	-> (23, mapper reducer sqoop)	            --> (mapper, 1) (reducer, 1) (sqoop, 1)									
  mapreduce hive flume
  hadoop hadoop
  sqoop flume kafka


    Creating a MapReduce Program from Scratch
    -----------------------------------------
	
	Two approaches: 

	1. Adding the required dependent jars from your local installation.

		-> Create a blank Java Project
		-> Add the following libraries in the build-path
		   
                    Right Click on Project -> Build Path -> Configure Build Path

		    Go to Libraries Tab -> Add External Jars
			-> /usr/lib/hadoop/hadoop-common.jar
			-> /user/lib/hadoop-0.20-mapreduce/hadoop-core.jar

		-> Start writing your MR code

	2. Using maven (package manager) you can add dependencies from maven-repo.

 










