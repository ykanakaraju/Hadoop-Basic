 
 Agenda
 ------

 1. Introduction to Big Data and Hadoop
    -> Hadoop Basics
    -> Hadoop components – HDFS, MapReduce and YARN
    -> HDFS architecture
    -> MapReduce architecture and stages
    -> YARN architecture

 2. HDFS and MapReduce
    -> HDFS file operations
    -> Hadoop file formats
    -> Working with MapReduce - basics

 3. Hive
   -> Hive basic components & architecture
   -> Managed tables and External Tables
   -> Partitioned Tables
   -> Bucketing of Tables
   -> Hive Indexes
   -> Hive Joins
   -> Performance considerations

 4. PySpark
   -> Spark basics
   -> Spark architecture
   -> Spark APIs
   -> Spark Core API
	-> RDD Transformations
	-> RDD Actions
	-> RDD Persistence
	-> Shared Variables
   -> Spark Execution Flow - DAG Scheduling
   -> Spark Submit command

 5. PySpark SQL
	-> Working with DataFrames
	-> Running SQL on DataFrames - TempViews
	-> Basic Transformations & UDFs
	-> Spark SQL Joins
	-> Working with JDBC
	-> Working with Hive

 6. PySpark Streaming
    -> Structured Streaming API
    -> Working with Streaming sources & sinks
    -> Working with Kafka

 7. Data Engineering using Spark on Azure Databricks
    -> Databricks basics and features
    -> Working with DBFS – DBFS file operations
    -> Azure Databricks architecture – control plane and data plane.
    -> Setup Azure Databricks Workspace
    -> Azure Resource groups and storage account.
    -> Databricks clusters – all-purpose, job clusters and cluster pools
    -> Databricks databases and tables
    -> Setup Azure CLI 
    -> Working with Storage Account using Azure CLI
    -> Accessing Azure ADLS from Databricks
    -> Mount ADLS on Azure Databricks

 8.  Azure Databricks - Delta Lake
    -> Introduction to Delta Lake
    -> Time Travel - Restore a Delta Table to an old version
    -> Vacuum a Delta Table
    -> Delta Table Compaction
    -> Develop and end-to-end ELT Pipeline using Delta Lake 
 

  Materials
  ---------    
    -> PDF Presentations
    -> Code Modules
    -> Class Notes 
    -> Github: https://github.com/ykanakaraju/Hadoop-Basic

 

   Big Data
   --------    
   Type of data which large and complex that your traditional data processing systems
   can not reasonably store and process.


   Big Data scenarios
   ------------------
       -> The volume to be handled is very high (much higher than 1 TB)
       -> You want to support real-time analytics (velocity)
       -> You data is growing very fast, so that in no time your system becomes inadeqaute. 
       -> Process unstructured, semi-structured data etc.

    
  Big Data Characteristics
  ------------------------
       -> Volume
       -> Velocity
       -> Variety 
       -> Veracity (correctness of the data)
       -> Value (extracting insights from the data)   
    

  Why your traditional systems have big data problem
  --------------------------------------------------
   -> Because your traditional systems (that exists in pre-internet era) are not designed
      for internet scale applications.
 
   -> They are designed to run on a single machine. The single machine has only limited 
      amount of resources.

  
  Scalability Options
  -------------------
    -> Scale up  (Vertical Scaling)   -> Migrate to a machine with higher configuration
    -> Scala out (Horizontal Scaling) -> Use many machines instead of one. 
                                      -> Use computing cluster

   What is a Computing Cluster?
   ----------------------------
   A unified network of nodes (machines) whose cumulative resources can be used to store
   or process data as if you working with a single machine. 


   What is a Framework ?
   ---------------------
    A framework is a set of libraries that implement all the basic low-level functionalities
    that the framework is intended for. The users of the framework should focus on the 
    business-logic and implement their solutions as per the guidelines provided by the 
    framework. 



   What is the solution to storage & processing problems associated with Big Data
   ------------------------------------------------------------------------------
    -> Hadoop provides a solution for big-data problems.

 
   What is Hadoop ?
   ----------------     
    Hadoop is a Open Source Framework to Store and Process data in a distributed way 
    using a cluster of commdity harware. 

     -> Provides two things:
	   -> Distributed Storage Solution (HDFS)
	   -> Distributed Processing Solution (MapReduce)
           -> Resource Management (YARN)


    Hadoop Distribution Modes
    -------------------------

	1. standalone mode   
		-> All daemons run in the same JVM in the same machine

	2. psudo distribution mode
		-> All daemons run in the separate JVMs in the same machine

	3. (full) distribution mode
		-> Different daemons run on different nodes (production deployment)



    HDFS (Hadoop Distributed File System)
    --------------------------------------

      -> Stores data in the form of "blocks" of 128 MB each
      -> Each block is replicated on other nodes so many times as defined by a 'replication-factor'
           (default rep-factor is 3)      
      
     Q1:  If I want to stote 1024 MB (1 GB) file on HDFS, how much physical storage space 
          is required (using default configs) => 3 GB   

     Q2:  If you want to store 1034 MB file, how many unique blocks are created on HDFS?
	  -> 8 blocks of 128MB each & 1 block of 10 MB



 =============================================================================
   HDFS - Hadoop Distributed File Systems
 =============================================================================

    -> Stores data in the form of blocks
    -> Each block is replicated (by default, 3 times)

    HDFS Daemons   
    -------------

	-> Daemon: continuously running background job
        -> Name Node (NN)
		-> Runs on the master machine
		-> Maintains two file called "FSImage" & "EditLog" file

                -> FSImage:  Stores all storage related info from the starting of the cluster up to
                             last check-point. 
                              
                             Persistent file stored on disk.

		-> Edit Log: Stores the ongoing actions related to storage from the last check-point
                             and is stored in RAM


        -> Data Node (DN)
		-> Runs in each slave machine
		-> Sends heart-beats to NN once in 3 sec, If NN does not receive 3 heart-beats, it 
                   will mark the slave node as dead. 
		-> DNs send "block-reports" to NN in heart-beats. 


        -> Secondary Name Node (SNN)
		-> Runs on a serapare machine. 
 		-> Responsible for check-pointing process
                   -> Check-pointing is the process of merging editLog with fSImage and copy the new
                      fsImage file back to the NN. 

        -> Balancer
		-> A background process that monitors the replication of files and maintains the 
                   proper replication factor.
                -> Files can become under-replicated (when a DN goes down) or over-replicated when
                   a previously down DN comes backup. Or when we change the exiting replication-factor
                   of the file.
                -> Balancer maintains proper replication in such cases, by creating additional replicas or
                   by deleting overly replicated blocks.


   HDFS High Avaialability
   -----------------------

    => In Hadoop 1.x, NN was a 'single-point-of-failure'

    => Hadoop 2.0 onwards, Hadoop HA concept is introduced where Hadoop maintains Active/StandBy Name Node.

       => Active Name Node and StandBy Nama Node shared the meta-data in real-time using a 
          shared storage system called "Quorum Journal Manager". 
       => If Active NN goes down automatically the standBy NN will become Active NN.  
       => The StandBy subsumes the role of "secondary name node"


   Hadoop Distributions
   --------------------

     -> Hadoop Distributions package and pre-configure a set of commonly used hadoop eco-system components.
     
     -> Using these distribution will save us a lot time and effort, because we don't have to download and 
        install them separatly.

     -> Because all distributions use "open-source" software, they provide one free version and many offer
        licensed version where they provide separate admin-tools and so on.


     Popular distributions:
     ----------------------

       On-premisis Clusters
       --------------------
	-> Cloudera  (CDH 4.7, CDH 5.13, 5.14...)
	-> HortonWorks  (merged with Cloudera in 2019)
        -> MapR

       Cloud Clusters
       --------------
       -> AWS:  EMR (Elastic Map Reduce)
       -> Azure: HadoopInsights  


   Q: Haw many maximum name nodes we can have?
   -------------------------------------------

     Only one "Active" name node (generally) --> one or more Stand-by Name Node

     Very large clusters  may use multiple Active NNs using a concept called "Hadoop Syndication" where 
     each Active NN maintains one or few name-space. 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                  Some important HDFS file locations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 Default Paths:

        * Cloudera Local Home :  /home/cloudera
        * HDFS Home: 		 /user/cloudera

 Configuration Files:
	
	* /etc/hadoop/config/core-site.xml
	* /etc/hadoop/config/hdfs-site.xml
	* /etc/hadoop/config/mapred-site.xml

 There are individual folders for each node in the cluster in directories as shown below :
  
        * /etc/hadoop/conf.cloudera.hdfs1
	* /etc/hadoop/conf.cloudera.mapred1
	* /etc/hadoop/conf.cloudera.yarn1

 All daemon log files are written under:

	* /var/log/hadoop/


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                  HDFS Commands
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        
 * hadoop fs -help   				=> lists all the commands with a brief discription
 * hadoop fs -help <command-name>  
 * hadoop fs -chmod -R 755 <file/dir>		=> change permission for a file/directory
 * hadoop fs -cat <file-name>
 * hadoop fs -ls
 * hadoop fs -ls <dir-path>
 * hadoop fs -ls -R  <dir-path>

 * hadoop fs -cp <source file> <target dir>     => Copies within HDFS frpm source to target
 * hadoop fs -mv <source file> <target dir>     => Moves within HDFS frpm source to target
 * hadoop fs -rm -r <file-name/directory-name>
 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 Moving files to/from local file system to/from hdfs 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 * hadoop -copyFromLocal <local file path>  <hdfs file path> 
 * hadoop -put <local file path>  <hdfs file path> 

 * hadoop -copyToLocal <hdfs file path> <local file path>   
 * hadoop -get <hdfs file path> <local file path>   

 * hadoop -moveFromLocal <local file path>  <hdfs file path> 
 * hadoop -moveToLocal <hdfs file path> <local file path> 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 Admin Commands
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 * sudo -u hdfs hdfs dfsadmin -report   => reports the usage stats of the cluster
 * sudo -u hdfs hdfs fsck / -files -blocks

Safe Mode:

 * sudo -u hdfs hdfs dfsadmin -safemode leave
 * sudo -u hdfs hdfs dfsadmin -safemode enter
 * sudo -u hdfs hdfs dfsadmin -safemode get
 * sudo -u hdfs hdfs dfsadmin -safemode wait

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 More Commands
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 * du - disk usage  
      
        ex:  hadoop fs -du /user/cloudera


 * du -s :   Like -du, but prints a summary of disk usage of all files/directories in the path

        ex: hadoop fs -du -s /user/cloudera

 * df : Shows the capacity, free and used space of the filesystem
         -h  Formats the sizes of files in a human-readable fashion.

        ex: hadoop fs -df -h /user/cloudera/
            hadoop fs -df /user/cloudera/

 * getmerge <src> <localDest>    :  Retrieves all files that match the path src in HDFS, 
                                    and copies them to a single, merged file in the local 
                                    file system identified by localDest. 

        ex: hadoop fs -getmerge test/*.txt test1/merged.txt

 * Get the replication factor of a file:

        ex: hadoop fs -stat %r test/empinfo.txt

 * Set the replication factor of a file while copying or moving from linux

        ex: hadoop fs -D dfs.replication=5 -copyFromLocal <source> <destination>
  
            hadoop fs -D dfs.replication=5 -copyFromLocal datasets/empinfo.txt test2
            hadoop fs -stat %r test2/empinfo.txt

 * Set the replication factor for an existing file:  -setrep [-R] [-w] <rep-factor> <path>
       
        ex: hadoop fs -setrep 2 test2/empinfo.txt   => sets the rep-factor of the file to 2
	    hadoop fs -stat %r test2/empinfo.txt    => test the rep-factor of the file

 * appendToFile

	Usage: hdfs dfs -appendToFile <localsrc> ... <dst>

	Append single src, or multiple srcs from local file system to the destination file system. 
	Also reads input from stdin and appends to destination file system.

	hdfs dfs -appendToFile localfile /user/hadoop/hadoopfile
	hdfs dfs -appendToFile localfile1 localfile2 /user/hadoop/hadoopfile
	hdfs dfs -appendToFile localfile hdfs://nn.example.com/hadoop/hadoopfile
	hdfs dfs -appendToFile - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin.


 * chmod, chgrp, chown  => Change the permissions of files.

 

   =============================================
       MapReduce
   =============================================

     -> Is a distributed computing framework

     Two perspectives:

	-> MapReduce Framework  (How MapReduce execute user-applications)
		-> Mapper Phase (developer will write logic)
		-> Shuffle & Sort Phase (framework will do this)
		-> Reduce Phase (developer will write logic)

	-> MapReduce Programming Paradigm
		-> How to write Map-Reduce programs:
		
		-> MapReduce Program contains several files (maximum 5 files)

			-> Driver Program (mandatory)
			    -> Define the configurations of the program

			-> Mapper Program (mandatory, kinf of)
    			    -> Define business logic to be executed by the mappers

			-> Reducer Program (mostly used, but not mandatory)
			    -> Define business logic related to aggregation of results of mappers

			-> Combiner Program (optional)
			    -> Is a reducer applied to the output of every mapper, so that the number
                               of (K, V) pairs produced by the mapper can be reduced.

                            -> This reduces the amount of the data that has to be shuffled in the 
                               S&S phase.

                            -> This is an optimization.

                            -> This is useful in only those case where your operation is commutative
                               (the order does not matter, like addition)
        
                        -> Partitioner Program (optional)
      			
			    -> Allows developer to specify partitioning logic to control which keys
                               go to which partition

                            -> Only useful when you are having many reducers.


       Mapper => Combiner (like mini-reducer) => Shuffle & Sort => Partitioner => Reducer



















