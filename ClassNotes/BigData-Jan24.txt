 
 Agenda
 ------

 1. Introduction to Big Data and Hadoop
    -> Hadoop Basics
    -> Hadoop components – HDFS, MapReduce and YARN
    -> HDFS architecture
    -> MapReduce architecture and stages
    -> YARN architecture

 2. HDFS and MapReduce
    -> HDFS file operations
    -> Hadoop file formats
    -> Working with MapReduce - basics

 3. Hive
   -> Hive basic components & architecture
   -> Managed tables and External Tables
   -> Partitioned Tables
   -> Bucketing of Tables
   -> Hive Indexes
   -> Hive Joins

 4. PySpark
   -> Spark basics
   -> Spark architecture
   -> Spark APIs
   -> Spark Core API
	-> RDD Transformations
	-> RDD Actions
	-> RDD Persistence
	-> Shared Variables
   -> Spark Execution Flow - DAG Scheduling
   -> Spark Submit command

 5. PySpark SQL
	-> Working with DataFrames
	-> Running SQL on DataFrames - TempViews
	-> Basic Transformations & UDFs
	-> Spark SQL Joins
	-> Working with JDBC
	-> Working with Hive

 6. PySpark Streaming
    -> Structured Streaming API
    -> Working with Streaming sources & sinks
    -> Working with Kafka

 7. Data Engineering using Spark on Azure Databricks
    -> Databricks basics and features
    -> Working with DBFS – DBFS file operations
    -> Azure Databricks architecture – control plane and data plane.
    -> Setup Azure Databricks Workspace
    -> Azure Resource groups and storage account.
    -> Databricks clusters – all-purpose, job clusters and cluster pools
    -> Databricks databases and tables
    -> Setup Azure CLI 
    -> Working with Storage Account using Azure CLI
    -> Accessing Azure ADLS from Databricks
    -> Mount ADLS on Azure Databricks

 8.  Azure Databricks - Delta Lake
    -> Introduction to Delta Lake
    -> Time Travel - Restore a Delta Table to an old version
    -> Vacuum a Delta Table
    -> Delta Table Compaction
    -> Develop and end-to-end ELT Pipeline using Delta Lake 
 

  Materials
  ---------    
    -> PDF Presentations
    -> Code Modules
    -> Class Notes 
    -> Github: https://github.com/ykanakaraju/Hadoop-Basic
 

   Big Data
   --------    
   	Type of data which large and complex that your traditional data processing systems
   	can not reasonably store and process.


   Big Data scenarios
   ------------------
       -> The volume to be handled is very high (much higher than 1 TB)
       -> You want to support real-time analytics (velocity)
       -> You data is growing very fast, so that in no time your system becomes inadeqaute. 
       -> Process unstructured, semi-structured data etc.

    
  Big Data Characteristics
  ------------------------
       -> Volume
       -> Velocity
       -> Variety 
       -> Veracity (correctness of the data)
       -> Value (extracting insights from the data)   
    

  Why your traditional systems have big data problem
  --------------------------------------------------
   -> Because your traditional systems (that exists in pre-internet era) are not designed
      for internet scale applications.
 
   -> They are designed to run on a single machine. The single machine has only limited 
      amount of resources.

  
  Scalability Options
  -------------------
    -> Scale up  (Vertical Scaling)   -> Migrate to a machine with higher configuration
    -> Scala out (Horizontal Scaling) -> Use many machines instead of one. 
                                      -> Use computing cluster

   What is a Computing Cluster?
   ----------------------------
   A unified network of nodes (machines) whose cumulative resources can be used to store
   or process data as if you working with a single machine. 


   What is a Framework ?
   ---------------------
    A framework is a set of libraries that implement all the basic low-level functionalities
    that the framework is intended for. The users of the framework should focus on the 
    business-logic and implement their solutions as per the guidelines provided by the 
    framework. 



   What is the solution to storage & processing problems associated with Big Data
   ------------------------------------------------------------------------------
    -> Hadoop provides a solution for big-data problems.

 
   What is Hadoop ?
   ----------------     
    Hadoop is a Open Source Framework to Store and Process data in a distributed way 
    using a cluster of commdity harware. 

     -> Provides two things:
	   -> Distributed Storage Solution (HDFS)
	   -> Distributed Processing Solution (MapReduce)
           -> Resource Management (YARN)


    Hadoop Distribution Modes
    -------------------------

	1. standalone mode   
		-> All daemons run in the same JVM in the same machine

	2. psudo distribution mode
		-> All daemons run in the separate JVMs in the same machine

	3. (full) distribution mode
		-> Different daemons run on different nodes (production deployment)



    HDFS (Hadoop Distributed File System)
    --------------------------------------

      -> Stores data in the form of "blocks" of 128 MB each
      -> Each block is replicated on other nodes so many times as defined by a 'replication-factor'
           (default rep-factor is 3)      
      
     Q1:  If I want to stote 1024 MB (1 GB) file on HDFS, how much physical storage space 
          is required (using default configs) => 3 GB   

     Q2:  If you want to store 1034 MB file, how many unique blocks are created on HDFS?
	  -> 8 blocks of 128MB each & 1 block of 10 MB



 =============================================================================
   HDFS - Hadoop Distributed File Systems
 =============================================================================

    -> Stores data in the form of blocks
    -> Each block is replicated (by default, 3 times)

    HDFS Daemons   
    -------------

	-> Daemon: continuously running background job
        -> Name Node (NN)
		-> Runs on the master machine
		-> Maintains two file called "FSImage" & "EditLog" file

                -> FSImage:  Stores all storage related info from the starting of the cluster up to
                             last check-point. 
                              
                             Persistent file stored on disk.

		-> Edit Log: Stores the ongoing actions related to storage from the last check-point
                             and is stored in RAM


        -> Data Node (DN)
		-> Runs in each slave machine
		-> Sends heart-beats to NN once in 3 sec, If NN does not receive 3 heart-beats, it 
                   will mark the slave node as dead. 
		-> DNs send "block-reports" to NN in heart-beats. 


        -> Secondary Name Node (SNN)
		-> Runs on a serapare machine. 
 		-> Responsible for check-pointing process
                   -> Check-pointing is the process of merging editLog with fSImage and copy the new
                      fsImage file back to the NN. 

        -> Balancer
		-> A background process that monitors the replication of files and maintains the 
                   proper replication factor.
                -> Files can become under-replicated (when a DN goes down) or over-replicated when
                   a previously down DN comes backup. Or when we change the exiting replication-factor
                   of the file.
                -> Balancer maintains proper replication in such cases, by creating additional replicas or
                   by deleting overly replicated blocks.


   HDFS High Avaialability
   -----------------------

    => In Hadoop 1.x, NN was a 'single-point-of-failure'

    => Hadoop 2.0 onwards, Hadoop HA concept is introduced where Hadoop maintains Active/StandBy Name Node.

       => Active Name Node and StandBy Nama Node shared the meta-data in real-time using a 
          shared storage system called "Quorum Journal Manager". 
       => If Active NN goes down automatically the standBy NN will become Active NN.  
       => The StandBy subsumes the role of "secondary name node"


   Hadoop Distributions
   --------------------

     -> Hadoop Distributions package and pre-configure a set of commonly used hadoop eco-system components.
     
     -> Using these distribution will save us a lot time and effort, because we don't have to download and 
        install them separatly.

     -> Because all distributions use "open-source" software, they provide one free version and many offer
        licensed version where they provide separate admin-tools and so on.


     Popular distributions:
     ----------------------

       On-premisis Clusters
       --------------------
	-> Cloudera  (CDH 4.7, CDH 5.13, 5.14...)
	-> HortonWorks  (merged with Cloudera in 2019)
        -> MapR

       Cloud Clusters
       --------------
       -> AWS:  EMR (Elastic Map Reduce)
       -> Azure: HadoopInsights  


   Q: Haw many maximum name nodes we can have?
   -------------------------------------------

     Only one "Active" name node (generally) --> one or more Stand-by Name Node

     Very large clusters  may use multiple Active NNs using a concept called "Hadoop Syndication" where 
     each Active NN maintains one or few name-space. 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                  Some important HDFS file locations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 Default Paths:

        * Cloudera Local Home :  /home/cloudera
        * HDFS Home: 		 /user/cloudera

 Configuration Files:
	
	* /etc/hadoop/config/core-site.xml
	* /etc/hadoop/config/hdfs-site.xml
	* /etc/hadoop/config/mapred-site.xml

 There are individual folders for each node in the cluster in directories as shown below :
  
        * /etc/hadoop/conf.cloudera.hdfs1
	* /etc/hadoop/conf.cloudera.mapred1
	* /etc/hadoop/conf.cloudera.yarn1

 All daemon log files are written under:

	* /var/log/hadoop/


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                  HDFS Commands
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        
 * hadoop fs -help   				=> lists all the commands with a brief discription
 * hadoop fs -help <command-name>  
 * hadoop fs -chmod -R 755 <file/dir>		=> change permission for a file/directory
 * hadoop fs -cat <file-name>
 * hadoop fs -ls
 * hadoop fs -ls <dir-path>
 * hadoop fs -ls -R  <dir-path>

 * hadoop fs -cp <source file> <target dir>     => Copies within HDFS frpm source to target
 * hadoop fs -mv <source file> <target dir>     => Moves within HDFS frpm source to target
 * hadoop fs -rm -r <file-name/directory-name>
 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 Moving files to/from local file system to/from hdfs 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 * hadoop -copyFromLocal <local file path>  <hdfs file path> 
 * hadoop -put <local file path>  <hdfs file path> 

 * hadoop -copyToLocal <hdfs file path> <local file path>   
 * hadoop -get <hdfs file path> <local file path>   

 * hadoop -moveFromLocal <local file path>  <hdfs file path> 
 * hadoop -moveToLocal <hdfs file path> <local file path> 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 Admin Commands
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 * sudo -u hdfs hdfs dfsadmin -report   => reports the usage stats of the cluster
 * sudo -u hdfs hdfs fsck / -files -blocks

Safe Mode:

 * sudo -u hdfs hdfs dfsadmin -safemode leave
 * sudo -u hdfs hdfs dfsadmin -safemode enter
 * sudo -u hdfs hdfs dfsadmin -safemode get
 * sudo -u hdfs hdfs dfsadmin -safemode wait

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 More Commands
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 * du - disk usage  
      
        ex:  hadoop fs -du /user/cloudera


 * du -s :   Like -du, but prints a summary of disk usage of all files/directories in the path

        ex: hadoop fs -du -s /user/cloudera

 * df : Shows the capacity, free and used space of the filesystem
         -h  Formats the sizes of files in a human-readable fashion.

        ex: hadoop fs -df -h /user/cloudera/
            hadoop fs -df /user/cloudera/

 * getmerge <src> <localDest>    :  Retrieves all files that match the path src in HDFS, 
                                    and copies them to a single, merged file in the local 
                                    file system identified by localDest. 

        ex: hadoop fs -getmerge test/*.txt test1/merged.txt

 * Get the replication factor of a file:

        ex: hadoop fs -stat %r test/empinfo.txt

 * Set the replication factor of a file while copying or moving from linux

        ex: hadoop fs -D dfs.replication=5 -copyFromLocal <source> <destination>
  
            hadoop fs -D dfs.replication=5 -copyFromLocal datasets/empinfo.txt test2
            hadoop fs -stat %r test2/empinfo.txt

 * Set the replication factor for an existing file:  -setrep [-R] [-w] <rep-factor> <path>
       
        ex: hadoop fs -setrep 2 test2/empinfo.txt   => sets the rep-factor of the file to 2
	    hadoop fs -stat %r test2/empinfo.txt    => test the rep-factor of the file

 * appendToFile

	Usage: hdfs dfs -appendToFile <localsrc> ... <dst>

	Append single src, or multiple srcs from local file system to the destination file system. 
	Also reads input from stdin and appends to destination file system.

	hdfs dfs -appendToFile localfile /user/hadoop/hadoopfile
	hdfs dfs -appendToFile localfile1 localfile2 /user/hadoop/hadoopfile
	hdfs dfs -appendToFile localfile hdfs://nn.example.com/hadoop/hadoopfile
	hdfs dfs -appendToFile - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin.


 * chmod, chgrp, chown  => Change the permissions of files.

 

 =============================================
     MapReduce
 =============================================

     -> Is a distributed computing framework

     Two perspectives:

	-> MapReduce Framework  (How MapReduce execute user-applications)
		-> Mapper Phase (developer will write logic)
		-> Shuffle & Sort Phase (framework will do this)
		-> Reduce Phase (developer will write logic)

	-> MapReduce Programming Paradigm
		-> How to write Map-Reduce programs:
		
		-> MapReduce Program contains several files (maximum 5 files)

			-> Driver Program (mandatory)
			    -> Define the configurations of the program

			-> Mapper Program (mandatory, kind of)
    			    -> Define business logic to be executed by the mappers

			-> Reducer Program (mostly used, but not mandatory)
			    -> Define business logic related to aggregation of results of mappers

			-> Combiner Program (optional)
			    -> Is a reducer applied to the output of every mapper, so that the number
                               of (K, V) pairs produced by the mapper can be reduced.

                            -> This reduces the amount of the data that has to be shuffled in the 
                               S&S phase.

                            -> This is an optimization.

                            -> This is useful in only those cases where your operation is commutative
                               (the order does not matter, like addition)
        
                        -> Partitioner Program (optional)      			
			    -> Allows developer to specify partitioning logic to control which keys
                               go to which partition

                            -> Only useful when you are having many reducers.


       Mapper => Combiner (like mini-reducer) => Shuffle & Sort => Partitioner => Reducer


    Reference Libraries to be added to your MR programs
    ---------------------------------------------------

	/usr/lib/hadoop/hadoop-common.jar
	/usr/lib/hadoop-0.20-mapreduce/hadoop-core-2.6.0-mr1-XXX.jar


    Executing a Map-Reduce Program
    ------------------------------

    $hadoop jar <jar-file-path> <driver-class-qualified-name> <command-line-args>
    $hadoop jar /home/cloudera/mrpractice.jar com.sample.WordCountJob <input-path> <output-dir>

    // To submit to YARN
    $yarn jar /home/cloudera/mrpractice.jar com.sample.WordCountJob <input-path> <output-dir>


   MapReduce Framework Classes
   ---------------------------

   1. InputFormat  -> Is to identify "records" from the block.
                   -> For text files each line is regarded as a record. 

   2. InputSplit   -> create a whole number of records that can span
                       one or many blocks. 
		   -> Each map task processes one input-split.

   3. RecordReader -> Will reader each record from the input-split and calls
                      the "map" method that is defined inside the Mapper class.
  


    Hadoop Data Types
    -----------------

     => All hadoop classes work with only hadoop data types
     => Hadoop data types are optimized for serialization and deserialization
          -> They implement Writer and Comparator interfaces. 
     
       Java         	Hadoop
       ----         	------        
	String		Text
	Integer		IntWritable
	Long		LongWritable
	Null		NullWritable
	Bool		BoolWritable
  

   Mapreduce Life Cycle Methods
   ----------------------------


    Mapper Class: 
    -------------
	 
	1. setup    -> called once at the beginning (before the map method called for the first record)
		    -> define initializations, define global vars. 

	2. map	    -> once per record
                    -> define the logic to be applied on each record

	3. cleanup  -> executed once after all the records are processed
		    -> to cleanup resources or write collect data into the context

	4. run	    -> generally not overridden
		    -> controls the execution flow.

    Reducer Class:
    --------------
	
	1. setup    -> called once at the beginning (before the reduce method called for the first record)
		    -> define initializations, define global vars. 

	2. reduce   -> once per record
                    -> define the logic to be applied on each record

	3. cleanup  -> executed once after all the records are processed
		    -> to cleanup resources or write collect data into the context

	4. run	    -> generally not overridden
		    -> controls the execution flow.



 ==============================
	Hive
 ==============================
	
   Hive -> Data Warehousing Framework for Hadoop built on top of MapReduce
        -> Provides an SQL wrapper on top of MR, so that developers can work with SQL (and not MR)
        -> Hive provides a dialect of SQL called "Hive Query Language" (HQL / HiveQL)

        -> Hive is not an RDBMS/NoSQL database
	   -> Hive does not have constraints such as PKs, FKs, etc..
	   -> All the data is stored as data-files in "hive warehouse" directory. 

        -> Hive, by default, uses MR as the execution framework. 

        -> From Hive 2.x onwards, Hive can be configured to use other distributed execution 
           frameworks also, instead of MR. 

		-> Tez is widely used. 
                -> Hive can be configured to use 'Spark' also.  


   Important components of Hive:
   ----------------------------

      1. Hive Warehouse  => Is a directory managed by hive in HDFS
                            Where Hive stores all data files of all databases & tables that it manages.
			    default location: /user/hive/warehouse 


      2. Hive Metastore  => Usually it is an RDBMS (usually MySQL) in production system.
                            Also comes with embedded metastote called "Derby"
                            Hive stores all its metadata in Hive metastore.

         ** Actual data files are stored in Hive Warehouse
         ** All the metadata is managed in Hive MetaStore 

      3. Hive Query Engine (Hive Server) => Responsible for executing Hive QL statements.


   How To Connect to Hive
   ----------------------

    1. CLI (Command Line Interface) 

       -> Two Shells:
		-> Hive Shell :    Suitable for local hive installation
				   Less Secure

                -> Beeline Shell : Connects to local/remote Hive through JDBC connection.
                                   Better Security

    2. HUE (Hadoop User Environment)

    3. Client Applications 
		-> Can connect to Hive using JDBC, ODBC, Thrift. 
                -> For external applications and tools to connect to Hive

  
    Hive Database File Structure
    ----------------------------

    -> All managed table are stored in a warehouse directory @ "/user/hive/warehouse"
	
    -> Under the warehouse directory each user-created database will have its own (<db-name>.db) directory
	ex: 'db1' database is localed at:    /user/hive/warehouse/db1.db  
	
    -> Under the database directory, each table will have its own directory
	ex: "db1.emp" table's datafiles are localed at:    /user/hive/warehouse/db1.db/emp/<data-files>

    Exception: "default" database has no separate directory inside hive warehouse dir. 
               All the tables that belong to "default" database are stored directly under  "/user/hive/warehouse" directory.
    

  
   Working with Hive Tables
   ------------------------
    
    -> Data is loaded into Hive tables generaly in two ways:
	  1. From external files such as demilited files (like CSV, TSV), parquet files, ORC files, Seqence files etc..
          2. From already existing table using "INSERT..SELECT"
          3. Inserting data into table using "INSERT INTO .. VALUES(..)" command is supported,
             but not preferred.

     Defaults:
     ---------
	* STORAGE FORMAT:  TEXTFILE
        * Lines Terminator: \n
        * Fields Terminator: Ctrl-A (\0001)

     Describe Tables
     ---------------
       	$describe empinfo
	$describe extended empinfo
	$describe formatted empinfo;
   

    => Can hive validate data while loading agaist the schema of the table ?  NO

  
    Schema on Read
    --------------
    -> Hive does not validate data while loading (using LOAD DATA command)
    -> Hive validates the data on reading. 
	-> If some field can not parse as per the defined datatype, Hive simply returns NULL

    Managed Tables
    --------------

     -> Managed Table are owned by Hive and is stored inside hive warehouse.
     -> When you drop the table Hive drops both the schema from metastore and the 
        data-files from the warehouse. 


    External Tables
    ---------------

     -> External tables are tables whose data resides outside of hive warehouse.
     -> External tables are not managed by Hive.
     -. When you drop an external table, only the schema is dropped (from metastore) but not the datafiles. 
     -> Using external tables, we can connect to existing data and analyse that data using Hive without
        owning that data.


    Partitioned Tables
    ------------------

      -> Hive allows us to partition table into many dircetories based on one or more columns. 

      -> By partitioning your table, you can reduce the amount of data files that Hive have to scan 
         if query has the partition column specified. 

      -> When you partition a table by a column, Hive creates a separate directory within the
         table's directory, for every unique value of the partition column.

      -> Based on how we load the data into partitioned tables, we can think of partitioning as two types

       1. Static Parititioning	

          -> Is applied when our data does not have the partition info and we hardcode the partition
             value in the "LOAD" statement.

       2. Dynamic Partitioning

	  -> When we can not decide into which partition the data file will go to (because the file 
             may have the partition data itself and may have data for several partition.), then we
             use dynamic partitioning approach.

	  -> Here, we do not use "LOAD" statements to load, instead we load data from a base table 
             using "INSERT..SELECT" command. 

          -> You have set the following config to load using dynamic partitioning
                set hive.exec.dynamic.partition.mode=nonstrict
  

    Limitations of Partitioning
    ---------------------------

     -> What if your partitions are too big, and you want further reduce the amount of data
        in a given partition into more segments.		
	   => In this case, we may using bucketting within each partition. And we create some
              predefined number of segments within each partition.

     -> What if your partitions are too small (like for eaxmple partitioning by high cardinality
        columns such as product_id, name, email ....). MR is not efficient with lot of small file.
        In such cases, partitioning may be a solution at all.
	    => You can use bucketting at the table level and define a predefined number of buckets. 


   Bucketing
   ----------

    -> Allows you to organize your data into some predefined number of segments. This will add another
       layer of segmentation on top of partitioning. Or you can also bucket at the table level.

       Table data file structure in Hive
       ----------------------------------

       	DATABASE --> TABLE
       	DATABASE --> TABLE --> PARTITIONS
		DATABASE --> TABLE --> BUCKETS
		DATABASE --> TABLE --> PARTITIONS --> BUCKETS
       
    
      Loading data into bucketted tables
      ----------------------------------

	-> Can not use "LOAD DATA .." statements
	-> Should use "INSERT .. SELECT" command to load data into bucketted table.
	-> set hive.enforce.bucketing = true;  // enforce bucketting

      
     Managing Output
     ---------------

     [select * from emp]    
	--> return the results to the client;
	--> insert/overwrite the data into a table  (INSERT .. SELECT)
        --> create a table with the query output (CREATE TABLE AS SELECT)
        --> Save the contents of the query into a local/hdfs directory in the desired format.

--------------------------------------------------------------------
                         GENERAL INFO
--------------------------------------------------------------------
 
 $hive -f empinfo.hql       --> This command launched fom a linux shell and executes a script file

 show functions;            --> shows list of functions available in hive
  
 describe function length   --> gives usage descriptin about a function

 ! : you can use ! to invoke linux & hdfc command from hive shell
 ex: hive> ! hadoop fs -ls

--------------------------------------------------------------------
		 BEELINE SHELL
--------------------------------------------------------------------
$ beeline
beeline> !connect jdbc:hive2://quickstart:10000 cloudera cloudera org.apache.hive.jdbc.HiveDriver

0:jdbc:hive2://quickstart:10000> show databases;  

--------------------------------------------------------------------
		  BASIC COMMANDS
--------------------------------------------------------------------

 create database if not exists <database-name>;

 show databases;

 use <database-name>;

 show tables;

 describe <table-name>;

 describe extended <table-name>;  --> gives a lot more info about the table

 show partitions <table-name>;    --> lists the partitions created for a table

--------------------------------------------------------------------
		  CREATE TABLE  & CTAS Construct
--------------------------------------------------------------------

--- Managed table ---
CREATE TABLE empinfo (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n' 
STORED AS TEXTFILE;


--- External table ---
CREATE EXTERNAL TABLE empexternal(empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION '/user/cloudera/emp_external';


--- Partitioned table (two level partions) ---
CREATE TABLE logs (ts BIGINT, line STRING)
PARTITIONED BY (dt STRING, country STRING)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
STORED AS TEXTFILE

--- Bucketed table ---
CREATE TABLE bucketed_users (id INT, name STRING)
CLUSTERED BY (id) 
SORTED BY (id ASC) 
INTO 4 BUCKETS;

--- Create an empty table with same schema as an existing table ---
CREATE TABLE <new_table> LIKE <existing_table>;

--- CTAS ---
CREATE TABLE target AS SELECT col1, col2 FROM source;

--- Create table with complex data types ----------
CREATE TABLE complex_data
(col1 array<string>,
 col2 map<int,string>,
 col3 struct<name:string,score:int>)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
COLLECTION ITEMS TERMINATED BY '/'
MAP KEYS TERMINATED BY ':'
LINES TERMINATED BY '\n'

---- complex data set example for the above table ----
Hadoop/HDFS/MR/Hive/Sqoop,1100:Virat/1101:Sachin/1102:Dhoni,Ram/80
HDFS/MR/Hive/Hadoop/Sqoop,1100:Virat/1101:Sachin/1102:Dhoni,Laxman/75
Hadoop/HDFS/MR/Hive/Sqoop,1100:Virat/1101:Sachin/1102:Dhoni,Bharat/70

--------------------------------------------------------------------
		DATA LOADING 
--------------------------------------------------------------------

LOAD DATA LOCAL INPATH '<local linux file path>'
OVERWRITE INTO TABLE <table name>


LOAD DATA INPATH '<hdfs file path>'
OVERWRITE INTO TABLE <table name>


--loading data into external table --
LOAD DATA LOCAL INPATH '<local linux file path>' INTO TABLE <table name>


--loading data into partitioned table --
LOAD DATA LOCAL INPATH 'input/hive/partitions/file1' INTO TABLE logs
PARTITION (dt='2001-01-01', country='IN');

--------------------------------------------------------------------
		IMPORTING DATA USING INSERT
--------------------------------------------------------------------
NOTE:
~~~~
INSERT INTO  	 => Adds/Append to existing data
INSERT OVERWRITE => Replaces the existing data (if any)

-- load data into partition table with existing table data ----
INSERT OVERWRITE TABLE emp_part
PARTITION (country='IN')
SELECT empid, ename, salary, deptid FROM emp;


-- load data into partition table with dynamic partitioning ----
INSERT OVERWRITE TABLE target
PARTITION (country)
SELECT empid, ename, salary, deptid, country FROM emp;


-- load data into bucketed table with existing table data ----
INSERT OVERWRITE TABLE emp_bucketed
SELECT <columns-list> from <table-name> 
WHERE <where clause>

-- load data into external directory  ----
INSERT OVERWRITE DIRECTORY '/user/cloudera/hbase_student_info' 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '\t' 
SELECT * FROM hbase_student_info


-- multitable insert ----------------
FROM records2
  INSERT OVERWRITE TABLE stations_by_year
    SELECT year, COUNT(DISTINCT station) GROUP BY year
  INSERT OVERWRITE TABLE records_by_year
    SELECT year, COUNT(1) GROUP BY year
  INSERT OVERWRITE TABLE good_records_by_year
    SELECT year, COUNT(1)
    WHERE temperature != 9999 AND quality IN (0, 1, 4, 5, 9)
    GROUP BY year;


--------------------------------------------------------------------
	  QUERYING DATA FROM BUCKET TABLES
--------------------------------------------------------------------

SELECT * FROM <table-name> TABLESAMPLE (BUCKET 1 OUT OF 2 on empid)

SELECT * FROM <table-name> TABLESAMPLE (BUCKET 1 OUT OF 2 on rand())

--------------------------------------------------------------------
   BINARY STORAGE FORMATS - avro, sequencefile, parquet, rc, orc
--------------------------------------------------------------------
# ... avro .....
SET hive.exec.compress.output = true;
SET avro.output.codec = snappy;
CREATE TABLE ... STORED AS AVRO;


# ... sequencefile .....
CREATE TABLE ... STORED AS SEQUENCEFILE;


# ... parquet .....
CREATE TABLE users_parquet STORED AS PARQUET
AS
SELECT * FROM users;


#... Using a custom SerDe: RegexSerDe ...
## .. in this example RegexSerDe is a java class that implements 
## .. org.apache.hadoop.hive.contrib.serde2.RegexSerDe class.

CREATE TABLE stations (usaf STRING, wban STRING, name STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'
WITH SERDEPROPERTIES (
"input.regex" = "(\\d{6}) (\\d{5}) (.{29}) .*"
);

--------------------------------------------------------------------
	ALTER - Alter Table defitions, Partitions etc.
--------------------------------------------------------------------
-- rename a table ---
ALTER TABLE <table-name> RENAME TO <new-name>;

--renaming a partition --
ALTER TABLE employee PARTITION (country='cn') RENAME TO PARTITION (country='CN');

-- add a column to an existing table --- 
ALTER TABLE <table-name> ADD COLUMNS (new_column STRING);

-- add a new partition to a partioned table ---
ALTER TABLE emp ADD PARTITION (country='CN') LOCATION '<directory-path>';

-- change a column name and type ---
ALTER TABLE emp CHANGE empname ename STRING;
ALTER TABLE emp CHANGE empname ename STRING AFTER empid;

-- alter table location / partition location --
ALTER TABLE emp SET LOCATION '<directory-path>';
ALTER TABLE emp PARTITION (country = 'IN') SET LOCATION '<directory-path>';


 ==================================
      Spark  (using PySpark)
 ==================================
   
    -> Spark is in-memory distributed computing framework

    -> Spark is used for big data analysis

    -> Spark is written in Scala
	-> We can write Spark applications in Scala, Java, Python & R

    -> Spark is 'unified' analytics framework

	Spark provides a consistent set of APIs for performing different analytics workloads
        using the same execution engine and some well defined data abstractions and operations.

   	Batch Analytics of unstructured data	-> Spark Core API (low-level api)
	Batch Analytics of structured data 	-> Spark SQL
	Streaming Analytics (real time)		-> Spark Streaming, Structured Streaming
	Predictive analytics (ML)		-> Spark MLLib
	Graph parallel computations  		-> Spark GraphX

   -> Spark applications can run on clusters managed by multiple cluster managers
  

  Spark Architecture
  ------------------

    	1. Cluster Manager (CM)
		-> Applications are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the 'SparkContext' object (represents an application)
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		-> Where to run the driver process
		 1. Client : default, driver runs on the client. 
		 2. Cluster: driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks run the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver. 



   Getting started with Spark on Databricks
   ----------------------------------------

   	Signup to Databricks Community Edition (free edition)
 	-----------------------------------------------------		
	Signup: https://www.databricks.com/try-databricks

		Screen 1: Fill up the details with valid email address
		Screen 2: Click on "Get started with Community Edition" link (Not the 'Continue' button)

	Login: https://community.cloud.databricks.com/login.html


	Downloading a file from Databricks
	----------------------------------
	/FileStore/<FILEPATH>
	https://community.cloud.databricks.com/files/<FILEPATH>?o=4949609693130439

	Example:
	file path to be downloaded: dbfs:/FileStore/ctsdatasets/output/wc/part-00000
	https://community.cloud.databricks.com/files/ctsdatasets/output/wc/part-00000?o=1072576993312365


	Enabling DBFS link in Catalog
	-----------------------------
	-> When you login to Databricks for the first time you won't see the DBFS button.
	-> To enable this, do the following:
		-> Click on the Account link (top-right corner)
		   -> Admin Settings
		      -> Workspace settings
			-> Advanced
		           -> Click on "DBFS File Browser" toggle button (to enable it)
			      -> Refresh the browser window. 

	Working with Databricks
	-----------------------

	1. Create a Cluster 
	   (Community edition allows you create one single node cluster with default config)

	   Click on 'Compute' (left-menu option)
	   -> Click on 'Create compute' button
	   -> Provide a cluster name and click on 'Create compute' button
		-> Spinning up the cluster can take 5 to 7 mins

       2. Databricks Workspace
	  -> Is a location where you organize all your work (notebooks)
          -> Navigate to your user account (User -> your email) and create a folder


  RDD (Resilient Distributed Dataset)
  -----------------------------------
    -> RDD is the fundamental data abstraction in Spark Core API
	
    -> RDD is a collection of distributed in-memory partitions.
	-> Each partition is a collection of objects of some type.

    -> RDDs are immutable

    -> RDDs are lazily evaluated
	-> Transformations does not cause execution
	-> Action commands cause execution. 
  
    
  Creating RDDs
  -------------
	
    Three ways:

	1. Create an RDD from external data files.

		rddFile = sc.textFile( <filePath>, 4 )

		default Number of partitions: sc.defaultMinPartition
		  sc.defaultMinPartition = 2, if number of cores >= 2
					   1, otherwise

	2. Create an RDD from programmatic data

		rdd1 = sc.parallelize([2,4,3,1,5,6,7,8,6,8,9,0,7,5,4,6,8], 2)
		
		default Number of partitions: sc.defaultParallelism
		sc.defaultParallelism = number of CPU cores allocated.


	3. By applying transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x*10)


  RDD Operations
  --------------
	Two types of operations

	1. Transformations
		-> Transformations return RDDs
		-> Transformations does not cause execution of RDDs
		-> Cause lineage DAGs to be created

	2. Actions
		-> Triggers execution of RDDs
		-> Produces output


  RDD Lineage DAG
  ---------------
  Driver maintains a Lineage DAG for every RDD
  Lineage DAG is a heirarchy of dependencies of RDDs all the way upto th very first RDD	
  Lineage DAG is a logical plan on how to create the RDD.


     rddFile = sc.textFile(<filePath>, 4)	
     Lineage of rddFile: (4) rddFile -> textFile on <filePath>

     rddWords = rddFile.flatMap(lambda x: x.split(" "))
     Lineage of rddWords: (4) rddWords -> rddFile.flatMap -> textFile on <filePath>

     rddPairs = rddWords.map(lambda x: (x, 1))
     Lineage of rddPairs: (4) rddPairs -> rddWords.map -> rddFile.flatMap -> textFile on <filePath>

     rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
     Lineage of rddWc: (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> textFile on <filePath>


  Types of RDD Transformation
  ---------------------------

	Two types:

	 1. Narrow Transformations
           -> Narrow transformations are those, where the computation of each partition depends ONLY
              on its input partition.
           -> There is no shuffling of data.
           -> Simple and efficient


      	2. Wide Transformations
           -> In wide transformations, the computation of a single partition depends on many
              partitions of its input RDD.
           -> Data shuffle across partitions will happen.
           -> Complex and expensive


  RDD DAG Scheduling and Execution Flow
  -------------------------------------

      Application (created by a SparkContext)
	|
	|=> Jobs (each action command on an RDD lauches a job)
	    |
	    |=> Stages (one or more stages; each wide transformation create a new stage)
		|
		|=> Tasks ( set of transformations that can run in partition)
		    |
  		    |=> Transformations (that can run in parallel)


  RDD Persistence
  ---------------	
	rdd1 = sc.textFile( <file>, 4 )
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.DISK_ONLY )   ----> is an instruction to spark to save RDD partitions
	rdd7 = rdd6.t7(...)

	rdd6.collect()
	lineage of rdd6 : (4) rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[sc.textFile, t3, t5, t6] -> collected to the driver

	rdd7.collect()   
	lineage of rdd6 : (4) rdd7 -> rdd6.t7 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[t7] -> collected to the driver

	rdd6.unpersist()

	StorageLevels
	-------------
	MEMORY_ONLY	      -> default, Memory Serialized 1x Replicated
	MEMORY_AND_DISK	      -> Disk Memory Serialized 1x Replicated
	DISK_ONLY	      -> Disk Serialized 1x Replicated 	
	MEMORY_ONLY_2	      -> Memory Serialized 2x Replicated
	MEMORY_AND_DISK_2     -> Disk Memory Serialized 2x Replicated

	Commands
	--------
	rdd1.cache()      -> in-memory persistence
	rdd1.persist() 	  -> in-memory persistence
	rdd1.persist( StorageLevel.DISK_ONLY )

	rdd1.unpersist()


   Spark Executor Memory Structure
   -------------------------------
     
      Let us say we are requesting an executor with 10 GB RAM. 
      The spark job will be allocated executors with 10.3 GB (10GB + 300MB) RAM. 
     
      
      1. Reserved Memory (300 MB)
           -> Spark's internal usage
      
      2. Spark Memory (spark.memory.fraction: 0.6)   => 6 GB (Unified Memory)
          
         2.1  Execution Memory
                 -> Used for RDD partition creation and transformation
                 -> Can forcibly evict RDD partitions from storage memory if it requires
                    additional memory upto the quota allocated for it.

         2.2  Storage Memory (spark.memory.storageFraction: 0.5) => 3 GB
                 -> The RDD partitions and broadcast variables are persisted
                    in this memory.

      3. User Memory  => 4 GB
         -> Running non-spark related code execution. 
         -> Related to running python methods, storing python collection.


  RDD Transformations
  -------------------

  1. map		P: U -> V
			Object to Object transformation
			input RDD: N objects, output RDD: N objects
			
	 rdd2 = rdd1.map(lambda x: (x%5, x))


  2. filter		P: U -> Boolean
			Objects for which the function returns True will be in the output RDD
			input RDD: N objects, output RDD: <= N objects

	rddFile.filter(lambda x: len(x.split(" ")) > 8).collect()
	
				
  3. glom		P: None
			Return one list object per partition with all the objects of that partition.

		rdd1		rdd2 = rdd1.glom()
		P0: 3,2,1,4,5,6 -> glom -> P0: [3,2,1,4,5,6]
		P1: 5,6,7,8,9,3 -> glom -> P1: [5,6,7,8,9,3]
		P2: 4,7,6,2,1,0 -> glom -> P2: [4,7,6,2,1,0]

		rdd1.count() = 10 (int)    rdd2.count = 3 (list)

  4. flatMap		P: U -> Iterable[V]
			fatMap flattens the iterables produced by the function.

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


  5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation

		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p) ).collect()
		rdd1.mapPartitions(lambda p: [sum(p)] ).collect()


  Types of RDDs
  -------------
	-> Generaric RDDs : RDD[U]
	   Pair RDDs : RDD[(K, V)]


  6. mapValues		P: U -> V
			Applied only on Pair RDD
			Applies the function only to the 'value' part of the key-value pairs

		rddPairs.mapValues(lambda x: x*10).collect()
		-> In the above fn, x represents only the 'value' part of key-value pairs


  7. distinct		P: None, Optional: numPartition
			Returns distinct objects of the RDD.

		rddWords.distinct().collect()


  8. sortBy		P: U -> V,  Optional: ascending (True/False), numPartitions
			Sorts the objects of RDD based on the function output that they generate.

		rdd1.sortBy(lambda x: x%2).glom().collect()
		rdd1.sortBy(lambda x: x%2, False).glom().collect()
		rdd1.sortBy(lambda x: x%2, True, 2).glom().collect()


  9. groupBy		P: U -> V, Optional: numPartitions
			Returns a Pair RDD where
				key: Each unique value of the function output
				value: ResultIterable object contained grouped values

			CAUTION: Avoid groupByKey if possible.

		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 12) \
        		.flatMap(lambda x: x.split()) \
        		.groupBy(lambda x: x) \
        		.mapValues(len) \
        		.sortBy(lambda x: x[1], False, 1)
			

  10. partitionBy	P: numPartitions, Optional: partition-function (default: hash)
			Applied only on Pair RDDs
			Controls which keys go to which partition based on partition-function applied to keys.
           

  		rdd4 = rdd1.map(lambda x: (x, 1)).partitionBy(3).map(lambda x: x[0])

	
  ..ByKey Transformations
  -----------------------
	=> Are wide transformations
	=> Applied on Pair RDDs only
	

  11. sortByKey	P: None, Optional: ascending (True/False), numPartitions
			Sorts the data based on the keys.

		rddPairs.sortByKey().glom().collect()
		rddPairs.sortByKey(False).glom().collect()
		rddPairs.sortByKey(False, 3).glom().collect()


  12. groupByKey	P: None, Optional: numPartitions
			Returns a Pair RDD where:
				key: Unique keys of the RDD
				value: ResultIterable. Grouped values

			CAUTION: do not use groupByKey if possible. 

		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 1) \
        		.flatMap(lambda x: x.split()) \
        		.map(lambda x: (x, 1)) \
        		.groupByKey() \
        		.mapValues(sum) \
        		.sortBy(lambda x: x[1], False)


  13. reduceByKey	P: (U, U) -> U
			Reduce all the values of each key by iterativly applying the reduce function.
			
		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 1) \
        		.flatMap(lambda x: x.split()) \
        		.map(lambda x: (x, 1)) \
        		.reduceByKey(lambda x, y: x + y) \
        		.sortBy(lambda x: x[1], False)


  14. repartition	P: numPartitions
			Is used to increase or decrease the number of output partitions	
			Global shuffle	

			
		rdd2 = rdd1.repartition(5)



  15. coalesce		P: numPartitions
			Is used to only decrease the number of output partitions	
			Partition merging


		rdd2 = rdd1.coalesce(5)


		Recommendations
		---------------
		-> The size of each partition should be between 100 MB to 1 GB
		   (Ideally 128 MB if you are running on Hadoop)
		-> The number of partitions should be a multiple of number of cores
		-> The number of cores per executor should be 5


  RDD Actions
  ------------

  1. collect

  2. count

  3. saveAsTextFile

  4. reduce		P: (U, U) -> U
			Reduces an entire RDD to one object by iterativly applying the function, first on
			each partition, and then, across the reduced values of all partitions. 	
		
		rdd1									
	     	P0: 1, 2, 4, 3, 5 -> reduce -> 5 -> reduce -> 9
		P1: 9, 8, 7, 9, 7 -> reduce -> 9
		P2: 1, 3, 2, 1, 3 -> reduce -> 3

		rdd1.reduce(lambda x, y: x + y)
		rdd1.reduce(max)

  5. take

  6. takeOrdered

  7. takeSample

  8. countByValue

  9. countByKey

  10. foreach


   Case Study
   ------------
		
   dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

   From cars.tsv dataset, get the average weight of all the models of each make of American origin cars. 
   -> Arrange in the DESC order of average weight. 
   -> Save the output as a single text file. 

   => Try it yourself


  Spark-Submit Command
  ====================

     -> Is a single command to submit any spark application (Python, Scala, Java, R) to any cluster
        manager (local, spark standalone, YARN, mesos, kubernetes)

 	spark-submit [options] <app jar | python file | R file> [app arguments]

	spark-submit --master yarn \
		--deploy-mode cluster \
		--driver-memory 2G \
		--executor-memory 10G \
		--driver-cores 2 \
		--executor-cores 5 \
		--num-executors 10 \
		E:\PySpark\wordcount.py [app args]

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wcout 2


  Spark Closure
  =============

	In spark, a closure constitute all the code (variables and methods) that must be visible inside an
        executor for the task to perform their computations on the RDD partitions. 

	Spark serializes the closure and separate copy of it is sent to every executor. 


	c = 0

	def isPrime(n):
	  return True if n is Prime
	  return False if n is not Prime

	def f1(n):
	  global c
	  if (isPrime(n)):
		c += 1
	  return n*2

	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(c)   // 0


        Limitation of closure: The local variables can not be used to implement global counters, as these varibale
	that are part of the closure and will get separate copies at every executors. 
	
	Solution: Use "Accumulator" varibale.


  Shared Variables
  ================

   1. Accumulator variable

	-> Is a shared variable, not part of a closure
	-> Not a local copy
	-> One variable maintained by the driver
	-> All tasks can add to this variable (the driver copy is updated)
	-> Is used to implement "global counters"


	c = sc.accumulator(0)

	def isPrime(n):
	  return True if n is Prime
	  return False if n is not Prime

	def f1(n):
	  global c
	  if (isPrime(n)):
		c.add(1)
	  return n*2

	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(c)   // 170



   2. Broadcast variable

	-> Not part of closure, hence not a local copy at task level
        -> A single copy of the broadcast varibale is cache	d on every executor node
	-> All tasks can read from this one copy (per node)
	-> You can broadcast large immutable collection to optimize memory usage.


	d = sc.broadcast({1:a, 2:b, 3:c, 4:d, 5:e, 6:f, 7:g, .....})   #100MB

	def f1(n):
	   global d
	   return d.value[n]
	
	rdd1 = sc.parallelize( range(1, 400), 4 )
	rdd2 = rdd1.map(f1)
	rdd2.collect()	 


  ==============================
     Spark SQL
  ==============================

     -> Spark's structured data processing API
     -> Built on top of Spark Core API
     -> Main API for Spark.

	File Formats: Parquet (default), ORC, JSON, CSV (delimited text), Text


    SparkSession
    ------------
	-> Starting point of execution
	-> Represents a user session (SparkSession) running inside an application (SparkContext)
	-> Each SparkSession can have its own configuration

	spark = SparkSession \
    		.builder \
    		.appName("Basic Dataframe Operations") \
    		.config("spark.master", "local") \
    		.getOrCreate()

   DataFrame
   ---------

	-> Collection of distributed in-memory partitions
	-> Immutable
	-> Lazily Evaluated

	-> DataFrame contains two components:

		-> Data    : Collection of 'Row' object
		-> Schema  : StructType object

		StructType(
		   [
			StructField('age', LongType(), True), 
			StructField('gender', StringType(), True), 
			StructField('name', StringType(), True), 
			StructField('phone', StringType(), True), 
			StructField('userid', LongType(), True)
		   ]
	        )
	       

  Basic steps in creating a Spark SQL Application
  -----------------------------------------------
	
	
	1. Create a DataFrame from some external / programmatic data. 

		inputPath = "E:\\PySpark\\data\\users.json"
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

		df1.show()
		df1.printSchema()

	
	2. Transform the DF using DF transformation methods or using SQL


		Using DF transformation
		-----------------------

		df2 = df1.select("userid", "name", "age", "gender", "phone") \
        		.where("age is not null") \
        		.orderBy("gender", "age") \
        		.groupBy("age").count() \
        		.limit(4)


		Using SQL
		---------
		df1.createOrReplaceTempView("users")
		spark.catalog.listTables()

		qry = """select age, count(*) as count
			from users
			where age is not null
			group by age
			order by age
			limit 4"""
					
		df3 = spark.sql(qry)
		df3.show()
		

	3. Save the dataframe into some external destination (such as files/databases/hive etc)

		df3.write.format("json").save(outputPath)
		df3.write.save(outputPath, format="json")
		df3.write.json(outputPath)

		df3.write.json(outputPath, mode="overwrite")
		
	
  SaveModes
  ---------	
    -> Control the behaviour when saving a DF into an existing directory.

	1. errorIfExists (default)
	2. ignore
	3. append
	4. overwrite

	df3.write.mode("overwrite").json(outputPath)
	df3.write.json(outputPath, mode="overwrite")


  LocaltempView & GlobalTempView
  -------------------------------	
   LocalTempView 
       -> Local to a specific SparkSession
       -> Created using createOrReplaceTempView command
		df1.createOrReplaceTempView("users")

   GlobalTempView
       -> Can be accessed from multiple SparkSessions within the application
       -> Tied to "global_temp" database
       -> Created using createOrReplaceGlobalTempView command
		df1.createOrReplaceGlobalTempView ("gusers")


  DF Transformations
  ------------------

 1. select

	df2 = df1.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME", "count")


	df2 = df1.select( col("ORIGIN_COUNTRY_NAME").alias("origin"),
                  col("DEST_COUNTRY_NAME").alias("destination"),
                  expr("count").cast("int"),
                  expr("count + 10 as newCount"),
                  expr("count > 200 as highFrequecy"),
                  expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic"))


  2. where / filter

	df3 = df2.where("domestic = false and count > 1000")
	df3 = df2.filter("domestic = false and count > 1000")

	df3 = df2.filter( col("count") > 1000 )


  3. orderBy / sort

	df3 = df2.orderBy("count", "origin")
	df3 = df2.orderBy(desc("count"), asc("origin"))
	df3 = df2.sort(desc("count"), asc("origin"))


  4. groupBy  => returns a pyspark.sql.group.GroupedData object. Use an aggregation function to return a DataFrame.
	
	df3 = df2.groupBy("highFrequecy", "domestic").count()
	df3 = df2.groupBy("highFrequecy", "domestic").sum("count")
	df3 = df2.groupBy("highFrequecy", "domestic").max("count")
	df3 = df2.groupBy("highFrequecy", "domestic").avg("count")

	df3 = df2.groupBy("highFrequecy", "domestic") \
        	.agg( count("count").alias("count"),
              		sum("count").alias("sum"),
              		max("count").alias("max"),
              		round(avg("count"), 2).alias("avg")
            	  )

  5. limit

	df2 = df1.limit(10)


  6. selectExpr

	df2 = df1.selectExpr("DEST_COUNTRY_NAME as destination", 
                     "ORIGIN_COUNTRY_NAME as origin",
                     "count",
                     "count + 10 as newCount",
                     "count > 200 as highFrequency",
                     "DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic")

	df2.show()
	df2.printSchema()

  7. withColumn & withColumnRenamed

	df3 = df1.withColumn("newCount", expr("count + 10")) \
         	.withColumn("highFrequency", expr("count > 200")) \
         	.withColumn("domestic", expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME")) \
         	.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
         	.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin") \
            	.withColumn("countryCode", lit(91))

	df3.show(5)

        ---------------------

	df4 = df3.withColumn("ageGroup", when(expr("age < 13"), "child")
                                .when(expr("age < 20"), "teenager")
                                .when(expr("age < 60"), "adult")
                                .otherwise("senior") )

	df4.show()


   8.  udf (user defined function)

		def getAgeGroup( age ):
			if (age <= 12):
				return "child"
			elif (age >= 13 and age <= 19):
				return "teenager"
			elif (age >= 20 and age < 60):
				return "adult"
			else:
				return "senior"

		get_age_group = udf(getAgeGroup, StringType())

		df4 = df3.withColumn("ageGroup", get_age_group(col("age")) )

		df4.show()

		----------------------------------------

		@udf (returnType = StringType())
		def getAgeGroup( age ):
			if (age <= 12):
				return "child"
			elif (age >= 13 and age <= 19):
				return "teenager"
			elif (age >= 20 and age < 60):
				return "adult"
			else:
				return "senior"

		df4 = df3.withColumn("ageGroup", getAgeGroup(col("age")) )
	
		--------------------------------------------
		# applying UDF in SQL

		def getAgeGroup( age ):
			if (age <= 12):
				return "child"
			elif (age >= 13 and age <= 19):
				return "teenager"
			elif (age >= 20 and age < 60):
				return "adult"
			else:
				return "senior"


		spark.udf.register("get_age_group", getAgeGroup, StringType())
		
		spark.catalog.listFunctions()

		qry = "select id, name, age, get_age_group(age) as ageGroup from users"

		df5 = spark.sql(qry)
		df5.show()


  9. drop	=> drops/excludes the specified columns


		df3 = df2.drop("newCount", "highFrequency")

		df3.printSchema()
		df3.show(4)


   10. dropna	=> drops the Rows that has null values in any column or specified columns

		usersDf = spark.read.json("E:\\PySpark\\data\\users.json")
		usersDf.show()

		df3 = usersDf.dropna()  # drop rows if there is null in any column
		df3 = usersDf.dropna(subset=["phone", "age"])  # drop rows if there is null in phone or age columns

		df3.show()


   11. dropDuplicates => drop duplicates based on any column or specified columns


		listUsers = [(1, "Raju", 5),
					 (1, "Raju", 5),
					 (3, "Raju", 5),
					 (4, "Raghu", 35),
					 (4, "Raghu", 35),
					 (6, "Raghu", 35),
					 (7, "Ravi", 70)]

		df3 = spark.createDataFrame(listUsers, ["id", "name", "age"])
		df3.show()

		df4 = df3.dropDuplicates()
		df4 = df3.dropDuplicates(["name","age"])

		df4.show()

   12. distinct => returns distinct rows

		listUsers = [(1, "Raju", 5),
					 (1, "Raju", 5),
					 (3, "Raju", 5),
					 (4, "Raghu", 35),
					 (4, "Raghu", 35),
					 (6, "Raghu", 35),
					 (7, "Ravi", 35)]


		df3 = spark.createDataFrame(listUsers, ["id", "name", "age"])
		df3.show()

		df4 = df3.distinct()
		df4.show() 


		# How many unique DEST_COUNTRY_NAMEs are there ??
		df1.select("DEST_COUNTRY_NAME").distinct().count()
		df1.dropDuplicates(["DEST_COUNTRY_NAME"]).count()	


   13. randomSplit

		dfList = df1.randomSplit([0.5, 0.5], 464)
		print( dfList[0].count(), dfList[1].count() )


   14. sample

		df2 = df1.sample(True, 0.65)		# with-replacement sampling
		df2 = df1.sample(True, 1.65)		# fraction > 1 is allowed
		df2 = df1.sample(True, 0.65, 42342)	# 42342 is a seed

		df2 = df1.sample(False, 0.65)		# without-replacement sampling
		df2 = df1.sample(False, 1.65)		# ERROR: fraction > 1 is NOT allowed
		df2 = df1.sample(False, 0.65, 42342)    # 42342 is a seed


   15. union, intersect, subtract

		df4 = df2.union(df3)
		df4.show()
		df4.count()
		df4.rdd.getNumPartitions()
		df4.write.mode("overwrite").csv("E:\\PySpark\\output\\union")

		df5 = df4.intersect(df3)
		df5.show()
		df5.rdd.getNumPartitions()

		df6 = df4.subtract(df3)
		df6.show()
		df6.rdd.getNumPartitions()


    spark.sql.shuffle.partitions
    ----------------------------
	The default number of partitions of a dataframe created as a result of shuffle operation
	is determined by the 'spark.sql.shuffle.partitions' configuration value of the sparksession.

		spark.conf.get("spark.sql.shuffle.partitions")
		spark.conf.set("spark.sql.shuffle.partitions", "5")


   16. repartition
	 -> Is used to increase or decrease the number of partitions of the output DF
	 -> Causes global shuffle

		df2 = df1.repartition(6)
		df2.rdd.getNumPartitions()

		df3 = df2.repartition(3)
		df3.rdd.getNumPartitions()

		df4 = df2.repartition(3, col("DEST_COUNTRY_NAME"))
		df4.rdd.getNumPartitions()

		df4 = df2.repartition(col("DEST_COUNTRY_NAME"))
		df4.rdd.getNumPartitions()

   17. coalesce
	 -> Is used to only decrease the number of partitions of the output DF
	 -> Causes partition merging

		df5 = df2.coalesce(3)
		df5.rdd.getNumPartitions()

   18. join  => is discussed separatly



  Working with different file formats
  -----------------------------------

  JSON
	read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

	write
		df3.write.format("json").save(outputPath)
		df3.write.save(outputPath, format="json")
		df3.write.json(outputPath)	

  Parquet (default)
	read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.load(inputPath, format="parquet")
		df1 = spark.read.parquet(inputPath)

	write
		df3.write.format("parquet").save(outputPath)
		df3.write.save(outputPath, format="parquet")
		df3.write.parquet(outputPath)


  ORC
	read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.load(inputPath, format="orc")
		df1 = spark.read.orc(inputPath)

	write
		df3.write.format("orc").save(outputPath)
		df3.write.save(outputPath, format="orc")
		df3.write.orc(outputPath)	


  CSV (delimited text)

	read
		df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputPath)
		df1 = spark.read.format("csv").load(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")

	write
		df3.write.format("csv").save(outputPath, header=True)
		df2.write.csv(outputPath, header=True)
		df2.write.csv(outputPath, header=True, sep="|", mode="overwrite")


  Text
	read
		df1 = spark.read.text(inputPath)

	write
		df1.write.text(outputPath)



  Creating an RDD from DataFrame
  ------------------------------
	rdd1 = df1.rdd


  Creating a DataFrame from an RDD
  --------------------------------
	listUsers = [(1, "Raju", 5),
		(2, "Ramesh", 15),
		(3, "Rajesh", 18),
		(4, "Raghu", 35),
		(5, "Ramya", 25),
		(6, "Radhika", 35),
		(7, "Ravi", 70)]

	rdd1 = spark.sparkContext.parallelize(listUsers)

	df1 = rdd1.toDF(["id", "name", "age"])

	df1.show()
	df1.printSchema()


  Creating a DataFrame from programmatic data
  --------------------------------------------
	listUsers = [(1, "Raju", 5),
		(2, "Ramesh", 15),
		(3, "Rajesh", 18),
		(4, "Raghu", 35),
		(5, "Ramya", 25),
		(6, "Radhika", 35),
		(7, "Ravi", 70)]

	df1 = spark.createDataFrame(listUsers, ["id", "name", "age"])
	df1 = spark.createDataFrame(listUsers).toDF("id", "name", "age")

  
  Create a DataFrame with programmatic schema
  -------------------------------------------

	mySchema = "id INT, name STRING, age INT"
	df1 = spark.createDataFrame(listUsers, schema=mySchema)

	------------------------

	mySchema = StructType([
            StructField("id", IntegerType(), True),
            StructField("name", StringType(), True),
            StructField("age", IntegerType(), True)
        ])
	df1 = spark.createDataFrame(listUsers, schema=mySchema)
   
	--------------------

	mySchema = StructType([
            StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
            StructField("DEST_COUNTRY_NAME", StringType(), True),
            StructField("count", IntegerType(), True)
        ])

	df1 = spark.read.schema(mySchema).json(inputPath)



   Use Case
   --------
	Datasets: movies.csv & ratings.csv
	URL: https://github.com/ykanakaraju/pyspark/tree/master/data_git/movielens

	From movies & ratings files, find the top 10 movies with highest average user rating
	-> Consider only those movies that are rated by atleast 50 user
	-> data:  movieId, title, totalRatings, averageRating
	-> Sort the data in the DESC order of averageRating
	-> Save the data as a single pipe-separated CSV file with header. 

	Use only DF transformation methods approach (not SQL)

	=> Try it yourself..





   






































