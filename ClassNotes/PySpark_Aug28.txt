
 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
   Spark Streaming
	-> Structured Streaming


  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

 ============================================================
    
    Spark
    ------	
	-> Is a unified in-memory distributed computing framework for big data analytics.  

    	-> Spark is written Scala programming language.
	
	in-memory: ability to persist intermediate results and subsequent operations
		   can directly work on these persisted intermediate results. 

	-> Spark is a unified framework

	-> Spark is a polyglot
		-> Supports Scala, Java, Python, R

	-> Spark can run on multiple cluster managers
		local, spark standalone scheduler, YARN, Mesos, Kubernetes. 		


   Spark Unified Framework
   -----------------------

	Spark provides a consistent set of APIs for performing different analytics workloads
        using the same execution engine and some well defined data abstractions and operations.

   	Batch Analytics of unstructured data	-> Spark Core API (low-level api)
	Batch Analytics of structured data 	-> Spark SQL
	Streaming Analytics (real time)		-> Spark Streaming, Structured Streaming
	Predictive analytics (ML)		-> Spark MLLib
	Graph parallel computations  		-> Spark GraphX


 
   Getting started with Spark
   --------------------------
	
	1. Working in you vLab

	2. Setting up PySpark dev environment on your personal machine.

		Create a Python v-env
			cd <dir>
	     		python -m venv pyspark-venv
		Activate the v-env
			cd <dir>\pyspark-venv\Scripts
			activate
		Install PySpark
			pip install pyspark==3.4.1


	     => If the above approach is not working, you can also follow
                the instrauctions given in the shared document.
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf


		-> Install Anaconda distribution and then follow the steps.

	3. Signup to databricks community edition.

		Signup: https://www.databricks.com/try-databricks
		   -> Fill the details with valid email address.
		   -> In the next screen, click on 'Get started with Community Edition' link
		      (Do not click on 'Continue')

		Login: https://community.cloud.databricks.com/login.html


   Spark Architecture
   ------------------

    	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver. 


   RDD (Resilient Distributed Dataset)
   -----------------------------------
	
    -> RDD is the fundamental data abstraction in Spark Core API
	
    -> RDD is a collection of distributed in-memory partitions.
	-> Each partition is a collection of objects of some type.

    -> RDDs are immutable

    -> RDDs are lazily evaluated
	-> Transformations does not cause execution
	-> Action commands cause execution. 

	
  Creating RDDs
  -------------
	
    Three ways:

	1. Create an RDD from external data files.

		rddFile = sc.textFile( <filePath> , 4 )

		default Number of partitions: sc.defaultMinPartition
		  sc.defaultMinPartition = 2, if number of cores >= 2
					   1, otherwise

	2. Create an RDD from programmatic data

		rdd1 = sc.parallelize([2,4,3,1,5,6,7,8,6,8,9,0,7,5,4,6,8], 2)
		
		default Number of partitions: sc.defaultParallelism
		sc.defaultParallelism = number of CPU cores allocated.


	3. By applying transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x*10)


  RDD Operations
  --------------

	Two types of operations

	1. Transformations
		-> Transformations return RDDs
		-> Transformations does not cause execution of RDDs
		-> Cause lineage DAGs to be created

	2. Actions
		-> Triggers execution of RDDs
		-> Produces output
		

  RDD Lineage DAG
  ---------------
  Driver maintains a Lineage DAG for every RDD
  Lineage DAG is a heirarchy of dependencies of RDDs all the way upto th very first RDD	
  Lineage DAG is a logical plan on how to create the RDD.
    
     rddFile = sc.textFile(<filePath>, 4)	
     Lineage of rddFile: (4) rddFile -> textFile on <filePath>
	
     rddWords = rddFile.flatMap(lambda x: x.split(" "))
     Lineage of rddWords: (4) rddWords -> rddFile.flatMap -> textFile on <filePath>
  
     rddPairs = rddWords.map(lambda x: (x, 1))
     Lineage of rddPairs: (4) rddPairs -> rddWords.map -> rddFile.flatMap -> textFile on <filePath>

     rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
     Lineage of rddWc: (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> textFile on <filePath>


  Types of RDD Transformation
  ---------------------------

	Two types:

	 1. Narrow Transformations

           -> Narrow transformations are those, where the computation of each partition depends ONLY
              on its input partition.
           -> There is no shuffling of data.
           -> Simple and efficient


      	2. Wide Transformations

           -> In wide transformations, the computation of a single partition depends on all/many
              partitions of its input RDD.
           -> Data shuffle across partitions will happen.
           -> Complex and expensive


  RDD Persistence
  ---------------

	rdd1 = sc.textFile(<File>, 4)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.DISK_ONLY )      --> instruction to spark to save rdd6 partition
	rdd7 = rdd6.t7(...)

	rdd6.collect()
	lineage of rdd6: (4) rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[textFile, t3, t5, t6] -> collected

	rdd7.collect()
	lineage of rdd6: (4) rdd7 -> rdd6.t7
		[t7] -> collected

	rdd6.unpersist()


       	Storage Levels
       	--------------	
	MEMORY_ONLY		=> default, Memory Serialized 1x replicated
	MEMORY_AND_DISK		=> Disk Memory Serialized 1x replicated
	DISK_ONLY		=> Disk Serialized 1x replicated
	MEMORY_ONLY_2		=> Memory Serialized 2x replicated	
	MEMORY_AND_DISK_2	=> Disk Memory Serialized 2x replicated	

	Commands
	--------	
	rdd1.cache()     -> in-memory persistence
	rdd1.persist()	 -> in-memory persistence
	rdd1.persist( StorageLevel.DISK_ONLY )

	rdd1.unpersist()


   Spark Executor Memory Structure
   -------------------------------
     
      Let us say we are requesting an executor with 10 GB RAM. 
      The spark job will be allocated executors with 10.3 GB (10GB + 300MB) RAM. 
     
      
      1. Reserved Memory (300 MB)
           -> Spark's internal usage
      
      2. Spark Memory (spark.memory.fraction: 0.6)   => 6 GB (Unified Memory)
          
         2.1  Execution Memory
                 -> Used for RDD partition creationg and transformation
                 -> Can forcibly evict RDD partitions from storage memory if it requires
                    additional upto the quote allocated for it.

         2.2  Storage Memory (spark.memory.storageFraction: 0.5) => 3 GB
                 -> The RDD partitions and broadcast variables are persisted
                    in this memory.

      3. User Memory  => 4 GB
         -> Running non-spark related code execution. 
         -> Related to running python methods, storing python collection.

   
  RDD Transformations
  -------------------

  1. map		P: U -> V
			Object to Object transformation
			input RDD: N objects, output RDD: N objects
			
	 rdd2 = rdd1.map(lambda x: (x%5, x))


  2. filter		P: U -> Boolean
			Objects for which the function returns True will be in the output RDDs
			input RDD: N objects, output RDD: <= N objects

	rddFile.filter(lambda x: len(x.split(" ")) > 8).collect()

  3. glom		P: None
			Return one list object per partition with all the objects of that partition.

		rdd1			rdd2 = rdd1.glom()
		P0: 3,2,1,4,5,6 -> glom -> P0: [3,2,1,4,5,6]
		P1: 5,6,7,8,9,3 -> glom -> P1: [5,6,7,8,9,3]
		P2: 4,7,6,2,1,0 -> glom -> P2: [4,7,6,2,1,0]

		rdd1.count() = 18 (int)	   rdd2.count() = 3 (list)

  4. flatMap		P: U -> Iterable[V]
			fatMap flattens the iterables produced by the function.

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


  5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation

		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p) ).collect()
		rdd1.mapPartitions(lambda p: [sum(p)] ).collect()


  6. mapPartitionsWithIndex   P: Int, Iterable[U] -> Iterable[V]
			similat to mapPartitions, but we get the partition index as an additional parameter.

		rdd1 \
		.mapPartitionsWithIndex(lambda i,p: map(lambda x: (i, x), p)) \
		.filter(lambda x: x[0] == 1) \
		.map(lambda x: x[1]) \
		.collect()

  7. distinct		P: None, Optional: numPartitions
			Returns distinct objects of the RDD.

		rddWords.distinct().collect()


  Types of RDDs
  -------------

	Two types of RDDs:

	Generic RDD : RDD[U]
	Pair RDD :    RDD[(K, V)]


  8. mapValues		P: U -> V
			Applied only on Pair RDD
			Applies the function only to the 'value' part of the key-value pairs

		rddPairs.mapValues(lambda x: x*10).collect()
		-> In the above fn, x represents only the 'value' part of key-value pairs


  9. sortBy		P: U -> V,  Optional: ascending (True/False), numPartitions
			Sorts the objects of RDD based on the function output that they generate.

		rdd1.sortBy(lambda x: x%2).glom().collect()
		rdd1.sortBy(lambda x: x%2, False).glom().collect()
		rdd1.sortBy(lambda x: x%2, True, 2).glom().collect()


  10. groupBy		P: U -> V, Optional: numPartitions
			Returns a Pair RDD where
				key: Each unique value of the function output
				value: ResultIterable object contained grouped values

		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 12) \
        		.flatMap(lambda x: x.split()) \
        		.groupBy(lambda x: x) \
        		.mapValues(len) \
        		.sortBy(lambda x: x[1], False, 1)


  11. randomSplit	P: List of weights (ex: [0.6, 0.4]), Optional: seed
			Splits the RDD into multiple RDDs (randomly) in the specified weights

		rddList = rdd1.randomSplit([0.6, 0.4],)
		rddList = rdd1.randomSplit([0.6, 0.4], 4641)

  
  12. repartition	P: numPartitions
			Is used to increase or decrease the number of output partitions	
			Global shuffle	

			
		rdd2 = rdd1.repartition(5)



  13. coalesce		P: numPartitions
			Is used to only decrease the number of output partitions	
			Partition merging


		rdd2 = rdd1.coalesce(5)


		Recommendations
		---------------
		-> The size of each partition should be between 100 MB to 1 GB
		   (Ideally 128 MB if you are running on Hadoop)
		-> The number of partitions should be a multiple of number of cores
		-> The number of cores per executor should be 5


  14. union, intersection, subtract, cartesian

		-> Operate on two generic RDDs.

		Assume rdd1 has M partitions and rdd2 has N partitions

		command				output
		----------------------------------------------
		rdd1.union(rdd2)		M+N, narrow
		rdd1.intersection(rdd2)		M+N, wide
		rdd1.subtract(rdd2)		M+N, wide
		rdd1.cartesian(rdd2)		M*N, wide


  15. partitionBy	P: numPartitions, Optional: partition-function (default: hash)
			Applied only on Pair RDDs
			Controls which keys go to which partition based on partition-function applied to keys.
           

  		rdd4 = rdd1.map(lambda x: (x, 1)).partitionBy(3).map(lambda x: x[0])

	
  ..ByKey Transformations
  -----------------------
	=> Are wide transformations
	=> Applied on Pair RDDs only
	

  16. sortByKey	P: None, Optional: ascending (True/False), numPartitions
			Sorts the data based on the keys.

		rddPairs.sortByKey().glom().collect()
		rddPairs.sortByKey(False).glom().collect()
		rddPairs.sortByKey(False, 3).glom().collect()


  17. groupByKey	P: None, Optional: numPartitions
			Returns a Pair RDD where:
				key: Unique keys of the RDD
				value: ResultIterable. Grouped values

			CAUTION: do not use groupByKey if possible. 

		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 1) \
        		.flatMap(lambda x: x.split()) \
        		.map(lambda x: (x, 1)) \
        		.groupByKey() \
        		.mapValues(sum) \
        		.sortBy(lambda x: x[1], False)


  18. reduceByKey	P: (U, U) -> U
			Reduce all the values of each key by iterativly applying the reduce function.
			
		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 1) \
        		.flatMap(lambda x: x.split()) \
        		.map(lambda x: (x, 1)) \
        		.reduceByKey(lambda x, y: x + y) \
        		.sortBy(lambda x: x[1], False)


  19. aggregateByKey	Aggregates all the values of each key to a type which is different then the type of RDD objects
			
			Three parameters:

			1. zero-value : starting value of the type of final output you want to produce
			2. sequence fn : Operate on each partition
					Iterativly merges all the values of the partition with the zero-value
			3. combine fn : Reduces all the per-partition outputs generated by seq-fn.

			Optional: numPartitions	


		student_rdd = sc.parallelize([
  			("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  			("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  			("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  			("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  			("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  			("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  			("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
      
		student_rdd.glom().collect()

		output_rdd = student_rdd \
                		.map(lambda t: (t[0], t[2])) \
                		.aggregateByKey( (0,0),
                                 	lambda z, v: (z[0]+v, z[1]+1),
                                 	lambda x, y: (x[0]+y[0], x[1]+y[1])) \
                		.mapValues(lambda p: p[0]/p[1])


  20. joins => join (inner), leftOuterJoin, rightOuterJoin, fullOuterJoin

		-> Operate on two pair RDDs

		RDD[(K, V)].join( RDD[(K, W)] ) => RDD[(K, (V, W))]


		names1 = sc.parallelize(["harsha", "aditya", "raju", "veer"]).map(lambda a: (a, 1))
		names2 = sc.parallelize(["amrita", "raju", "veer", "anita"]).map(lambda a: (a, 'Hi'))

		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)
		
		

  21. cogroup 		


  RDD Actions
  -----------

   1. collect

   2. count

   3. saveAsTextFile

   4. reduce		P: (U, U) -> U
			Reduces an entire RDD to one object by iterativly applying the function first on
			each partition and then across the reduced values of all partitions. 	
		
		rdd1									
	     	P0: 1, 2, 4, 3, 5, 7, 6, 8, 7, 0    -> reduce -> 43 -> reduce -> 153
		P1: 9, 8, 7, 9, 7, 9, 6, 8, 4, 2    -> reduce -> 69
		P2: 1, 3, 2, 1, 3, 5, 9, 0, 9, 0, 8 -> reduce -> 41

		rdd1.reduce(lambda x, y: x + y)

		rddWc.reduce(lambda x, y: (x[0] + "," + y[0], x[1] + y[1]) )


  5. aggregate		Three parameters
			
			1. zero-value : starting value of the type of final output you want to produce
			2. sequence fn : Operate on each partition
					Iterativly merges all the values of the partition with the zero-value
			3. combine fn : Reduces all the per-partition outputs generated by seq-fn.

	rdd1.aggregate( (0,0), lambda z,v:(z[0]+v, z[1]+1), lambda x,y: (x[0]+y[0], x[1]+y[1]) )
	
  
  6. take(n)

		rdd1.take(10)  -> returns a list of first 10 objects


  7. takeOrdered(n, [fn])

		rddWords.takeOrdered(20)
		rddWords.takeOrdered(20, len)


  8. takeSample(withReplacement, n, [seed])

		with-replacement sampling
			rdd1.takeSample(True, 10)
			rdd1.takeSample(True, 10, 45645)    # 45645 is a seed

		without-replacement sampling
			rdd1.takeSample(False, 10)
			rdd1.takeSample(False, 10, 45645)   # 45645 is a seed


  9. countByValue


  10. countByKey

  11. foreach  => P: function;
		  Returns nothing. 
		  Executes the function on all objects of the RDD.

  12. saveAsSequenceFile => Sequence File is Hadoop file format that stores key-value pairs.

	

   Use Case
   ---------
		
	dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

	From cars.tsv dataset, get the average weight of all the models of each make 
	of American origin cars. 
	-> Arrange in the DESC order of average weight. 
        -> Save the output as a single text file. 

        => Try it yourself



  Spark Closures
  --------------

	In spark, a closure constitute all the code (variables and methods) that must be visible inside an
        executor to the task to perform their computations on RDD partitions. 

	Spark serializes the closure and separate copy of it is sent to every executor. 


	c = 0

	def isPrime(n):
	  return True if n is Prime
	  else return False

	def f1(n):
	  global c
	  if (isPrime(n)) c += 1
	  return n*2     	

	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(c)     


        Limitation of closure: The local variables can not be used to implement globl counters, as these varibale
	that are part od the closure will separate copies at every executors. 
	
	Solution: Use "Accumulator" varibale.


  Distributed Shared Variables
  ----------------------------

    1. Accumulator
	-> Is a shared variable, not part of a closure
	-> Not a local copy
	-> One variable maintained by the driver
	-> All tasks can add to this variable (the driver copy is updated)
	-> Is used to implement "global counter"


	c = sc.accumulator(0)

	def isPrime(n):
	  return True if n is Prime
	  else return False

	def f1(n):
	  global c
	  if (isPrime(n)) c.add(1)
	  return n*2     	

	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print( c.value )    # 80 


  2. Broadcast variable

	d = {1: a, 2: b, 3: c, 4: d, 5: e, 6: f, ....}   # 100 MB
	bc = sc.broadcast(d)

	def f1(n):
	   global bc
	   return bc.value[n]

	rdd1 = sc.parallelize( [1,2,3,4,5,......], 4)

	rdd2 = rdd1.map( f1 )

	rdd2.collect()   => a,b,c,d,e,...



  Spark-Submit Command
  ====================

     -> Is a single command to submit any spark application (Python, Scala, Java, R) to any cluster
        manager (local, spark standalone, YARN, mesos, kubernetes)

 	spark-submit [options] <app jar | python file | R file> [app arguments]

	spark-submit --master yarn \
		--deploy-mode cluster \
		--driver-memory 2G \
		--executor-memory 10G \
		--driver-cores 2 \
		--executor-cores 5 \
		--num-executors 10 \
		E:\PySpark\wordcount.py [app args]

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt putcountout 2


  --------------------------------
      Spark SQL (pyspark.sql)
  --------------------------------
  
    -> High Level API built on top of Spark Core
    -> Spark's structured data processing API

           File Formats : Parquet (default), ORC, JSON, CSV (delimited text), Text
	   JDBC Format  : RDBMS, NoSQL
	   Hive Format  : Hive Warehouse


    SparkSession
    ------------
	-> Starting point of execution
	-> Represents a user session (SparkSession) running inside an application (SparkContext)
	-> Each SparkSession can have its own configuration

	spark = SparkSession \
    		.builder \
    		.appName("Basic Dataframe Operations") \
    		.config("spark.master", "local") \
    		.getOrCreate()


   DataFrame (DF)
   ---------
	-> Main data abstraction of Spark SQL

	-> Is a collection of distributed in-memory partitions
	-> Immutable
	-> Lazily evaluated. 

	-> DataFrame is a collection of "Row" objects.

	-> DataFrame contains two components:
		-> Data    : Collection of 'Row' object
		-> Schema  : StructType object

		StructType(
		   List(
			StructField(age,LongType,true),
			StructField(gender,StringType,true),
			StructField(name,StringType,true),
			StructField(phone,StringType,true),
			StructField(userid,LongType,true)
		   )
		)


  
  Basic steps in creating a Spark SQL Application
  -----------------------------------------------
	
	0. Create a Spark Session

		spark = SparkSession \
    			.builder \
    			.appName("Basic Dataframe Operations") \
    			.config("spark.master", "local") \
    			.getOrCreate()


	1. Create a DataFrame from some external / programmatic data. 

		inputPath = "E:\\PySpark\\data\\users.json"
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

		df1.show()
		df1.printSchema()

	
	2. Transform the DF using DF transformation methods or using SQL

		Using DF transformation
		-----------------------

		df2 = df1.select("userid", "name", "age", "gender", "phone") \
        		.where("age is not null") \
        		.orderBy("gender", "age") \
        		.groupBy("age").count() \
        		.limit(4)


		Using SQL
		---------
		df1.createOrReplaceTempView("users")
		spark.catalog.listTables()

		qry = """select age, count(*) as count
			from users
			where age is not null
			group by age
			order by age
			limit 4"""
					
		df3 = spark.sql(qry)
		df3.show()


	3. Save the dataframe into some external destination (such as files/databases/hive etc)
		
		df3.write.format("json").save(outputPath)
		df3.write.save(outputPath, format="json")
		df3.write.json(outputPath)

		df3.write.json(outputPath, mode="overwrite")


  Save Modes
  ----------
    -> Control the behaviour when saving a DF into an existing directory.

	1. errorIfExists (default)
	2. ignore
	3. append
	4. overwrite

	df3.write.mode("overwrite").json(outputPath)
	df3.write.json(outputPath, mode="overwrite")


 LocaltempView & GlobalTempView
 -------------------------------	
	LocalTempView 
	   -> Local to a specific SparkSession
	   -> Created using createOrReplaceTempView command
		df1.createOrReplaceTempView("users")

	GlobalTempView
	   -> Can be accessed from multiple SparkSessions within the application
	   -> Tied to "global_temp" database
	   -> Created using createOrReplaceGlobalTempView command
		df1.createOrReplaceGlobalTempView ("gusers")
	

 DataFrame Transformations
 -------------------------

  1. select

	df2 = df1.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME", "count")


	df2 = df1.select( col("ORIGIN_COUNTRY_NAME").alias("origin"),
                  col("DEST_COUNTRY_NAME").alias("destination"),
                  expr("count").cast("int"),
                  expr("count + 10 as newCount"),
                  expr("count > 200 as highFrequecy"),
                  expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic"))


  2. where / filter

	df3 = df2.where("domestic = false and count > 1000")
	df3 = df2.filter("domestic = false and count > 1000")

	df3 = df2.filter( col("count") > 1000 )


  3. orderBy / sort

	df3 = df2.orderBy("count", "origin")
	df3 = df2.orderBy(desc("count"), asc("origin"))
	df3 = df2.sort(desc("count"), asc("origin"))


  4. groupBy  => returns a pyspark.sql.group.GroupedData object. Use an aggregation function to return a DataFrame.
	
	df3 = df2.groupBy("highFrequecy", "domestic").count()
	df3 = df2.groupBy("highFrequecy", "domestic").sum("count")
	df3 = df2.groupBy("highFrequecy", "domestic").max("count")
	df3 = df2.groupBy("highFrequecy", "domestic").avg("count")

	df3 = df2.groupBy("highFrequecy", "domestic") \
        	.agg( count("count").alias("count"),
              		sum("count").alias("sum"),
              		max("count").alias("max"),
              		round(avg("count"), 2).alias("avg")
            	  )

  5. limit

	df2 = df1.limit(10)


  6. selectExpr

	df2 = df1.selectExpr("DEST_COUNTRY_NAME as destination", 
                     "ORIGIN_COUNTRY_NAME as origin",
                     "count",
                     "count + 10 as newCount",
                     "count > 200 as highFrequency",
                     "DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic")

	df2.show()
	df2.printSchema()


  7. withColumn & withColumnRenamed

	df3 = df1.withColumn("newCount", expr("count + 10")) \
         	.withColumn("highFrequency", expr("count > 200")) \
         	.withColumn("domestic", expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME")) \
         	.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
         	.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin") \
            	.withColumn("countryCode", lit(91))

	df3.show(5)

        ---------------------

	df4 = df3.withColumn("ageGroup", when(expr("age < 13"), "child")
                                .when(expr("age < 20"), "teenager")
                                .when(expr("age < 60"), "adult")
                                .otherwise("senior") )

	df4.show()


   8.  udf (user defined function)

		def getAgeGroup( age ):
			if (age <= 12):
				return "child"
			elif (age >= 13 and age <= 19):
				return "teenager"
			elif (age >= 20 and age < 60):
				return "adult"
			else:
				return "senior"

		get_age_group = udf(getAgeGroup, StringType())

		df4 = df3.withColumn("ageGroup", get_age_group(col("age")) )

		df4.show()

		----------------------------------------

		@udf (returnType = StringType())
		def getAgeGroup( age ):
			if (age <= 12):
				return "child"
			elif (age >= 13 and age <= 19):
				return "teenager"
			elif (age >= 20 and age < 60):
				return "adult"
			else:
				return "senior"

		df4 = df3.withColumn("ageGroup", getAgeGroup(col("age")) )
	
		--------------------------------------------
		# applying UDF in SQL

		def getAgeGroup( age ):
			if (age <= 12):
				return "child"
			elif (age >= 13 and age <= 19):
				return "teenager"
			elif (age >= 20 and age < 60):
				return "adult"
			else:
				return "senior"


		spark.udf.register("get_age_group", getAgeGroup, StringType())
		
		spark.catalog.listFunctions()

		qry = "select id, name, age, get_age_group(age) as ageGroup from users"

		df5 = spark.sql(qry)
		df5.show()


   9. drop	=> drops/excludes the specified columns


		df3 = df2.drop("newCount", "highFrequency")

		df3.printSchema()
		df3.show(4)


   10. dropna	=> drops the Rows that has null values in any column or specified columns

		usersDf = spark.read.json("E:\\PySpark\\data\\users.json")
		usersDf.show()

		df3 = usersDf.dropna()  # drop rows if there is null in any column
		df3 = usersDf.dropna(subset=["phone", "age"])  # drop rows if there is null in phone or age columns

		df3.show()


   11. dropDuplicates => drop duplicates based on any column or specified columns


		listUsers = [(1, "Raju", 5),
					 (1, "Raju", 5),
					 (3, "Raju", 5),
					 (4, "Raghu", 35),
					 (4, "Raghu", 35),
					 (6, "Raghu", 35),
					 (7, "Ravi", 70)]

		df3 = spark.createDataFrame(listUsers, ["id", "name", "age"])
		df3.show()

		df4 = df3.dropDuplicates()
		df4 = df3.dropDuplicates(["name","age"])

		df4.show()

   12. distinct => returns distinct rows

		listUsers = [(1, "Raju", 5),
					 (1, "Raju", 5),
					 (3, "Raju", 5),
					 (4, "Raghu", 35),
					 (4, "Raghu", 35),
					 (6, "Raghu", 35),
					 (7, "Ravi", 35)]


		df3 = spark.createDataFrame(listUsers, ["id", "name", "age"])
		df3.show()

		df4 = df3.distinct()
		df4.show() 


		# How many unique DEST_COUNTRY_NAMEs are there ??
		df1.select("DEST_COUNTRY_NAME").distinct().count()
		df1.dropDuplicates(["DEST_COUNTRY_NAME"]).count()	


   13. randomSplit

		dfList = df1.randomSplit([0.5, 0.5], 464)
		print( dfList[0].count(), dfList[1].count() )


   14. sample

		df2 = df1.sample(True, 0.65)		# with-replacement sampling
		df2 = df1.sample(True, 1.65)		# fraction > 1 is allowed
		df2 = df1.sample(True, 0.65, 42342)	# 42342 is a seed

		df2 = df1.sample(False, 0.65)		# without-replacement sampling
		df2 = df1.sample(False, 1.65)		# ERROR: fraction > 1 is NOT allowed
		df2 = df1.sample(False, 0.65, 42342)    # 42342 is a seed


   15. union, intersect, subtract

		df4 = df2.union(df3)
		df4.show()
		df4.count()
		df4.rdd.getNumPartitions()
		df4.write.mode("overwrite").csv("E:\\PySpark\\output\\union")

		df5 = df4.intersect(df3)
		df5.show()
		df5.rdd.getNumPartitions()

		df6 = df4.subtract(df3)
		df6.show()
		df6.rdd.getNumPartitions()


    spark.sql.shuffle.partitions
    ----------------------------
	The default number of partitions of a dataframe created as a result of shuffle operation
	is determined by the 'spark.sql.shuffle.partitions' configuration value of the sparksession.

		spark.conf.get("spark.sql.shuffle.partitions")
		spark.conf.set("spark.sql.shuffle.partitions", "5")


   16. repartition
	 -> Is used to increase or decrease the number of partitions of the output DF
	 -> Causes global shuffle

		df2 = df1.repartition(6)
		df2.rdd.getNumPartitions()

		df3 = df2.repartition(3)
		df3.rdd.getNumPartitions()

		df4 = df2.repartition(3, col("DEST_COUNTRY_NAME"))
		df4.rdd.getNumPartitions()

		df4 = df2.repartition(col("DEST_COUNTRY_NAME"))
		df4.rdd.getNumPartitions()

   17. coalesce
	 -> Is used to only decrease the number of partitions of the output DF
	 -> Causes partition merging

		df5 = df2.coalesce(3)
		df5.rdd.getNumPartitions()

   18. join  => is discussed separatly


  Working with different file formats
  -----------------------------------

  JSON
	read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

	write
		df3.write.format("json").save(outputPath)
		df3.write.save(outputPath, format="json")
		df3.write.json(outputPath)	

  Parquet (default)
	read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.load(inputPath, format="parquet")
		df1 = spark.read.parquet(inputPath)

	write
		df3.write.format("parquet").save(outputPath)
		df3.write.save(outputPath, format="parquet")
		df3.write.parquet(outputPath)


  ORC
	read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.load(inputPath, format="orc")
		df1 = spark.read.orc(inputPath)

	write
		df3.write.format("orc").save(outputPath)
		df3.write.save(outputPath, format="orc")
		df3.write.orc(outputPath)	


  CSV (delimited text)

	read
		df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputPath)
		df1 = spark.read.format("csv").load(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")

	write
		df3.write.format("csv").save(outputPath, header=True)
		df2.write.csv(outputPath, header=True)
		df2.write.csv(outputPath, header=True, sep="|", mode="overwrite")


  Text
	read
		df1 = spark.read.text(inputPath)

	write
		df1.write.text(outputPath)


  Creating an RDD from DataFrame
  ------------------------------
	rdd1 = df1.rdd


  Creating a DataFrame from an RDD
  --------------------------------
	listUsers = [(1, "Raju", 5),
		(2, "Ramesh", 15),
		(3, "Rajesh", 18),
		(4, "Raghu", 35),
		(5, "Ramya", 25),
		(6, "Radhika", 35),
		(7, "Ravi", 70)]

	rdd1 = spark.sparkContext.parallelize(listUsers)

	df1 = rdd1.toDF(["id", "name", "age"])

	df1.show()
	df1.printSchema()


  Creating a DataFrame from programmatic data
  --------------------------------------------
	listUsers = [(1, "Raju", 5),
		(2, "Ramesh", 15),
		(3, "Rajesh", 18),
		(4, "Raghu", 35),
		(5, "Ramya", 25),
		(6, "Radhika", 35),
		(7, "Ravi", 70)]

	df1 = spark.createDataFrame(listUsers, ["id", "name", "age"])
	df1 = spark.createDataFrame(listUsers).toDF("id", "name", "age")

  
  Create a DataFrame with programmatic schema
  -------------------------------------------

	mySchema = "id INT, name STRING, age INT"
	df1 = spark.createDataFrame(listUsers, schema=mySchema)

	------------------------

	mySchema = StructType([
            StructField("id", IntegerType(), True),
            StructField("name", StringType(), True),
            StructField("age", IntegerType(), True)
        ])
	df1 = spark.createDataFrame(listUsers, schema=mySchema)
   
	--------------------

	mySchema = StructType([
            StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
            StructField("DEST_COUNTRY_NAME", StringType(), True),
            StructField("count", IntegerType(), True)
        ])

	df1 = spark.read.schema(mySchema).json(inputPath)


  DataFrame Joins
  ---------------

     Supported Joins:  inner, left, right, full, left_semi, left_anti, cross

     left_semi join
     ---------------
	-> Similar to inner join but the data comes only from the left side table.
	-> Equivalent to the following subquery:

		select * from emp where deptid IN (select id from dept)	 

     left_anti join
     ---------------
	-> Equivalent to the following subquery:

		select * from emp where deptid NOT IN (select id from dept)	 
       

  		===== sample code =====

		employee = spark.createDataFrame([
			(1, "Raju", 25, 101),
			(2, "Ramesh", 26, 101),
			(3, "Amrita", 30, 102),
			(4, "Madhu", 32, 102),
			(5, "Aditya", 28, 102),
			(6, "Pranav", 28, 10000)])\
		  .toDF("id", "name", "age", "deptid")  
		  
		department = spark.createDataFrame([
			(101, "IT", 1),
			(102, "ITES", 1),
			(103, "Opearation", 1),
			(104, "HRD", 2)])\
		  .toDF("id", "deptname", "locationid")  
		  
		  
		employee.show()
		department.show()

		spark.catalog.listTables()

		spark.catalog.dropTempView("dept")


		joinCol = employee.deptid == department.id

		joinedDf = employee.join(department, joinCol, "left_anti")

		joinedDf.show()


   Window Transformations
   ----------------------

      empid	dept		salary		row_number
      ----------------------------------------------------------	
	6	IT		50000		1		
	8	IT		50000		2	
	12	IT		55000		3	
	13	IT		55000		4	
	2	IT		60000		5		
	1	IT		70000		6

	10	Sales		40000		1		
	5	Sales		50000		2		
	11	Sales		60000		3		
	7	Sales		60000		4		
	15	Sales		80000		5
		
	9	HR		20000		1
	14	HR		60000		2
	3	HR		65000		3
	4	HR		80000		4
 

       windowSpec = Window.partitionBy("dept") \
			.orderBy("salary") \ 
			.rowsBetween(Window.unboundedPreceding, Window.currentRow)

	values for rowsBetween
	-----------------------
	Window.unboundedPreceding
	Window.unboundedFollowing
	Window.currentRow


	df2 = df1.withColumn("rank", rank().over(windowSpec))\
		 .withColumn("dense_rank", dense_rank().over(windowSpec))\
		 .withColumn("row_number", row_number().over(windowSpec))\
		 .withColumn("sum", sum("salary").over(windowSpec))


   Use Case
   ---------

	Datasets: movies.csv & ratings.csv
	URL: https://github.com/ykanakaraju/pyspark/tree/master/data_git/movielens

	From movies & ratings files, find the top 10 movies with highest average user rating
	-> Consider only those movies that are rated by atleast 30 user
	-> data:  movieId, title, totalRatings, averageRating
	-> Sort the data in the DESC order of averageRating
	-> Save the data as a single pipe-separated CSV file with header. 

	Use only DF transformation methods approach (not SQL)

	=> Try it yourself..


  JDBC Format - Working with MySQL
  --------------------------------
import os
import sys

# setup the environment for the IDE
os.environ['SPARK_HOME'] = '/home/kanak/spark-2.4.7-bin-hadoop2.7'
os.environ['PYSPARK_PYTHON'] = '/usr/bin/python'

sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python')
sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip')

from pyspark.sql import SparkSession

spark = SparkSession.builder \
            .appName("JDBC_MySQL") \
            .config('spark.master', 'local') \
            .getOrCreate()
  
qry = "(select emp.*, dept.deptname from emp left outer join dept on emp.deptid = dept.deptid) emp"
                  
df_mysql = spark.read \
            .format("jdbc") \
            .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
            .option("driver", "com.mysql.jdbc.Driver") \
            .option("dbtable", qry)  \
            .option("user", "root") \
            .option("password", "kanakaraju") \
            .load()
                
df_mysql.show()  

spark.catalog.listTables()

df2 = df_mysql.filter("age > 30").select("id", "name", "age", "deptid")

df2.show()   

df2.printSchema()    

df2.write.format("jdbc") \
    .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
    .option("driver", "com.mysql.jdbc.Driver") \
    .option("dbtable", "emp2")  \
    .option("user", "root") \
    .option("password", "kanakaraju") \
    .mode("overwrite") \
    .save()      
                                     
spark.stop()



  Working with Hive
  -----------------

# -*- coding: utf-8 -*-
import findspark
findspark.init()

from os.path import abspath
from pyspark.sql import SparkSession
from pyspark.sql.functions import desc, avg, count

warehouse_location = abspath('spark-warehouse')

spark = SparkSession \
    .builder \
    .appName("Datasorces") \
    .config("spark.master", "local") \
    .config("spark.sql.warehouse.dir", warehouse_location) \
    .enableHiveSupport() \
    .getOrCreate()
    
spark.catalog.listTables()  

spark.catalog.currentDatabase()
spark.catalog.listDatabases()

spark.sql("show databases").show()

spark.sql("drop database sparkdemo cascade")
spark.sql("create database if not exists sparkdemo")
spark.sql("use sparkdemo")

spark.catalog.listTables()
spark.catalog.currentDatabase()

spark.sql("DROP TABLE IF EXISTS movies")
spark.sql("DROP TABLE IF EXISTS ratings")
spark.sql("DROP TABLE IF EXISTS topRatedMovies")
    
createMovies = """CREATE TABLE IF NOT EXISTS 
         movies (movieId INT, title STRING, genres STRING) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadMovies = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/moviesNoHeader.csv' 
         OVERWRITE INTO TABLE movies"""
    
createRatings = """CREATE TABLE IF NOT EXISTS 
         ratings (userId INT, movieId INT, rating DOUBLE, timestamp LONG) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadRatings = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/ratingsNoHeader.csv' 
         OVERWRITE INTO TABLE ratings"""
         
spark.sql(createMovies)
spark.sql(loadMovies)
spark.sql(createRatings)
spark.sql(loadRatings)
    
spark.catalog.listTables()
     
#Queries are expressed in HiveQL

moviesDF = spark.sql("SELECT * FROM movies")
ratingsDF = spark.sql("SELECT * FROM ratings")

moviesDF.show()
ratingsDF.show()
           
summaryDf = ratingsDF \
            .groupBy("movieId") \
            .agg(count("rating").alias("ratingCount"), avg("rating").alias("ratingAvg")) \
            .filter("ratingCount > 25") \
            .orderBy(desc("ratingAvg")) \
            .limit(10)
              
summaryDf.show()
    
joinStr = summaryDf["movieId"] == moviesDF["movieId"]
    
summaryDf2 = summaryDf.join(moviesDF, joinStr) \
                .drop(summaryDf["movieId"]) \
                .select("movieId", "title", "ratingCount", "ratingAvg") \
                .orderBy(desc("ratingAvg")) \
                .coalesce(1)
    
summaryDf2.show()
 
summaryDf2.write \
   .mode("overwrite") \
   .format("hive") \
   .saveAsTable("topRatedMovies")
   
   
spark.catalog.listTables()
        
topRatedMovies = spark.sql("SELECT * FROM topRatedMovies")
topRatedMovies.show() 
    
spark.catalog.listFunctions()  
  
spark.stop()


 ===========================================
   Spark Streaming (Structured Streaming) 
 ===========================================

   Spark Streaming APIs:

	-> Spark Streaming (DStreams API) built on top of Spark Core
		-> pyspark.streaming
		-> Old & legacy API

	-> Structured Streaming built on top of Spark SQL 
		-> pyspark.sql.streaming
		-> Current and preferred API

  Notes:

  The key idea in Structured Streaming is to treat a live data stream as a table that is being 
  continuously appended.

  You express your streaming computation as standard batch-like query as on a static table, 
  and Spark runs it as an incremental query on the unbounded input table.

  Programming Model
  -----------------
	A query on the input will generate the “Result Table”.

	Every trigger interval (say, every 1 second), new rows get appended to the Input Table, 
	which eventually updates the Result Table.

	Whenever the result table gets updated, we would want to write the changed result rows 
	to an external sink.

	Structured Streaming does not materialize the entire table.

	It reads the latest available data, processes it incrementally to update the result, and 
	then discards the source data.

	Spark is responsible for updating the Result Table when there is new data, thus relieving 
	the users from reasoning about fault-tolerance and data consistency.


  Sources
  -------

	Socket Stream (host & port)	
	File Stream (directory) => text, csv, json, parquet, orc
	Rate stream
	Kafka Stream


  Sinks
  -----

	Console Sink
	File Sinks  (directory) => text, csv, json, parquet, orc
	Kafka sink
	ForEachBatch sink
	ForEach sink 
	Memory sink



  Assignments Submission
  ----------------------

	=> Download the assignements document from C-Learn portal.
	
        => Assignment -> 5 problems
				
			-> 1 assignments from Spark-Core (RDD)    10
			-> 2 assignments from Spark-SQL           20 + 20
			-> 2 assignements from Spark-Streaming 	  25 + 25


	=> Submit a text file (using notepad) to "kanakaraju.y2@cognizant.com" as attachment

		Attachment File Name : PySpark_<Your Employee ID>  ex: PySpark_234234
		Subject Line: PySpark Assignment - <Your Employee ID>

        => Due date : 30th August, 2023 by 6:00 PM 

	=> Content of the Solutions file (notepad text file)


       Associate ID: XXXXXXX
       Name: XXXXXXX


       Assignment 1

               <source-code>


       =============================

      
 	Assignment 2

               <source-code>

       =============================

      
 	Assignment 3

               <source-code>

       =============================

      
 	Assignment 4

               <source-code>

       =============================

      
 	Assignment 5

               <source-code>



















	





















































  

