 
 Agenda
 ------

 1. Introduction to Big Data and Hadoop
    -> Hadoop Basics
    -> Hadoop components – HDFS, MapReduce and YARN
    -> HDFS architecture
    -> MapReduce architecture and stages
    -> YARN architecture

 2. HDFS and MapReduce
    -> HDFS file operations
    -> Hadoop file formats
    -> Working with MapReduce - basics

 3. Hive
   -> Hive basic components & architecture
   -> Managed tables and External Tables
   -> Partitioned Tables
   -> Bucketing of Tables
   -> Hive Indexes
   -> Hive Joins
   -> Performance considerations

 4. PySpark
   -> Spark basics
   -> Spark architecture
   -> Spark APIs
   -> Spark Core API
	-> RDD Transformations
	-> RDD Actions
	-> RDD Persistence
	-> Shared Variables
   -> Spark Execution Flow - DAG Scheduling
   -> Spark Submit command

 5. PySpark SQL
	-> Working with DataFrames
	-> Running SQL on DataFrames - TempViews
	-> Basic Transformations & UDFs
	-> Spark SQL Joins
	-> Working with JDBC
	-> Working with Hive

 6. PySpark Streaming
    -> Structured Streaming API
    -> Working with Streaming sources & sinks
    -> Working with Kafka

 7. Data Engineering using Spark on Azure Databricks
    -> Databricks basics and features
    -> Working with DBFS – DBFS file operations
    -> Azure Databricks architecture – control plane and data plane.
    -> Setup Azure Databricks Workspace
    -> Azure Resource groups and storage account.
    -> Databricks clusters – all-purpose, job clusters and cluster pools
    -> Databricks databases and tables
    -> Setup Azure CLI 
    -> Working with Storage Account using Azure CLI
    -> Accessing Azure ADLS from Databricks
    -> Mount ADLS on Azure Databricks

 8.  Azure Databricks - Delta Lake
    -> Introduction to Delta Lake
    -> Time Travel - Restore a Delta Table to an old version
    -> Vacuum a Delta Table
    -> Delta Table Compaction
    -> Develop and end-to-end ELT Pipeline using Delta Lake 
 

  Materials
  ---------    
    -> PDF Presentations
    -> Code Modules
    -> Class Notes 
    -> Github: https://github.com/ykanakaraju/Hadoop-Basic

 

   Big Data
   --------    
   Type of data which large and complex that your traditional data processing systems
   can not reasonably store and process.


   Big Data scenarios
   ------------------
       -> The volume to be handled is very high (much higher than 1 TB)
       -> You want to support real-time analytics (velocity)
       -> You data is growing very fast, so that in no time your system becomes inadeqaute. 
       -> Process unstructured, semi-structured data etc.

    
  Big Data Characteristics
  ------------------------
       -> Volume
       -> Velocity
       -> Variety 
       -> Veracity (correctness of the data)
       -> Value (extracting insights from the data)   
    

  Why your traditional systems have big data problem
  --------------------------------------------------
   -> Because your traditional systems (that exists in pre-internet era) are not designed
      for internet scale applications.
 
   -> They are designed to run on a single machine. The single machine has only limited 
      amount of resources.

  
  Scalability Options
  -------------------
    -> Scale up  (Vertical Scaling)   -> Migrate to a machine with higher configuration
    -> Scala out (Horizontal Scaling) -> Use many machines instead of one. 
                                      -> Use computing cluster

   What is a Computing Cluster?
   ----------------------------
   A unified network of nodes (machines) whose cumulative resources can be used to store
   or process data as if you working with a single machine. 


   What is a Framework ?
   ---------------------
    A framework is a set of libraries that implement all the basic low-level functionalities
    that the framework is intended for. The users of the framework should focus on the 
    business-logic and implement their solutions as per the guidelines provided by the 
    framework. 



   What is the solution to storage & processing problems associated with Big Data
   ------------------------------------------------------------------------------
    -> Hadoop provides a solution for big-data problems.

 
   What is Hadoop ?
   ----------------     
    Hadoop is a Open Source Framework to Store and Process data in a distributed way 
    using a cluster of commdity harware. 

     -> Provides two things:
	   -> Distributed Storage Solution (HDFS)
	   -> Distributed Processing Solution (MapReduce)
           -> Resource Management (YARN)


    Hadoop Distribution Modes
    -------------------------

	1. standalone mode   
		-> All daemons run in the same JVM in the same machine

	2. psudo distribution mode
		-> All daemons run in the separate JVMs in the same machine

	3. (full) distribution mode
		-> Different daemons run on different nodes (production deployment)



    HDFS (Hadoop Distributed File System)
    --------------------------------------

      -> Stores data in the form of "blocks" of 128 MB each
      -> Each block is replicated on other nodes so many times as defined by a 'replication-factor'
           (default rep-factor is 3)      
      
     Q1:  If I want to stote 1024 MB (1 GB) file on HDFS, how much physical storage space 
          is required (using default configs) => 3 GB   

     Q2:  If you want to store 1034 MB file, how many unique blocks are created on HDFS?
	  -> 8 blocks of 128MB each & 1 block of 10 MB


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                  Some important HDFS file locations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 Default Paths:

        * Cloudera Local Home :  /home/cloudera
        * HDFS Home: /user/cloudera

 Configuration Files:
	
	* /etc/hadoop/config/core-site.xml
	* /etc/hadoop/config/hdfs-site.xml
	* /etc/hadoop/config/mapred-site.xml

 There are individual folders for each node in the cluster in directories as shown below :
  
        * /etc/hadoop/conf.cloudera.hdfs1
	* /etc/hadoop/conf.cloudera.mapred1
	* /etc/hadoop/conf.cloudera.yarn1

 All daemon log files are written under:

	* /var/log/hadoop/


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                  HDFS Commands
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        
 * hadoop fs -help   				=> lists all the commands with a brief discription
 * hadoop fs -help <command-name>  
 * hadoop fs -chmod -R 755 <file/dir>		=> change permission for a file/directory
 * hadoop fs -cat <file-name>
 * hadoop fs -ls
 * hadoop fs -ls <dir-path>
 * hadoop fs -ls -R  <dir-path>

 * hadoop fs -cp <source file> <target dir>     => Copies within HDFS frpm source to target
 * hadoop fs -mv <source file> <target dir>     => Moves within HDFS frpm source to target
 * hadoop fs -rm -r <file-name/directory-name>
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 Moving files to/from local file system to/from hdfs 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 * hadoop -copyFromLocal <local file path>  <hdfs file path> 
 * hadoop -put <local file path>  <hdfs file path> 

 * hadoop -copyToLocal <hdfs file path> <local file path>   
 * hadoop -get <hdfs file path> <local file path>   

 * hadoop -moveFromLocal <local file path>  <hdfs file path> 
 * hadoop -moveToLocal <hdfs file path> <local file path> 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 Admin Commands
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 * sudo -u hdfs hdfs dfsadmin -report   => reports the usage stats of the cluster
 * sudo -u hdfs hdfs fsck / -files -blocks

Safe Mode:

 * sudo -u hdfs hdfs dfsadmin -safemode leave
 * sudo -u hdfs hdfs dfsadmin -safemode enter
 * sudo -u hdfs hdfs dfsadmin -safemode get
 * sudo -u hdfs hdfs dfsadmin -safemode wait

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 More Commands
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 * du - disk usage  
      
        ex:  hadoop fs -du /user/cloudera


 * du -s :   Like -du, but prints a summary of disk usage of all files/directories in the path

        ex: hadoop fs -du -s /user/cloudera

 * df : Shows the capacity, free and used space of the filesystem
         -h  Formats the sizes of files in a human-readable fashion.

        ex: hadoop fs -df -h /user/cloudera/
            hadoop fs -df /user/cloudera/

 * getmerge <src> <localDest>    :  Retrieves all files that match the path src in HDFS, 
                                    and copies them to a single, merged file in the local 
                                    file system identified by localDest. 

        ex: hadoop fs -getmerge test/*.txt test1/merged.txt

 * Get the replication factor of a file:

        ex: hadoop fs -stat %r test/empinfo.txt

 * Set the replication factor of a file while copying or moving from linux

        ex: hadoop fs -D dfs.replication=5 -copyFromLocal <source> <destination>
  
            hadoop fs -D dfs.replication=5 -copyFromLocal datasets/empinfo.txt test2
            hadoop fs -stat %r test2/empinfo.txt

 * Set the replication factor for an existing file:  -setrep [-R] [-w] <rep-factor> <path>
       
        ex: hadoop fs -setrep 2 test2/empinfo.txt   => sets the rep-factor of the file to 2
	    hadoop fs -stat %r test2/empinfo.txt    => test the rep-factor of the file

 * appendToFile

	Usage: hdfs dfs -appendToFile <localsrc> ... <dst>

	Append single src, or multiple srcs from local file system to the destination file system. 
	Also reads input from stdin and appends to destination file system.

	hdfs dfs -appendToFile localfile /user/hadoop/hadoopfile
	hdfs dfs -appendToFile localfile1 localfile2 /user/hadoop/hadoopfile
	hdfs dfs -appendToFile localfile hdfs://nn.example.com/hadoop/hadoopfile
	hdfs dfs -appendToFile - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin.


 * chmod, chgrp, chown  => Change the permissions of files.

 


































