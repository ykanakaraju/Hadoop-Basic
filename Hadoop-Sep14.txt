
  Agenda  (20 sessions of 4 hours each)
  -------------------------------------
    	Understanding Big data
    	Hadoop - basics
    	HDFS - Architecture & File Operations
	YARN - Cluster Manager
	MapReduce 
	Hive - Data Warehousing
	Impala (introduction)	
	Sqoop (structured data ingestion tool)
	HBase ( NoSQL database )
	Kafka (pubsub messaging engine)	
	NiFi (Workflow Scheduler)
	Spark (Python) - PySpark  (Spark Core & Spark SQL)   
 ----------------------------------------------------------

   What is Big Data ?
   ------------------
  
    Refers to data that is large and complex data that is difficult to store and process
    using traditional data management tools or data processing technologies.

    Big Data Challenges:
	-> Storing huge amount of data
	-> Processing data faster
	-> Processing complex data

    What qualities of data make some data as Big Data ?   
	-> Volume  (10 TBs or PB scala data stores)
	-> Velocity (real time data processing)
	-> Variety (unstructured & semi-structured data)
	-> Veracity (data in doubt)
	-> Value (deep analytics)

   => Hadoop provides a solution to big data.


   What is a Cluster?
   ------------------		
        -> Is a unified entity comprising og many nodes whise cumulative resources (disk space, RAM, 
           CPU cores etc) can be used to distribute your storage and processing. 
  
   What is Hadoop ?
   ----------------
	Is an opensource framework for "storing" and "processing" big data on 
	"computing clusters" made of commodity machines. 

         1. HDFS   -> Hadoop Distributed File System
		   -> Distributed Storage Solution
		   -> Allows data of a single file to be distributed across many machines.
	
	 2. MapReduce -> Distributed Processing Framework
		    -> Used to distribute processing across many nodes in a cluster

	 3. YARN    -> Introduced in Hadoop 2 as a dedicated Resource Management Framework.


   
   HDFS (Hadoop Distributed File System)
   -------------------------------------	

     -> Is a distributed storage framework

     -> Stores the data as blocks of 128 MB each.
 
      1. Name Node (NN)
		-> Is a master daemon (continuously running background job)
		-> Is a Java background process
		-> Receives "Block reports" from all the DNs
		-> The meta-data of the entire file system name space is maintained using 2 files.

		   1. FSImage
			-> Contains the state of the entire cluster from the boot time upto
			   the last check point

		   2. EditLog
			-> Contains current data (on going changes to the cluster) from the last check
			   point upto the current moment. 

      2. Data Node (DN)
		-> Is a slave daemon
		-> Runs on all the slave machines
		-> DNs send heart-beats to NNs once every 3 sec.
		   -> if three successive heart-beats are missed, NN marks the DN as dead. 
		-> DNs send "block report" to NN
		   -> 'Block Report' contains such information as:
			-> Total Disk Space
			-> Available Space
			-> Reserved Space
			-> List of all the files, blocks of the file, owner of the file, permissions,
			   Replication-factor, etc
			-> If it is a dircetory, owner, timestamp etc.
	
      3. Secondary Name Node (SNN)
		-> Is a master daemon	
	
      4. Balancer
	       -> Is a daemon that runs on the master (same machine as NN)
	       -> Maintains the replication-factor of the HDFS files.	


   Hadoop EcoSystem
   ----------------
	-> A set of tools that are commonly used for various data analytics, data flow,
           data ingestion use-cases on top of Hadoop.	   

    
   Hadoop Distributions
   --------------------
	-> Third-party Hadoop Distributions help you to install many of the commonly used
	   components of Hadoop very easily.

	-> They will provide cluster-access or image-copies (pre-intslled hard-disk files)
	   containing all the hadoop components pre-installed.

	Hardware Requirements:	
	  -> Minimum 8 GB for CDH 4.7 (old and light-weight)
	  -> Minimum 12 GB to 16 GB RAM is required for CDH 5.13 and above.
	
	On-Premesis Installation
	------------------------
	-> Cloudera
	-> HortonWorks (merged with cloudera)
	-> MapR

	Cloud Hadoop Clusters
	---------------------
	-> Amazon: EMR (Elastic MapReduce - AWS)
	-> Microsoft: HDInsights (Azure)
	-> Google: DataProc (GCP)
		

   Setup Hadoop On your Local Machine
   ----------------------------------
     -> You can use Cloudera "ImageCopy" (CDH) as a virtual machine.
	
	https://docs.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh_download.html

      -> Get any one CDH Quickstrat version.

      -> Download Oracle VirtualBox from https://www.virtualbox.org/wiki/Downloads
	 (click on Windows Hosts)
	
      -> Follow the steps mentoned in the document shared with you.


   HDFS File Operations
   --------------------
     
      Linux File System  (local file system)
	-> Home Directory: /home/cloudera

      HDFS File System   (distributed file system)
	-> Home Directory: /user/cloudera  (hdfs://localhost.localdomain:8020/user/cloudera)

      Two command formats:
	
	-> hadoop fs -<command>
	-> hdfs dfs -<command>


      Common File Operations on HDFS
      ------------------------------

	List the files & directories:   
		
		hadoop fs -ls
		hdfs dfs -ls

		hdfs dfs -ls datasets
		hadoop fs -ls -R datasets    -> recursive listing

	Help
		hadoop fs -help
		hadoop fs -help stat

	Remove a files/directories:

		hadoop fs -rm <file-name>
		hadoop fs -rm dir1/*.txt
		hadoop fs -rm -R practice1    // remove non-empty directory		

	Create a directory:

		hadoop fs -mkdir demodir

	Delete an empty directory:

		hadoop fs -rmdir demodir

	Copy from one directory to other

		hadoop fs -cp <source> <destination>
		hadoop fs -cp practice1/NYSE_*.txt practice2

	To print the content of a file on stdout

		hadoop fs -cat wordcount_input.txt

	Move from one directory to other

		hadoop fs -mv practice1/NYSE_*.txt practice2/dir1

		
	Changing permissions on an HDFS file

		hadoop fs -chmod 777 practice1
		hadoop fs -chmod -R 777 practice1    // recursive permission


       Copy/Move files between Local File System and HDFS
       ---------------------------------------------------
	
	Copy from Linux to HDFS

	    -> hadoop fs -copyFromLocal datasets/NYSE_dividends.txt practice1
	    -> hadoop fs -put datasets/NYSE_daily.txt practice1

	    -> hadoop fs -put datasets practice1

       Copy from HDFS to Linux

	   -> hadoop fs -copyToLocal <hdfs-file-path> <linux-directory>
	   -> hadoop fs -get  <hdfs-file-path> <linux-directory>

       Move from Linux to HDFS
	
	   -> hadoop fs -moveFromLocal <linux-file-path> <hdfs-directory>
	      hadoop fs -moveFromLocal test1/baseball.txt practice2

       
	

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                  Some important file locations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 Default Paths:

        * Cloudera Local Home :  /home/cloudera
        * HDFS Home: /user/cloudera

 Configuration Files:
	
	* /etc/hadoop/conf/core-site.xml
	* /etc/hadoop/conf/hdfs-site.xml
	* /etc/hadoop/conf/mapred-site.xml

 There are individual folders for each node in the cluster in directories as shown below :
  
        * /etc/hadoop/conf.cloudera.hdfs1
	* /etc/hadoop/conf.cloudera.mapred1
	* /etc/hadoop/conf.cloudera.yarn1

 All daemon log files are written under:

	* /var/log/hadoop/


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                  HDFS Commands
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        
 * hadoop fs -help   				=> lists all the commands with a brief discription
 * hadoop fs -help <command-name>  
 * hadoop fs -chmod -R 755 <file/dir>		=> change permission for a file/directory
 * hadoop fs -cat <file-name>
 * hadoop fs -ls
 * hadoop fs -ls <dir-path>
 * hadoop fs -ls -R  <dir-path>

 * hadoop fs -cp <source file> <target dir>     => Copies within HDFS frpm source to target
 * hadoop fs -mv <source file> <target dir>     => Moves within HDFS frpm source to target
 * hadoop fs -rm -r <file-name/directory-name>
 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 Moving files to/from local file system to/from hdfs 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 * hadoop -copyFromLocal <local file path>  <hdfs file path> 
 * hadoop -put <local file path>  <hdfs file path> 

 * hadoop -copyToLocal <hdfs file path> <local file path>   
 * hadoop -get <hdfs file path> <local file path>   

 * hadoop -moveFromLocal <local file path>  <hdfs file path> 
 * hadoop -moveToLocal <hdfs file path> <local file path> 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 Admin Commands
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 * sudo -u hdfs hdfs dfsadmin -report   => reports the usage stats of the cluster
 * sudo -u hdfs hdfs fsck / -files -blocks

Safe Mode:

* sudo -u hdfs hdfs dfsadmin -safemode leave
  sudo -u hdfs hdfs dfsadmin -safemode enter
  sudo -u hdfs hdfs dfsadmin -safemode get
  sudo -u hdfs hdfs dfsadmin -safemode wait

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 More Commands
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 * du - disk usage  
      
        ex:  hadoop fs -du /user/cloudera


 * du -s : Like -du, but prints a summary of disk usage of all files/directories in the path

        ex: hadoop fs -du -s /user/cloudera


 * df : Shows the capacity, free and used space of the filesystem
         -h  Formats the sizes of files in a human-readable fashion.

        ex: hadoop fs -df -h /user/cloudera/
            hadoop fs -df /user/cloudera/


 * getmerge <src> <localDest>    :  Retrieves all files that match the path src in HDFS, 
                                    and copies them to a single, merged file in the local 
                                    file system identified by localDest. 

        ex: hadoop fs -getmerge test/*.txt test1/merged.txt


 * Get the replication factor of a file:

        ex: hadoop fs -stat %r test/empinfo.txt


 * Set the replication factor of a file while copying or moving from linux

        ex: hadoop fs -D dfs.replication=5 -copyFromLocal <sourc e> <destination>
  
            hadoop fs -D dfs.replication=5 -copyFromLocal datasets/empinfo.txt test2
            hadoop fs -stat %r test2/empinfo.txt


 * Set the replication factor for an existing file:  -setrep [-R] [-w] <rep-factor> <path>
       
        ex: hadoop fs -setrep 2 test2/empinfo.txt   => sets the rep-factor of the file to 2
	    hadoop fs -stat %r test2/empinfo.txt    => test the rep-factor of the file


 * appendToFile

	Usage: hdfs dfs -appendToFile <localsrc> ... <dst>

	Append single src, or multiple srcs from local file system to the destination file system. 
	Also reads input from stdin and appends to destination file system.

	hdfs dfs -appendToFile localfile /user/hadoop/hadoopfile
	hdfs dfs -appendToFile localfile1 localfile2 /user/hadoop/hadoopfile
	hdfs dfs -appendToFile localfile hdfs://nn.example.com/hadoop/hadoopfile
	hdfs dfs -appendToFile - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin.


 * chmod, chgrp, chown  => Change the permissions of files.


====================================================
   YARN  -- (Yet another resource negotiator)
====================================================

   1. Resource Manager
	-> Master daemon
	-> One per cluster
	-> receives job submissions and schedules jobs
	-> Allocates containers for Application masters
	-> Dynamically manages the resource allocation to all the jobs

   2. Node Manager
	-> Slave daemon
	-> One per node
	-> Sends heart-beats to RM
	-> reports the status of all the jobs/tasks running on that node to RM
	-> Launches App.Master for a job

   3. Application Master
	-> One instance per application
	-> manages the tasks related to a job on the cluster
	-> requests for resources required for the job

   4. Container    
	-> Multiple containers are allocated for a single application
	-> The actual tasks of a job are executed in these containers
	-> These containers communicates with the application master.

	
  Documentation  -> https://hadoop.apache.org/docs/r3.3.1/

  Book -> https://www.isical.ac.in/~acmsc/WBDA2015/slides/hg/Oreilly.Hadoop.The.Definitive.Guide.3rd.Edition.Jan.2012.pdf

  ====================================================
      MapReduce
  ====================================================

   -> Hadoop's distributed processing framework. 
	
    MapReduce can be seen from two perspectives

    1. MapReduce Programming Paradigm	
	
		-> What kind of code that the developer has to write.	

		1. Driver Program
			-> All the configurations are defined here.

		2. Mapper Program
			-> Define the program logic to be implememted during the Map phase

		3. Reducer Program
			-> Optional
			-> It will reduce the various values of each unique key produced by the
			   mapper phase into aggregrated values with unique keys.
		
		4. Combiner Program
			-> Optional
			-> Will apply a min-reducer on the output of each mapper
			-> reduced the amount of data shuffled across the network there by increasing
			   the performance

		5. Partititoner Program
			-> Allows uds to define a custom partitioning logic to control which
			   data goes to which partition.
			-> Applied on the output of shuffle and sort phase.
	

	2. MapReduce Execution Model (Framework)	
		-> How the program is executed by the framework

		1. Mapper Phase
			-> Driver process lauches a set of mappers across the cluster
			   in the container allocated to it.

			-> All the mappers execute the same code.

			-> All the mapper write (k,v) pairs to context-buffer from which
			   a lot of local intermediate files are generated (across the cluster)

		2. Combiner Phase
			-> Optional phase
			-> Applied to the output of every map instance.

		3. Shuffle & Sort Phase
			-> Collect all the intermediate data and group the data by keys
			   (i.e it will produce unique keys with grouped values)
				(key, [val1, val2, val3, ....])

			-> Number of partitions are created with this shuffled data.
			   The number of partitions are equal to the number of reducers. 
			   (the default number of reducers = 1)

		4. Partitioning Phase
			-> Optional Phase
			-> A custom partitioner can be applied to decide which keys go to which
			   partition.

		5. Reducer Phase
			-> All these partitions are then processed by the reducer and the
			   output the reducer is written to HDFS in a specified directory.

			-> The number of output files is equal to the number of reducers.

      Submit an MR Program to the Cluster (YARN Resource Manager)
      -----------------------------------------------------------
	  $hadoop jar /home/cloudera/mrpractice.jar com.sample.WordCountJob wordcount_input.txt wcout
	  $yarn jar /home/cloudera/mrpractice.jar com.sample.WordCountJob wordcount_input.txt wcout	  


      Input Split
      -----------
	-> Always has whole number of records
		-> record in atext file is a line (ending with \n)

	-> InputSplits are created from the blocks where where InputSplit
	   starts at a new record and ends at a record boundary

	-> InputSplits are given as input data to the mappers.

	InputFormat  -> Identified the records and creates InputSpits
	InpuSplit    -> One intergral number of records that could span multple blocks
     	RecordReader -> Creates a (K, V) pair from each record of the inputsplit
	             -> RecordReader calles the map() method with each (K, V) pair

InputSplit						   RecordReader
=========== 						    =========== 
hadoop hive mapreduce hdfs hadoop mapreduce     	=>  (0, hadoop hive mapreduce hdfs hadoop mapreduce)        ---> map
mapper reducer sqoop flume hadoop mapreduce		=>  (40, mapper reducer sqoop flume hadoop mapreduce)	    ---> map	
mapreduce hive flume kafka hadoop hive			=>  (74, mapreduce hive flume kafka hadoop hive)            ---> map
hadoop hadoop mapreduce mapreduce hive hive
sqoop flume kafka mapper reducer partitioner combiner
driver hadoop mapreduce driver mapper combiner
reducer partitioner hadoop sqoop mapreduce


   public class WordCountMapper extends Mapper<LongWritable, Text, Text, LongWritable>

	LongWritable, Text   -> Key data type and Value datatype of input (K,V) pair
				(0, hadoop hive mapreduce hdfs hadoop mapreduce)

	Text, LongWritable   -> Key data type and Value datatype of output (K,V) pair
			        (hadoop, 1), (hive, 1), (mapreduce, 1), (hdfs, 1), (hadoop, 1), (mapreduce,1)


  Input-Split		RecordReader
  -----------          --------------
  hadoop hive mapreduce -> (0, hadoop hive mapreduce) --> map(....) --> (hadoop, 1) (hive, 1) (mapreduce, 1)
  mapper reducer sqoop	-> (23, mapper reducer sqoop)	            --> (mapper, 1) (reducer, 1) (sqoop, 1)									
  mapreduce hive flume
  hadoop hadoop
  sqoop flume kafka


    Creating a MapReduce Program from Scratch
    -----------------------------------------
	
	Two approaches: 

	1. Adding the required dependent jars from your local installation.

		-> Create a blank Java Project
		-> Add the following libraries in the build-path
		   
                    Right Click on Project -> Build Path -> Configure Build Path

		    Go to Libraries Tab -> Add External Jars
			-> /usr/lib/hadoop/hadoop-common.jar
			-> /user/lib/hadoop-0.20-mapreduce/hadoop-core.jar

		-> Start writing your MR code

	2. Using maven (package manager) you can add dependencies from maven-repo.

 
   Mapper Life Cycle Methods
   -------------------------
	
	1. setup
		-> executed once at the beginning of the mapper.
		-> perform initializations.

	2. map
		-> invoked once for every record, after setup() method
		-> runs as many times as the records in the input-split

	3. cleanup
		-> executed once after map() is completed.
		-> cleanup the resources (close the connections)

	4. run
		-> To gain complete control of execution-flow


   Reducer Life Cycle Methods
   ---------------------------
	
	1. setup
		-> executed once at the beginning of the reducer.
		-> perform initializations.

	2. reduce
		-> invoked once for every record, after setup() method.
		-> runs as many times as the records in the partition.

	3. cleanup
		-> executed once after reduce() is completed.
		-> cleanup the resources (close the connections)

	4. run
		-> To gain complete control of execution-flow


   Hadoop Writable Data Types
    ---------------------------

	-> Hadoop uses its own data types which are optimizes for serialization and deserialization.
	-> All Hadoop API classes work with Hadoop data types only.
	
	Java		Hadoop
	----		------
	String		Text
	Int		IntWritable
	Double		DoubleWritable
	Long		LongWritable
	Bool		BoolWritable
	Null		NullWritable

	
   Use-case
   --------
   From facebook.tsv dataset, fetch the record with highest "likes_received" from among the people
   who are born between 1990 and 2000.

    Two MR programs :

    1. fetch all the records with dob_year between 1990 and 2000  		    => age
    2. fetch the record with max value of likes_received from the output of step 1  => max-likes

  ==============================================
     Hive
  ==============================================

     -> Distributed Data Warehousing Infrastructure built on top of Hadoop.
     -> Provides an SQL interface for analysts to analyse big data on Hadoop.
     -> Hive uses MapReduce as its execution engine.
     -> Hive generates MR programs for the user queries and runs them on Hadoop cluster.

        NOTE: Hive is not an RDBMS database
	      Hive is not a NoSQL database


     Working with Hive
   
	1. Using CLI

		-> Hive Shell (connects to the local hive)
		-> Beeline Shell (connects to hive using JDBC drive - secure)

	2. Using HUE (Beeswax interface)
		-> provides WebUI to send queries to Hive

	3. Using Client Application
	

    Hive Warehouse
    --------------
	-> Directory where hive stores all data files of managed tables. 
	-> default path:  /user/hive/warehouse

    Hive Metastore
    --------------
	-> Is an external servive using which Hive manages its metadata (table schemas)
	-> In production deployments, we use MySQL/PostgreSQL as metastore 
	-> Hive comes with an embedded matastore called "Derby"

		
   NOTES
   -----
	-> Hive does not support primary-key and foreign-key
	-> Hive does not support update and delete ops on regular managed tables.

   Databases / Schemas
   -------------------
	show databases;
	show schemas;

	drop database <database-name>;  // drop an empty database
	
	create database ctsdb1;
	create database hivedb1 location '/user/cloudera/hivedb/hivedb1.db';

	describe database hivedb1;

   Describe
   --------

	describe <table-name>;
	desc <table-name>;

	describe extended <table-name>;
	describe formatted <table-name>;

   Table
   ------
	
	CREATE TABLE empinfo (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
	ROW FORMAT DELIMITED
	FIELDS TERMINATED BY ','
	LINES TERMINATED BY '\n' 
	STORED AS TEXTFILE;

	defaults:

	default storage format:  TEXTFILE
	default line terminator: "\n"
	default field terminator: Ctrl-A  (\0001)
	

   Loading Data
   -------------

   => local file system
   
        File is "copied" from the local file system to hive warehouse

	-- append
	LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/data/emp.txt' INTO TABLE empinfo;

	--overwrite
	LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/data/emp.txt' OVERWRITE INTO TABLE empinfo;

   => hdfs file system
   
        File is "moved" from the linux file system to hive warehouse

	-- append
	LOAD DATA INPATH '/user/cloudera/hive/data/emp.txt' INTO TABLE empinfo;

	--overwrite
	LOAD DATA INPATH '/user/cloudera/hive/data/emp.txt' OVERWRITE INTO TABLE empinfo;

   
   Managed Tables
   ---------------
	-> Tables that are managed by Hive (created in hive warehouse directory)
	-> When you drop the table both data files as well as schema are deleted.


   External Tables
   ---------------
	-> Tables are created in custom location specified in the create command
	-> When you drop the table, ONLY schema is dropped. The data files are NOT dropped.


   Partitioned Tables
   ------------------
	
	=> Allows you to organize your table data into multiple partitions (separate directories) based
	   on unique values of one or more partition columns.
	
       From the way data is written to partition tables, we can think of two types of partitioning:

	1. Static Partitioning
	    -> When you know into which partition you load data, then you can use static partitioning
	       approach.
	    -> To load data you may use LOAD DATA command or INSERT..SELECT command

	2. Dynamic Partitioning  
	    -> When your data file contains records that belong to multiple partitions and you can't
	       determine the exact partition into which you want to load that data.

	    -> In such scenarios you use dynamic partitioning approach to data loading.

		 -> create a base table and load all the data into that base table.
		 -> set dynamic partition mode to nonstrict
			-> set hive.exec.dynamic.partition.mode=nonstrict;
		 -> Load data from the base table into the partitioned table using 
		    INSERT..SELECT command.




CREATE TABLE students_static (id int, name string, mobile string, email string, project string)
PARTITIONED BY (country STRING)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/data/students_IN.txt' 
INTO TABLE students_static
PARTITION (country='IN');

INSERT INTO students_static
PARTITION (country = 'Germany')
SELECT id, name, mobile, email, project FROM students_static
WHERE country = 'UK';

--dynamic partitioning
-----------------------
CREATE TABLE students_dynamic (id int, name string, mobile string, email string, project string)
PARTITIONED BY (country STRING, batch INT)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;


CREATE TABLE students (id int, name string, mobile string, email string, project string, country STRING, batch INT)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '/home/cloudera/hive/Hive/data/students_country_batch.csv' 
INTO TABLE students;

set hive.exec.dynamic.partition.mode=nonstrict;

INSERT INTO TABLE students_dynamic
PARTITION (country, batch)
SELECT id, name, mobile, email, project, country, batch FROM students;

INSERT INTO TABLE students_dynamic
PARTITION (country='Germany', batch=1)
SELECT id, name, mobile, email, project FROM students
WHERE country='Germany' and batch=1;

INSERT INTO TABLE students_dynamic
PARTITION (country='USA', batch)
SELECT id, name, mobile, email, project, batch FROM students
WHERE country='USA';

INSERT INTO TABLE students_dynamic
PARTITION (country, batch=1)
SELECT id, name, mobile, email, project, country FROM students
WHERE batch=1;

NOTE: Dynamic partition cannot be the parent of a static partition

CTAS command
--------------------------------------
-- using defaults
create table students_static_2
as 
select * from students_static;


-- using custom field separator.

create table students_static_3 
row format delimited 
fields terminated by ','
as 
select * from students_static;

-----------------------------------------------
   Bucketted Tables
-----------------------------------------------

CREATE TABLE students_buckets (id int, name string, phone string, email string, project string, country string, batch int)

CLUSTERED BY (id) INTO 4 BUCKETS
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n' 
STORED AS TEXTFILE;

NOTE: We can not use "LOAD DATA .." command to load data into a bucketted table. 
      We have to load data from a base table into a bucketted table using INSERT..SELECT

NOTE: Make sure bucketting is enforced before loading data into bucketted table
set hive.enforce.bucketing = true;

INSERT OVERWRITE TABLE students_buckets
SELECT * from students;

create table students_part_bk (id int, name string, phone string, email string, project string, country string)
partitioned by (batch int)
clustered by (id) sorted by (id ASC) into 3 buckets
row format delimited
fields terminated by '|'
lines terminated by '\n'
stored as textfile;

INSERT INTO TABLE students_part_bk
PARTITION (batch)
SELECT id, name, mobile, email, project, country, batch FROM students;
   

================================================
 SAMPLING Queries
================================================

  Three types of sampling can be performed

	=> bucket sample -> only on bucketted tables
	=> block sample  -> Fetch some blocks randomly
	=> row level sampling -> Fetch some number of rows from every block


  Bucket Sampling: Fetches minimum 1 bucket

     	=> SELECT * FROM students_part_bk TABLESAMPLE (BUCKET 2 OUT OF 3 ON id) 
  	=> SELECT * FROM students_buckets TABLESAMPLE (BUCKET 1 OUT OF 2 ON id) 

  Block Sampling:  Fetches minimum 1 block

  	=> SELECT * FROM students TABLESAMPLE (50 PERCENT)
	=> SELECT * FROM students TABLESAMPLE (5M)  // 4 blocks

  Row Sampling:  Fetches n rows from each input split

  	=> SELECT * FROM students TABLESAMPLE (5 ROWS)

 
================================================
 Managing outputs & Hadoop File Formats
================================================

  // append mode
  insert into empinfo2 select * from empinfo;

  // overwrite mode
  insert overwrite table empinfo2 select * from empinfo;

  // write to a local directory
  insert overwrite local directory '/home/cloudera/hive_data/empinfo' select * from empinfo;

  // write to a hdfs directory
  insert overwrite directory '/user/cloudera/hive_data/empinfo' select * from empinfo;

  // write to a local directory
  insert overwrite local directory '/home/cloudera/hive_data/empinfo_tab' 
  row format delimited 
  fields terminated by '\t'
  select * from empinfo;

  // write to a local directory in sequence file format
  insert overwrite local directory '/home/cloudera/hive_data/empinfo_seq' 
  stored as sequencefile
  select * from empinfo;

  // write to a local directory in avro file format
  insert overwrite local directory '/home/cloudera/hive_data/empinfo_avro' 
  stored as avro
  select * from empinfo;

  // write to a local directory in avro file format
  insert overwrite local directory '/home/cloudera/hive_data/empinfo_parquet' 
  stored as parquet
  select * from empinfo;

  // write to a local directory in avro file format
  insert overwrite local directory '/home/cloudera/hive_data/empinfo_orc' 
  stored as orc
  select * from empinfo;

  // write to a local directory in avro file format
  insert overwrite local directory '/home/cloudera/hive_data/empinfo_rc' 
  stored as rcfile
  select * from empinfo;
	
 ---------------------------------------  
 Creating tables in Hadoop File Formats
 ---------------------------------------

CREATE TABLE empinfo_seq (empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
STORED AS SEQUENCEFILE;

LOAD DATA LOCAL INPATH '/home/cloudera/hive_data/empinfo_seq/000000_0' INTO TABLE empinfo_seq;



























