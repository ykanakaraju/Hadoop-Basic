  ----------------------------
    Agenda
  ----------------------------
  Big Data - Basic
  Hadoop 
   - HDFS - File Ops & Architecture
   - YARN - Basics & Architecture
   - MapReduce - Basics & Examples & Architecture
  Hive
  Impala (introduction)
  Sqoop 
  HBase
  Kafka 
  NiFi
  Spark ( PySpark ) 

  ==============================================

   Materials
   
        => PDF Presentations
	=> Code Modules  
	=> Class Notes 

 ===============================================

   Big Data  
   --------
     -> Large & Complex Data that can not be reasonbly stored and processed using traditional
        data management systems that rely on single server architectures.

     	-> Volume 
	-> Velocity
	-> Variety  (unstructured & semi structured data)
	-> Veracity (data in doubt)
	-> Value (performing deep analytics)

   Big Data Problems
   -----------------
	-> Storing huge amount of data
	-> Processing data faster
	-> Processing complex data

   => As the problems mainly arise because of the limitations of a single machine, 
      we solve this by using the combined resource of many machines. 
	 -> We use "a computing cluster".

   What is a Computing Cluster ?
   -----------------------------
     => Is a unified entity that comprises of many nodes whose cumulative resources can be used
        to stores and process huge amount of data.

   What is Hadoop ?
   ---------------- 
    => Hadoop is an open source framework to storage and process big data using a cluster
       made of commodity hardware. 
	
    => Written in Java

	
	Hadoop  => HDFS 	: Distributed Storage Framework
		=> MapReduce 	: Distributed Processing Framework
		=> YARN		: Resource Management Tool (Cluster Manager)


   Hadoop Distributed File System (HDFS)
   -------------------------------------
	
     -> Distributed File System
	
     -> Stores data in blocks of 128 MB each
     -> Each block is replicated 3 times by default to provide fail-safety.
	 --> 3 copies of each block will be there on different nodes
	

   HDFS Daemons
   ------------
	Daemon -> A continuosly running background job (java program)

	1. Name Node (NN)
		-> Runs on the master machine
		-> NN receives heart-beats from DNs. 
		   -> If three successive heart-beats are missed, then the NN marks the DN as "unavailable" 

		-> NN receives block reports and maintains this info in two files:
                 
                   1. EditLog file
			-> dynamic file maintained in the RAM

		   2. FSImage file
			-> static file which is updated only during check-point process.
	

	2. Data Node (DN)
		-> Runs on all the slave machines.
		-> Data Nodes send heart-beats to NN once every three seconds.
		-> Data Nodes send "block reports"
			-> Files, Blocks, ReplicationFactor, Owner, Permissions
			-> Total storage, Reserverd Storage, Available Storage

	3. Secondary Name Node
		-> Master Process
		-> Performs check-pointing once every hour where it merges the edit-log with 
		   FSIMage files and copies and updated (checkpointed) FSImages file to the master node. 

	4. Balancer
		-> Maintain replication-factor of files.
		-> Background job that monitor under-replicated & over-replicated files.		
		

   Hadoop Distributions
   -------------------- 
   
     => Will provide a collection of pre-packaged and configured tools that are commonly required 
	for all big data projects, so that we do not have to download, install, configure and tune 
        those components separtly
	
     => Also provide some management tool to manage, monitor, repair, start  & stop services etc. to
        make managing te cluster easy. 
	
    On-Primesis Clusters
    --------------------
	1. Cloudera 
	2. HortonWorks  (merged with Cloudera)
	3. MapR

    Cloud Clusters
    ---------------
	1. AWS    - EMR (Elastic MapReduce - Managed Hadoop Cluster)
	2. Azure  - HDInsight
	3. GCP    - DataProc 
	

    HDFS File Operations   (CDH 4.7)
    --------------------------------
      
      User: cloudera, Password: cloudera

      Home Directory: 

	On Linux File System:  /home/cloudera
	On HDFS File System:   /user/cloudera

	HDFS Commands => hadoop fs -<command>
			 hdfs dfs -<command>

     
    Configuration Files:
    =====================	
	* /etc/hadoop/conf/core-site.xml
	* /etc/hadoop/conf/hdfs-site.xml
	* /etc/hadoop/conf/mapred-site.xml
	
    Hadoop HDFS commands
    ====================

    NOTE: HDFS is a "non-ediatble' file system
	  -> You can not edit the content of a HDFS file.

	hadoop fs -help
	hadoop fs -help cat

	hadoop fs -ls			=> list the files and directories
	hadoop fs -ls datasets
	hadoop fs -ls -R practice1	=> recursive listing

	hadoop fs -mkdir practice	=> create a directory

	hadoop fs -rmdir practice	=> delete an empty directory

	hadoop fs -rm practice2/students.txt
	hadoop fs -rm practice2/students_I*.txt	  => delete one or more files 
	hadoop fs -rm -R practice1   => recursive delete - deletes a non-empty directory

	hadoop fs -mv practice2/*.sh practice1  => move files/directory from one dir to another within HDFS

	hadoop fs -cp practice1/datasets  practice2 => copy files/directory from one dir to another within HDFS
	
	hadoop fs -cat wordcount_input.txt  => display the content on stdout

	hadoop fs -chmod 755 practice1		=> Change permission of files/directories
	hadoop fs -chmod -R 777 practice1

 
   Copy data between Linux & HDFS file systems
   -------------------------------------------
    
	Linux to HDFS:
		hadoop fs -put <linux-file-path> <hdfs-directory>
		hadoop fs -copyFromLocal <linux-file-path> <hdfs-directory>
		hadoop fs -moveFromLocal <linux-file-path> <hdfs-directory>		

		hadoop fs -put users.csv practice1
     		hadoop fs -copyFromLocal datasets/NYSE_daily.txt practice1
		hadoop fs -copyFromLocal datasets/NYSE_daily.txt   // copies to home directory	
		hadoop fs -moveFromLocal dir1/student*.txt practice2   // files deleted from source dir	

		// Copy with a set rep-factor
		hadoop fs -D dfs.replication=5 -copyFromLocal datasets/NYSE_daily.txt


	HDFS to Linux:	
		hadoop fs -get <hdfs-file-path> <linux-directory>
		hadoop fs -copyToLocal <hdfs-file-path> <linux-directory>

		hadoop fs -get NYSE_daily.txt dir1
		hadoop fs -copyToLocal datasets dir1



   More Commands
   --------------

 * du - disk usage  
      
        ex:  hadoop fs -du /user/cloudera


 * du -s : Like -du, but prints a summary of disk usage of all files/directories in the path

        ex: hadoop fs -du -s /user/cloudera


 * df : Shows the capacity, free and used space of the filesystem
         -h  Formats the sizes of files in a human-readable fashion.

        ex: hadoop fs -df -h /user/cloudera/
            hadoop fs -df /user/cloudera/


 * getmerge <src> <localDest>    :  Retrieves all files that match the path src in HDFS, 
                                    and copies them to a single, merged file in the local 
                                    file system identified by localDest. 

        ex: hadoop fs -getmerge test/*.txt test1/merged.txt
	    hadoop fs -getmerge wcout/part-r-* test1/merged.txt


 * Get the replication factor of a file:

        ex: hadoop fs -stat %r test/empinfo.txt


 * Set the replication factor of a file while copying or moving from linux

        ex: hadoop fs -D dfs.replication=5 -copyFromLocal <sourc e> <destination>
  
            hadoop fs -D dfs.replication=5 -copyFromLocal datasets/empinfo.txt test2
            hadoop fs -stat %r test2/empinfo.txt


 * Set the replication factor for an existing file:  -setrep [-R] [-w] <rep-factor> <path>
       
        ex: hadoop fs -setrep 2 test2/empinfo.txt   => sets the rep-factor of the file to 2
	    hadoop fs -stat %r test2/empinfo.txt    => test the rep-factor of the file


 * appendToFile

	Usage: hdfs dfs -appendToFile <localsrc> ... <dst>

	Append single src, or multiple srcs from local file system to the destination file system. 
	Also reads input from stdin and appends to destination file system.

	hdfs dfs -appendToFile localfile /user/hadoop/hadoopfile
	hdfs dfs -appendToFile localfile1 localfile2 /user/hadoop/hadoopfile
	hdfs dfs -appendToFile localfile hdfs://nn.example.com/hadoop/hadoopfile
	hdfs dfs -appendToFile - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin.

 * chmod, chgrp, chown  => Change the permissions of files.	
     
   ==============================================
      YARN  (Yet Another Resource Negotiator)
   ==============================================

     => Resource Management tool for Hadoop (from Hadoop 2.0 onwards)
       
     Basic Components of YARN :

	1. Resource Manager : One per cluster, master daemon
	2. Node Manager: One per node, slave daemon
	3. Application Master: One per application/job
			      (starts when job starts, and closed when the job is completed)
	4. Containers: Application processes where App Master sends the tasks to be executed.

	Typical application execution with YARN follows this flow:
	   => Client program submits the MR application to the RM, along with information to launch 
	      the application-specific Application Master.
	   => RM negotiates a container for the AM and launches the AM.
	   => AM boots and registers with the RM, allowing the original calling client to interface 
	      directly with the AM.
	   => AM negotiates resources (resource containers) for client application.
	   => AM gives the container launch specification to the NM, which launches a container for 
	      the application.
	   => During execution, client polls AM for application status and progress.
	   => Upon completion, AM deregisters with the RM and shuts down.

  
   ==============================================
      MapReduce
   ==============================================
    
     => Hadoop's default distributed processing framework. 
	
     Can be seen from two perspectives:

	=> Map Reduce execution framework lifecycle
               => How a Mapreduce Program is executed by the MR framework (the life cycle process)

		1. Mapper Phase

		2. Reducer Phase

		    2.1 Shuffle & Sort

		    2.2  Reduce


	=> Map Reduce Programming Paradigm
		=> How should a developer write MR programs	

	   MapReduce Application Classes (that a developer has to write)

		1. Driver Class (required)
		     => Where you define all the application configurations.

		2. Mapper Class (required)
		     => Extends "Mapper" base class and implements some life cycle methods:
		     => Implements "map" method, where you write you application logic.		     

		3. Reducer Class (required only if aggregation is needed)
		     => Extends "Reducer" base class and implements some life cycle methods.
		     => Implements "reduce" method, where you write you application logic.

		4. Combiner (Optional)
		     => Is like a mini-reducer which aggregates the output of each mapper.
		     => Acts on each mapper's output to reduce the amount of data to be shuffled.
		     => By reducing the amount of data shuffled, we can increase the performance.

		5. Partitioner (Optional)
		      => Acts on the output of S&S phase and applies custom logic to decide which
			 data goes to which partition (and hence to which reducer).
		      => Is applicably only if you use 2 or more reducers.


    MapReduce Framework
    -------------------

    1. Creates InputSplits
	-> A logical enity derived from blocks as whole number of records
	-> Input-Splits can span multiple blocks.
	-> The number of input splits created is equal to number of mappers lauched. 

	InputFormat => InputSplit => RecordReader

	1. InputFormat creates InputSplits
	2. RecordReader class reads from InputSplits and creates (K, V) pairs


	IS-1
     [InputSplit]			[RecordReader]
	[hadoop hive mapreduce hdfs  => (0, hadoop hive mapreduce hdfs)   ==>  map(...)  ==> (hadoop, 1) (hive, 1) (mapreduce, 1) (hdfs, 1)         
	mapper reducer sqoop flume   => (32, mapper reducer sqoop flume ) ==>  map(...)  ==> (mapper, 1) (reducer,1) (sqoop, 1) (flume, 1)
	mapreduce hive flume kafka   => (75, mapreduce hive flume kafka )	
	hadoop hadoop mapreduce	     => (120, hadoop hadoop mapreduce)
	sqoop flume kafka mapper]    => (154, sqoop flume kafka mapper )


     Sort & Shuffle
     --------------
	(flume, [1,1,1,1,1,1,1,1,...])       --> reduce(...)  --> (flume, 74)
	(hadoop, [1,1,1,1,1,1,1,1,...])	     --> reduce(...)  --> (hadoop, 90)
	(hive, [1,1,1,1,1,1,1,1,...])
	(scala, [1,1,1,1,1,1,1,1,...])


   Hadoop Data Types
   ------------------
   
   => All hadoop framework classes use only Hadoop Custom Data Types
   => Hadoop custom data type are optimized for Serialization & Deserialization

	Java Type		Hadoop Types
        ----------              ------------
	String			Text
	Int			IntWritable
	Long			LongWritable
	Null			NullWritable
	Double			DoubleWritable
	Byte			ByTeWriteable
	Boolean			BoolWritable


    Submitting an MR program
    ------------------------

      	hadoop jar /home/cloudera/wordcount.jar com.cts.WordCountJob wordcount_input.txt wordcount_output
	yarn jar /home/cloudera/wordcount.jar com.cts.WordCountJob wordcount_input.txt wordcount_output


   Mapper Life Cycle
   -----------------

	1. setup
		=> Called 'only once' at the beggining before the map method is called.
		=> Define initialization code here.

	2. map
		=> Called after setup method.
		=> Called once per every input (K, V) pair.

	3. cleanup
		=> Called 'only once'  after map is called for all input records.
		=> Do any cleaup tasks, clean-up your resources, connections etc. 

	4. run  (not generally overridden)


   Reducer Life Cycle
   ------------------
	1. setup
		=> Called 'only once' at the beggining before the reduce method is called.
		=> Define initialization code here.

	2. reduce
		=> Called after setup method.
		=> Called once per every input (K, Iterable[V]) pair.

	3. cleanup
		=> Called 'only once'  after reduce is called for all input records.
		=> Do any cleaup tasks, clean-up your resources, connections etc. 

	4. run  (not generally overridden)


    Facebook Dataset Analysis
    -------------------------	
     
     Use-Case: From facebook dataset, find out the record with highest 'likes_received' from
	       among the users who are born between 1990 and 2000.

     Two MR programs:
	1. age mapper => Filters the records with dob_year between 1990 and 2000
	2. maxlikes mapper  => Filters the record with highest maxlikes from the output of age mapper.

	In this example there is no need of reduce as this is simple filtering operation.
	No aggregation is involved, hence no reducer is required. 
 


   Creating MR java project from scratch 
   --------------------------------------

	=> Create a Java Project
	=> Add the following "external jar files" to build path of the Java project
	   -> /usr/lib/hadoop/hadoop-common.jar
	   -> /usr/lib/hadoop/hadoop-0.20-mapreduce/hadoop-core.jar

 ===============================================
     Hive
 ===============================================
    
   -> Data Warehousing Platform built on top of Hadoop
   -> Hive used HDFS as storage layer and used MapReduce as its execution engine.

   -> Hive provides an SQL abstraction on top of MR, so that user need not write MR programs

   -> Users can send HQL statements to Hive Engine.
	 -> Hive converts user queries to MR programs and runs them on the cluster.
   
     
    How to work with Hive
    ---------------------

    1. Command Line Interface (CLI) 

	1. hive shell
		-> Connects to local Hive engine. No JDBC required.

	2. beeline shell
		-> preferred in corporate environemnts, as it offers more security
		-> Connects to Hive engine through JDBC connection.

    2. Using Client applications (JDBC/ODBC Drivers)

    3. Hive User Environemnt (HUE) - (uses Beeswax protocol)  


    Hive Components
    ----------------

	1. Hive Warehouse   
	   -> Is a hdfs directory, where Hive stores all the data files of managed tables
	   -> defined in hive-site.xml config file ( /user/hive/warehouse )
	
	   -> We will have a default database called "default" 
	       -> This database will not have any directory of its own 

	   -> We can create user defined databases and tables
		-> Each user-defined database has a directory in hive warehouse

	  Some Notes
          ----------
		=> 'default' database wil not any separate directory in the warehouse.

        2. Metastore
           -> Usually an external service (such as MySQL) where Hve stores its metadata.
	
	
	
    Schema on Read
    ---------------
     => Hive does not validate the data while loading (schema on write is not supported)
     => data is validated against the schema while reading the data
	 -> If the data can not casted to defined types, or is invalid, then Hive will show NULL
	 -> Hive also shows NULL is empty data. 
	
    
  
==== Database ===

 create database ctsdb;
 create database if not exists ctsdb;
 

 drop database ctsdb1;   	  // drop an empty database
 drop database ctsdb1 cascade;    // drop a non-empty database

 // the terms 'database' and 'schema' can be interchangably used.
 create schema ctsdb;
 create schema if not exists ctsdb;
 drop schema ctsdb1;


 ==== Tables ====

create table empinfo(empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
row format delimited
fields terminated by ","
lines terminated by "\n"
stored as textfile;


create table empinfo2(empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
row format delimited
fields terminated by ",";

create table empinfo_tab(empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
row format delimited
fields terminated by "\t";


Describe Table 
--------------
desc empinfo;
desc formatted empinfo;
desc extended empinfo;


loading data from data files
----------------------------

// loading from local file system -- COPY operation

load data local inpath '<file-path>' into table <table-name>    	// append mode
load data local inpath '<file-path>' overwrite into table <table-name>  // overwrite mode


// loading from hdfs -- MOVE operation

load data inpath '<file-path>' into table <table-name>    	  // append mode
load data inpath '<file-path>' overwrite into table <table-name>  // overwrite mode

examples
--------
load data local inpath '/home/cloudera/hive/Hive/data/emp.txt' into table empinfo; 
load data local inpath '/home/cloudera/hive/Hive/data/emp.txt' overwrite into table empinfo; 
load data local inpath '/home/cloudera/hive/Hive/data/emp2.txt' into table empinfo; 
load data local inpath '/home/cloudera/hive/Hive/data/emp_tab.txt' into table empinfo_tab; 


   Hive Table Types
   ----------------
	=> Managed tables
	=> External tables


   Hive Managed Tables
   -------------------
	-> Tables managed by hive
	-> When you srop a managed table, both schema as well as data are droped. 


   Hive External Tables
   --------------------
	-> The data of external tables is not managed by Hive.
	-> External tables are created in a user-specified location (outside of hive warehouse directory)

	-> When you drop an external table:
		-> ONly the schema of the table is dropped from the metastore
		-> The datafiles of the table are not dropped. 
		
  Partitioned Tables
  ------------------

    In Hive, Partitions provide a way of dividing a table into coarse-grained parts 
    based on the value of a partition column, such as a date or country.

    => We can load data into partitioned tables using two approaches:
	
	1. Static Partitioning

	   -> When we know precisly into which partition we are loading the data.
	   -> We can 'load data' command to load data using static partitioning.

	2. Dynamic Partitioning

	   -> When a specific data file can not loaded into any given partition
		-> Your file may have partition data also and may have data for several partitions.

	   -> Here, the data must be loaded from a base table using "INSERT .. SELECT" command
		-> We can not use 'load data' command in this case.



CREATE TABLE students_partition_static (id INT, name STRING, mobile STRING, email STRING, project STRING)
PARTITIONED BY (country STRING)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE

show partitions students_partition_static;

CREATE TABLE students_partition_dynamic (id INT, name STRING, mobile STRING, email STRING, project STRING)
PARTITIONED BY (country STRING, batch INT)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
STORED AS TEXTFILE

CREATE TABLE students_partition_dynamic_tab (id INT, name STRING, mobile STRING, email STRING, project STRING)
PARTITIONED BY (country STRING, batch INT)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE

== loading data into partitioned tables ====

load data local inpath '/home/cloudera/hive/Hive/data/students_IN.txt' into table students_partition_static 
partition (country = 'UK')

-- loading data using dynamic partitioning method ---


-- load data into a base table
load data local inpath '/home/cloudera/hive/Hive/data/students_country_batch.csv' into table students;

-- make dynamic partition mode to nonstrict
set hive.exec.dynamic.partition.mode=nonstrict;

-- load data into partitioned table using insert .. select command
insert into students_partition_dynamic
partition(country, batch)
select id, name, mobile, email, project, country, batch from students;

insert into students_partition_dynamic
partition(country='India', batch)
select id, name, mobile, email, project, batch from students
where country = 'India';

insert into students_partition_dynamic
partition(country='India', batch=1)
select id, name, mobile, email, project from students
where country = 'India' and batch=1;

// ERROR
// Line 2:10 Dynamic partition cannot be the parent of a static partition '1'
insert into students_partition_dynamic
partition(country, batch=1)
select id, name, mobile, email, project, country from students
where batch=1;


// insert overwrite => overwrite mode
insert overwrite table students_partition_dynamic
partition(country, batch)
select id, name, mobile, email, project, country, batch from students;

  NOTE: The data files of the partitioned tables does not contain partition column info
	The partition info comes from directory-name.


    Bucketting of Tables
    ---------------------
     => Bucketting adds an additional layer of granularity at the table or at the partition level 
	where Hive creates fixed number of bucketted files based on hash of a column (called clustered column)


--- bucketted table ---
CREATE TABLE students_bucket (id INT, name STRING, mobile STRING, email STRING, project STRING, country STRING, batch INT)
CLUSTERED BY (id) 
INTO 4 BUCKETS;

CREATE TABLE students_bucket_pipe (id INT, name STRING, mobile STRING, email STRING, project STRING, country STRING, batch INT)
CLUSTERED BY (country) 
SORTED BY (id) 
INTO 3 BUCKETS
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

--loading data into bucketted table---
insert into students_bucket_pipe select * from students; 

-- bucketting a partitioned table
CREATE TABLE students_part_bucket (id INT, name STRING, mobile STRING, email STRING, project STRING, country STRING)
PARTITIONED BY (batch INT)
CLUSTERED BY (id) INTO 3 BUCKETS
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
STORED AS TEXTFILE

insert into students_part_bucket 
partition(batch)
select id, name, mobile, email, project, country, batch from students;


 Sampling Commands
 -----------------

   Three types of sampling queries:

	-> Bucket sampling  -> we sample for some number of buckets

		select * from students_bucket TABLESAMPLE (bucket 1 out of 2 on id) ;
		select * from students_bucket TABLESAMPLE (bucket 2 out of 5 on id) ;

	-> Block Sampling   -> we sample for some number of blocks

		select * from students TABLESAMPLE (20 percent)    // block sample
		select * from students TABLESAMPLE (100M)  

	-> Row Sampling     -> we sample for some rows from each input-split.

		select * from students TABLESAMPLE (10 ROWS) 


  Managing Outputs
  ----------------

    Three ways are there to save the output of a query:
	
	1. You can save the output to a table

		INSERT OVERWRITE TABLE students2 SELECT * FROM students

	2. You can save the output of a query to a local (linux) directory  

		insert overwrite local directory '/home/cloudera/hive_data/students' 
		select * from students;

		insert overwrite local directory '/home/cloudera/hive_data/students_pipe' 
		row format delimited
		fields terminated by '|'
		select * from students;

	3. You can save the output of a query to an HDFS directory

		insert overwrite directory '/user/cloudera/hivedata/students_pipe' 
		row format delimited
		fields terminated by '|'
		select * from students;



   Hadoop File Formats
   -------------------

	1. Sequence File   -> Stores data as  (K, V) pairs

	2. Parquet	   -> Columnar format file
			   -> Saves the content of each column physically in one location. 

	3. ORC	   	   -> Columnar format file
			   -> Saves the content of each column physically in one location. 

	4. Avro		   -> Supports dynamic schema evolutions.
			      ( Schema of the data is stored as json in the file itself)
	   

   SORTING
   --------

   	select * from students order by name
		=> Order by produces parallel global sort		

	select * from students sort by name
		=> Produces one sorted file per reducer

	select * from students distribute by country sort by name desc
		=> Each reducer get data partitioned by country

	select * from students cluster by country
		=> Short form of distribute by country sort by country'

    
   Multi-Table insert
   ------------------

   from students
	insert into students_usa 
	select * where country = 'USA'
	insert into students_uk 
	select * where country = 'UK'
	insert into students_india 
	select * where country = 'India';


   Complex Data Types  
   ------------------
    CREATE TABLE complex_data
	(col1 array<string>,
 	col2 map<int,string>,
 	col3 struct<name:string,score:int>)
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY ','
    COLLECTION ITEMS TERMINATED BY '/'
    MAP KEYS TERMINATED BY ':'
    LINES TERMINATED BY '\n'
    STORED AS TEXTFILE;


complex data to load
--------------------
Apple/Grapes/Orange,4306:Sneha/4307:Renu,Sneha/90
Apple/Microsoft/Google,4306:Renu/4307:Anita,Raju/100
Apple/Micromax/Samsung,4306:Raju/4307:Harsha,Harsha/80


load data local inpath '/home/cloudera/hive/Hive/hive_complex_data' into table complex_data


  queries
  -------
  select col1, col2, col3 from complex_data;
  select col1[0], col1[1], col2[4306], col2[4307], col3.name, col3.name from complex_data;


 Table Properties
 ----------------

show tblproperties emp_tp_metadata ;

show tblproperties emp_tp_metadata('numFiles')

CREATE TABLE emp_tp_metadata 
(empid INT, empname STRING, salary DOUBLE, deptid SMALLINT) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ',' 
LINES TERMINATED BY '\n' 
STORED AS TEXTFILE 
TBLPROPERTIES ('creator'='raju', 'created_at'='2020-03-01 10:00:00', 'created_for'='for demo');

load data local inpath '/home/cloudera/hive/Hive/data/emp.txt' into table emp_tp_metadata;


 Immutable Tables
 -----------------
 
CREATE TABLE emp_tp_immutable
(empid INT, empname STRING, salary DOUBLE, deptid SMALLINT) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ',' 
LINES TERMINATED BY '\n' 
STORED AS TEXTFILE 
TBLPROPERTIES ("immutable"="true")

show tblproperties emp_tp_immutable("immutable");

LOAD DATA LOCAL INPATH '/home/cloudera/hive/emp.txt' INTO TABLE emp_tp_immutable;

INSERT INTO TABLE emp_tp_immutable SELECT * FROM empinfo WHERE empid = 101;

INSERT OVERWRITE TABLE emp_tp_immutable SELECT * FROM empinfo WHERE empid = 101;

TRUNCATE TABLE emp_tp_immutable;

SELECT * FROM emp_tp_immutable;

INSERT INTO TABLE emp_tp_immutable SELECT * FROM empinfo;

SELECT * FROM emp_tp_immutable;

ALTER TABLE emp_tp_immutable SET TBLPROPERTIES ("immutable"="false")

INSERT INTO TABLE emp_tp_immutable SELECT * FROM empinfo WHERE empid = 101

 -----------------------------

   Transactional Tables
   --------------------

	Is a special hive table that supports ACID properties (supports updated and deletes)

	-> table property "transactional" should be set to "true"
	-> the table storage format must be ORC format.   (STORED AS ORC)
	-> the table must be 'bucketted' 
	-> a lot of config params have to set:

		set hive.support.concurrency = true;
		set hive.enforce.bucketing = true;
		set hive.exec.dynamic.partition.mode = nonstrict;
		set hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
		set hive.compactor.initiator.on = true;
		set hive.compactor.worker.threads = 1;

CREATE TABLE user_acid (userid int, name string, age int, gender string)
CLUSTERED BY (userid) INTO 5 buckets 
STORED AS ORC
TBLPROPERTIES('transactional'='true');

INSERT INTO user_acid(userid, name, age, gender) VALUES 
(1, 'Raju', 40, 'male'), 
(2, 'Raghu', 42, 'male'),
(3, 'Vinay', 20, 'male'), 
(4, 'Amrita', 14, 'female'), 
(5, 'Aditya', 14, 'male'), 
(6, 'Keertana', 12, 'female'), 
(7, 'Pranav', 24, 'male'), 
(8, 'Hari', 24, 'male'), 
(9, 'Syamala', 23, 'female')

SELECT * from user_acid;

UPDATE user_acid SET age = 41 where name='Raju';

DELETE FROM user_acid WHERE userid = 9;
=================================================================

  Joins
  -----

   Hive supports:

   inner, left-outer, right-outer, full-outer, left-semi.


Joins
-----
create table emp(empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
row format delimited
fields terminated by ","
lines terminated by "\n"
stored as textfile;

create table dept(deptid INT, deptname STRING)
row format delimited
fields terminated by ","
lines terminated by "\n"
stored as textfile;

 load data local inpath '/home/cloudera/hive/Hive/data/emp.txt' into table emp;
 load data local inpath '/home/cloudera/hive/Hive/data/dept.txt' into table dept;

 select emp.*, dept.* from emp join dept on emp.deptid = dept.deptid;
 //select emp.*, dept.* from emp, dept where emp.deptid = dept.deptid;

 select emp.*, dept.* from emp left outer join dept on emp.deptid = dept.deptid;
 select emp.*, dept.* from emp right outer join dept on emp.deptid = dept.deptid; 
 select emp.*, dept.* from emp full outer join dept on emp.deptid = dept.deptid; 


  Left Semi Join
  ---------------
	-> Is same as inner join, but the data comes ONLY from the left side table. 
	-> Is same as the following sub-query:
		select * from emp where deptid in (select deptid from dept)	

  select emp.* from emp left semi join dept on emp.deptid = dept.deptid; 


  Explode & Lateral Views
  ========================

create table lateral_view (id int,  phones array<string>, emails array<string>)
row format delimited
fields terminated by '\t'
collection items terminated by ','

1	1111111,11112222	email11@gmail.com,email1122@gmail.com
2	2222222,22223333	email22@gmail.com,email2222@gmail.com
3	3333333,33333444	email33@gmail.com,email3322@gmail.com
4	4444444,44445555	email44@gmail.com,email4422@gmail.com
5	5555555,55556666	email55@gmail.com,email5522@gmail.com
6	6666666,66667777	email66@gmail.com,email6622@gmail.com
7	7777777,77777888	email77@gmail.com,email7722@gmail.com

###########################################

 Lateral View & Explode

###########################################

create table tab_lateral (id int, phones array<string>, emails array<string>)
row format delimited 
fields terminated by '\t'
collection items terminated by ','
stored as textfile;

load data local inpath '/home/cloudera/hive/Hive/data/lateral_view_data.txt' overwrite into table tab_lateral;

# note: expode can not be merged with any other columns as is.
# use lateral views if you want to do so.

select explode(phones) from tab_lateral;

select id, phones_l from tab_lateral 
lateral view explode(phones) p as phones_l;

select id, phones_l, emails_l from tab_lateral 
lateral view explode(phones) p as phones_l
lateral view explode(emails) e as emails_l;

--------------------------------------------------------

-- This results in cross product of array values, which is not a good data representaion
-- Lets breakdown the data properly

create temporary table if not exists tab_lateral_tmp_phones
(rowid int, id int, phone string)
row format delimited
fields terminated by '|';

insert overwrite table tab_lateral_tmp_phones
select row_number() over(), id, phones_l from tab_lateral 
lateral view explode(phones) p as phones_l;

create temporary table if not exists tab_lateral_tmp_emails
(rowid int, id int, email string)
row format delimited
fields terminated by '|';

insert overwrite table tab_lateral_tmp_emails
select row_number() over(), id, emails_l from tab_lateral 
lateral view explode(emails) p as emails_l;

create table if not exists tab_lateral_breakdown
(rowid int, id int, phone string, email string)
row format delimited
fields terminated by '|';

insert overwrite table tab_lateral_breakdown
select a.rowid, a.id, a.phone, b.email
from   tab_lateral_tmp_phones a, tab_lateral_tmp_emails b
where  a.rowid = b.rowid and a.id = b.id;


 Window Functions
 ----------------
		
     empid	dept	salary  sum(salary)
     -------------------------------------------
	
	104	IT	45000	 NULL
	111	IT	45000 	 NULL  		 
	110	IT	60000 	45000 
	101	IT	60000	45000 	
	105	IT	80000   60000  
	
	102	Sales	45000		
	107	Sales	50000		
	106	Sales	60000		

	108	HR	35000		
	103	HR	35000		
	109	HR	90000		
	
       window  => partition by dept rows between 1 preceeding and 1 following
	
      select empid, dept, salary, sum(salary) over(partition by dept rows between unbounded preceeding and current row) 
      from emp;

	 ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
	 ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING
	 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING
	 ROWS BETWEEN 2 PRECEDING AND CURRENT ROW 
	 ROWS BETWEEN CURRENT ROW AND 2 FOLLOWING

  Running a script file in Hive
  ------------------------------------
  NOTE: Run this from command prompt (not from within the shell)

  hive -f '/home/cloudera/hive/Hive/churn_modelling.hql'


  Running Hive commands in interactive mode
  ----------------------------------------------
  NOTE: Run this from command prompt (not from within the shell)
 
  hive -e 'select * from ctsdb1.empinfo'


 ========================================  
     Impala
 ========================================  

   => Impala does not support loading data from local file system
	-> LOAD DATA LOCAL INPATH ..  command is not supported.

   => Any metadata changes happened outside of impala such as creating databases, tables etc
      will not be reflected automatically in impala. You have to run the 'invalidate metadata'
      command to refresh metadata changes.

		=> invalidate metadata;

   => Whenever data is loaded into the tables from outside of impala, the data changes are not
      auto matically reflected in impala. You have to run 'refresh <table-name>' command

		=> refresh empinfo2;

   When to use Impala ?
   --------------------

    => Because Impala uses in-memory computations, it does not provie run-time fail-safety.
        -> If any of the servers/nodes running the impala queries fail, the job will be aborted.

    => Use Hive for long-running queries
       Use impala for short-running queries

    => Hive is more suited for long running batch jobs, such those involving ETL. 

    => Impala is good for real-time queries while Hive is recommended for large batch jobs.
	  

 ========================================  
     Sqoop
 ======================================== 

    => Structured Data Ingestion Tool  (SQL on Hadoop)

    => Sqoop fecilitates imports (to Hadoop) and export (from hadoop) data to/from RDBMS databases.

  IMPORT
  ------
    sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users -m 1 --target-dir /user/cloudera/sqoop2/users1  

    NOTES: The default file format : Text File
	   The defualt delimiter: comma (,)


   sqoop list-databases --connect jdbc:mysql://localhost:3306 --username root -P

   sqoop list-tables --connect jdbc:mysql://localhost:3306/ctsdb --username root -P


   ========== IMPORT ================

   Import Modes:

	1. delete mode (overwrite mode)
		--delete-target-dir

	2. append mode 
		--append

   sqoop import --connect jdbc:mysql://localhost:3306/empdb
              --username root
              --password <password>
              --table emptable
              -m 1
              --target-dir /user/cloudera/empdb
              --fields-terminated-by ','
              --lines-terminated-by '\n'

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users -m 1 --target-dir /user/cloudera/sqoop2/users1  
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users -m 1 --target-dir /user/cloudera/sqoop2/users_tab --fields-terminated-by '\t'
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users -m 1 --target-dir /user/cloudera/sqoop2/users_pipe --fields-terminated-by '|'
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users --columns 'id,name,phone' -m 1 --target-dir /user/cloudera/sqoop2/users2 --fields-terminated-by ','
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students -m 1 --target-dir /user/cloudera/sqoop2/students
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students -m 1 --target-dir /user/cloudera/sqoop2/students2 --direct

-- import data with query --
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P -m 1 --target-dir /user/cloudera/sqoop2/students4 --query "select * from students where id <= 25 AND \$CONDITIONS"
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P -m 1 --target-dir /user/cloudera/sqoop2/students5 --query "select country, count(*) from students where 1=1 AND \$CONDITIONS group by country"

sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students -m 1 --target-dir /user/cloudera/sqoop2/students6

-- append mode appends new files to the same directory
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students -m 1 --target-dir /user/cloudera/sqoop2/students6 --append

-- overwrire mode (--delete-target-dir) deletes the directory first recreates the directory.
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students -m 1 --target-dir /user/cloudera/sqoop2/students6 --delete-target-dir

-- using enclosing chars ---
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users -m 1 --target-dir /user/cloudera/sqoop2/users3 --enclosed-by '"'

-- using different file formats ---
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users -m 1 --target-dir /user/cloudera/sqoop2/users_seq --as-sequencefile
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users -m 1 --target-dir /user/cloudera/sqoop2/users_avro --as-avrodatafile

-- import data using multiple mappers --
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students -m 4 --target-dir /user/cloudera/sqoop2/students_m4_id --split-by 'id'
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students -m 4 --target-dir /user/cloudera/sqoop2/students_m4_country --split-by 'country'
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table userspk -m 2 --target-dir /user/cloudera/sqoop2/userspk 


   ========== EXPORT ==============


===== EXPORT =====

sqoop export --connect jdbc:mysql://localhost:3306/ctsdb       

             --table emp2 
	     --username root 
	     -P                           			
	     -m 1                                       			

             --export-dir /user/cloudera/sqoop/demo    		

             --input-fields-terminated-by ','

sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop2/users1/ --input-fields-terminated-by ',' 
sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop2/users_pipe/part-m-00000 --input-fields-terminated-by '|' 

sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop2/users2/part-m-00000 --input-fields-terminated-by ',' --columns "id,name,phone"
sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop/users/part-m-* 
sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop/users/

sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop/users2/part-m-00000 --input-fields-terminated-by ',' --columns "id,name,age"
sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop/users2/part-m-00000 --input-fields-terminated-by ',' --columns "age,name,id"

-- this command causes run time exception as the data types are not compatible.
sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop/users2/part-m-00000 --input-fields-terminated-by ',' --columns "id,age,name"
sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop2/users_avro/part-m-00000.avro

-- does not work - sequence file can not be exported
sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop2/users_seq/part-m-00000

-- updating the data in mysql
sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir /user/cloudera/sqoop2/users1/part-m-00000 --update-key 'id'
sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table users2 --username root -P -m 1 --export-dir  /user/cloudera/sqoop/users3/part-m-00000 --update-key 'id' --update-mode allowinsert

--this one works. the table has a primary key in this case ..
sqoop export --connect jdbc:mysql://localhost:3306/ctsdb --table userspk --username root -P -m 1 --export-dir  /user/cloudera/sqoop/users3/part-m-00000 --update-key 'id' --update-mode allowinsert

------ import entire database ------
sqoop import-all-tables --connect jdbc:mysql://localhost:3306/ctsdb --username root -P -m 1 --warehouse-dir /user/cloudera/sqoop2/ctsdb 

------ create hive tables -----------
sqoop create-hive-table --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users --fields-terminated-by ',' --hive-table sqoopdb.users
sqoop create-hive-table --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students --fields-terminated-by '|' --hive-table sqoopdb.students

------ import data into hive tables -----------
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table users --fields-terminated-by ',' -m 1 --hive-table sqoopdb.users --hive-import 
sqoop import --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --table students --fields-terminated-by '|' -m 1 --hive-table sqoopdb.students --hive-import 


------ eval - run sql statement on traget rdbms database ---------
sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'select * from students limit 5'

sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'create table emp (id int, name varchar(30), age int)'

sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'insert into emp (id, name, age) values (1, "Raju", 45), (2, "Ramesh", 35), (3, "Raghu", 25)'

sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'update emp set name = "Kanakaraju" where id=1'

sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'delete from emp where id=1'

sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'truncate emp'

sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'drop table emp'

sqoop eval --connect jdbc:mysql://localhost:3306/ctsdb --username root -P --query 'show tables'


------- codegen command --------------
sqoop codegen --connect jdbc:mysql://localhost:3306/ctsdb --table users --class-name Users --username root -P

=========================================
    HBase 
=========================================
 
  HBase is a Transactional & Non-relational database built for Hadoop.

     NoSQL Databases
    
      	-> Distributed databases that run on clusters
	-> Non-relational in design
		-> Follow denormalized table design
		-> Does not support joins
	-> Does not follow regid schema
		-> Supports schema flexibility.
	 
    Advantages of NoSQL
  	-> Support unlimited scalability.
	-> Support schema evolution

    Types of NoSQL databases:
	1. Document oriented databases 		   - MongoDB, CouchDB
	2. Wide Column / Column-family databases   - Cassandra, HBase
	3. Key-value stores			   - Redis, DynamoDB
	4. Graph Databases			   - Neo4J 


   Hadoop's limitations for transcational processing:

	1. Unstructured Data
	2. No Random Access - ability to access and modify individual records
	3. High Latency 
	4. Not ACID complaint
   
   HBase
  
     	-> is a ditributed database management system that is part of Hadoop ecosytem.
	-> based on "Google Big Table" 
	-> Uses HDFS to store database
		-> distributed storage
		-> fault tolerant

   Hbase Vs RDBMS
   
	-> HBase does not support SQL
	
	-> HBase only supports a basic set of operations (CRUD) on a "single row"
		-> All operations are applied at row-level
		-> No JOINs
		-> No Group By, No Sub-queries

  	-> Denormalized design

	-> Uses column-family oriented storage
		-> All data of a single column family is physically stored together
	
	-> Limited ACID supports
		-> ACID complaint at row-level.
	
   HBase Data Model
   ----------------

        -> A table consists of column-family
	       -> Columns are not part of table definition
	-> A row is defined by arbitrary number of columns with the same "Row-Id"
		-> Each column belongs to row-id, column-family
		-> A column has key ( cf:col_name ), timestamp, and a value
        -> HBase manitains multiple versions of each column.
	-> Data is stored in sorted order
	      -> order of sorting -> rowid, column-family, column




  create 'emp', 'contact', 'basic', 'skills', 'salary', 'other' 

  create 't1', {NAME => 'f2', VERSIONS => 2}, {NAME => 'f1', VERSIONS => 5}

----------------------------------------------------------
   
put 'emp', '1', 'basic:empid', '100'
put 'emp', '1', 'basic:name', 'Raju........'
put 'emp', '1', 'basic:age', '45.......'
put 'emp', '1', 'contact:phone', '9246657890......'
put 'emp', '1', 'contact:email', 'raju@gmail.com.......'
put 'emp', '1', 'skills:rdbms', 'mysql,oracle.......'
put 'emp', '1', 'salary:basic', '40000'


put 'emp', '2', 'basic:empid', '101'
put 'emp', '2', 'basic:name', 'Komala'
put 'emp', '2', 'basic:dob', '1980-01-01'
put 'emp', '2', 'contact:mobile', '9988779988'
put 'emp', '2', 'contact:email', 'komala@gmail.com'
put 'emp', '2', 'skills:nosql', 'hbase.mongodb'
put 'emp', '2', 'salary:basic', '45000'
put 'emp', '2', 'other:state', 'TS'
put 'emp', '2', 'other:city', 'Hyderabad'


put 'emp', 'A10', 'basic:empid', '103'
put 'emp', 'A10', 'basic:fname', 'Mahesh'
put 'emp', 'A10', 'basic:dob', '2000-02-01'
put 'emp', 'A10', 'contact:city', 'Hyderabad'
put 'emp', 'A10', 'contact:state', 'TS'
put 'emp', 'A10', 'contact:email', 'ramesh@gmail.com'
put 'emp', 'A10', 'skills:nosql', 'hbase'
put 'emp', 'A10', 'skills:rdbms', 'oracle'
put 'emp', 'A10', 'salary:basic', '40000'
put 'emp', 'A10', 'salary:travel', '5000'
put 'emp', 'A10', 'salary:hra', '10000'


-------------------------------------------------
scan 'emp'    // show all the data
scan 'emp', {COLUMNS=>['basic:empid', 'basic:name']}
scan 'emp', {COLUMNS => ['basic:empid', 'basic:name', 'salary:basic'], TIMERANGE => [1634646583848, 1634647619374]}
-------------------------------------------------
get 'emp', '1'
get 'emp', '1', {COLUMN => ['basic:empid', 'basic:fname', 'basic:dob', 'contact:city']}
get 'emp', '1', {COLUMN => ['basic:empid', 'basic:name', 'basic:dob', 'contact:city'], VERSIONS => 3}
get 't1', '1', {COLUMN => ['f1:name', 'f2:age'], VERSIONS => 5}

----------------------------------------------------

  there is no 'update' command
  put command acts as both insert as well as update

put 'emp', '1', 'basic:empid', '100'
put 'emp', '1', 'basic:name', 'Raju'
put 'emp', '1', 'basic:age', '45'
put 'emp', '1', 'contact:phone', '9246657890'
put 'emp', '1', 'contact:email', 'raju@gmail.com'
put 'emp', '1', 'skills:rdbms', 'mysql,oracle'
put 'emp', '1', 'salary:basic', '40000'

get 'emp', '1', {COLUMN => ['basic:empid', 'basic:age'], VERSIONS => 5}


##------------ alter an hbase table -------------------
alter 'emp', NAME => 'basic', VERSIONS => 4 
alter 't1', NAME => 'f1', VERSIONS => 3 
alter 't1', NAME => 'f3', VERSIONS => 3
alter 't1', NAME => 'f3', METHOD => 'delete' 

----- disable ----------
disable 'emp'

----- enable ----------
enable 'emp'

------- delete --------------------
delete 'emp', '1', 'basic:age'

-------  truncate  ----------------
truncate 't1'

----- drop  ----------
disable 't1'
drop 't1'

-------- Hive & HBase Integration ------------

# run this from linux while launching hive shell - this adds the required jar files to the hive path for that session
$ hive --auxpath /usr/lib/hive/lib/hive-hbase-handler-0.10.0-cdh4.7.0.jar,/usr/lib/hive/lib/hbase.jar,/usr/lib/hive/lib/zookeeper.jar,/usr/lib/hive/lib/guava-11.0.2.jar


CREATE TABLE hiveempinfo(key int, empname string, salary double, deptid int) 
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' 
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key, emp:empname, emp:salary, dept:deptid") 
TBLPROPERTIES ("hbase.table.name" = "hbaseempinfo");

insert into table hiveempinfo select * from empinfo;


 =================================================
    Kafka 
 =================================================

    Event Streaming :
    ----------------

     -> A technological foundation for processing data in real time, where businesses are
        increasingly software dependent and automated. 

     -> Users of software are more software. 

     -> We have to store these event streams durably for processing and reacting to events
        in real-time as well as 'retrospectivly'.
     
     Use Cases of Event Streaming
    
       -> Processing payments and transactions in real time (banks, stock exchanges etc)
       -> Track and monitor trucks. (logistics & automotives)
       -> Capture and analyze sensor data from IoT devices (factories, hospitals, smart homes)
       -> Collect and immediatly react to customer interactions


   Data Pipelines
   --------------      
     -> Is a point to point communication channel that facilitated data transfer in real time
        between desperate applications that may use dfifferent data formats, protocols etc. 


   Messaging System
   ----------------

      -> A messaging system is a system that is used for transfering data from one application to another
         so that applications can focus on data and not on how to share the data with other applications. 
    	
      Two types of messaging systems:

	 -> Point to Point (Queues) -> Only one consumer per event
	 -> Publisher-Subscriber messaging systems -> events are not intended for any consumer

  
   What is Kafka ?
   ---------------	
	-> Is a distributed messaging system and streaming platform.
	   
            1. Publish & Subscribe streams of events
	    2. Store the records (durably) in a fault tolerent ways. 
	    3. Process streams of records in real-time       

   Kafka Basics
   -------------

     1. Zookeeeper (ZK)		
	-> Is a distrbuted coordination service which montitors all the brokers 
	   in a cluster

     2. Broker
	 -> Is a Kafka service running on each node in the Kafka cluster	
	 -> Each broker in a cluster should have a unique broker-id
	 -> Each brokers stores one or more partitions of topics
  
     3. Cluster
	 -> When Kafka has more than one broker coordinated by the same ZK, then it is called
	    a Kafka cluster.
	 -> A Kafka cluster can be expanded without download. 

     4. Topics
	 -> Is a feed/category to which records are published by produces and can be consumed 
            by one or more consumers. 
         -> For every topic, Kafka maintains topic partition logs (distributed commit logs)
	 -> All the messages are stored in a "serialized format"
	 -> The topic messages will be retained for 7 days (168 hours)

     5. Partitions
	 -> A single can have many partitions distributed across many brokers
	 -> A single broker can have any number of partitions
	 -> Having lot of partitions allow Kafka to handle arbitrary amount of data.

     6. Partition-offset
	  -> Each message in a partition has a unique sequence-id called 'offset'

     7. Replicas
	  -> Backups of partition.
	  -> Are used to prevent data loss.
	  -> The replica-id of a partition is same as the broker-id.
	  -> Only one replica acts as a "leader" replica
		-> All reads (from consumers) and writes (from producers) are served only by 
                   the leader replica	
    8. Producers
	  -> Publish messages to leader replicas of the topic partitions.
	  -> Messages are published in serialized format
	  -> A single producer can publish messages to mulitple topics

    9. Consumers
	 -> Consumers subscribe to one or more topics (or topic partitions), poll the messages
	    and processes the messages
	 -> A single consumer can read messages from mulitple topics	
	

   Getting started with Kafka
   --------------------------

     1. Make sure you are running Java 8 (jdk 1.8)

     2. Download Kafka binaries from URL: https://kafka.apache.org/downloads

     3. Extract the tar file to a suitable directory. Thats it!!

     Important directories:

	1. bin      -> contains all the shell scripts and windows batch files.
	2. config   -> all the configuration files are located here
	3. libs	    -> all the libraries (jar file) are located here.


        Important PORTS:
		-> Kafka Broker: 9092
		-> Zookeeper:    2181

   1. Start the zookeeper service

	bin/zookeeper-server-start.sh config/zookeeper.properties

   2. Start the Kafka broker service

	bin/kafka-server-start.sh config/server.properties

	Important Configurations:	
		-> broker.id=0
		-> listeners=PLAINTEXT://:9092
		-> log.dirs=/tmp/kafka-logs

		-> log.retention.hours=168
		-> zookeeper.connect=localhost:2181

  3. Working with Topics - Topic Operations
	
	List existing topics:
		bin/kafka-topics.sh --bootstrap-server localhost:9092 --list

	Create a topic:
		bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic demotopic --partitions 3 --replication-factor 1
	
	Describe a topic:
		bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic demotopic 

        Alter a topic:
		bin/kafka-topics.sh --bootstrap-server localhost:9092 --alter --topic ctstopic1 --partitions 4

	Delete a topic
		bin/kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic ctstopic1
	


   4. Start a Kafka Console Producer

	  bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic demotopic 

	  bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic demotopic --property "parse.key=true" --property "key.separator=:"

   4. Start a Kafka Console Consumer
	
	bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic demotopic 

	bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic demotopic --property print.key=true --property print.value=true --property key.separator=" | "


   Single-Node, Multiple-Broker Configuration
   ------------------------------------------

    I am creating two additional versions on server.properties file (server1.properties & server2.properties)
    with the following config changes..
			
  	file		    broker.id	listeners			log.dirs
	---------------------------------------------------------------------------------
	server1.properties	0	listeners=PLAINTEXT://:9092	/tmp/kafka-logs
	server1.properties	1	listeners=PLAINTEXT://:9093	/tmp/kafka-logs-1
	server2.properties	2	listeners=PLAINTEXT://:9094	/tmp/kafka-logs-2


    Message distribution Logic
    --------------------------

	1. If the message has a partition-id specified, the message goes to that partition

	2. If the message has a 'key'

		2.1 If you defined a custom-partitioner, it will be applied on the key, to determine
		    the partition to which the message goes to.

		2.2 If there is no custom partitioner, the default hash partitioner is used. 

        3. If the message has no key, the messages districuted to all partitions in a load balanced 
	   manner.
		

   Consumers - Standalone Consumer & Consumer Groups  
   --------------------------------------------------

    Standalone consumer is a consumer that consumes messages from all the partitions of a topic
    or from one or more partitions of a topic

    NOTE: Having a lot of partitions for a topics allows us to launch multiple instances of
	  standalone consumers, each consuming from 1 partition, there by allowing us to
	  increase consumer side scalability. 
	
	
   Consumer Groups
   ---------------
	-> One or more consumer instances subscribing to the same topic using the same group-id form
	   a 'consumer group'
	-> The partitions of the topics are distributed to the consumers in the consumer group.
    
        Partition Rebalance
	-------------------
	-> Changing the ownership of partitions among the existing consumers in the group is 
	   called 'rebalance'
	-> Rebalance happens automatically when a new consumer joins a group or an existing
           consumer leaves a group.
	
     Commands
     --------	
	bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list 
	bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group demogroup1 
	bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --topic demotopic --group demogroup1 --reset-offsets --shift-by -10 --execute
	bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --topic demotopic --group demogroup1 --reset-offsets --shift-by 10 --execute 
	bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --topic demotopic --group demogroup1 --reset-offsets --to-earliest --execute 
	bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092  --topic demotopic --group demogroup1 --reset-offsets --to-offset 25 --execute 
    
    Kafka APIs
    ----------

    1. Producer API
	  -> Allows us to write client application that listen to real time streaming data sources
	     produce the messages to Kafka

    2. Consumer API
	 -> Allows us to write client applications that subscribe to kafka topics (topic partitions)
	    and consume those messages. 

    3. Streams API
	-> Allows us to write client applications that consumer from kafka topic(s) and produce
	   to other kafka topic(s)

    4. Connect API
	-> Allows us to write connector application that move the data in real time from one
	   source to one sink using Kafka as intermediate storage system.
	
	
   
   Kafka Producer API
   ------------------

    What can a Record (message / event) have in Kafka procuder api.

	1. topic  (required)
	2. value  (required)
	3. key
	4. partition
	5. timestamp


   Steps to write a Kafka Producer application
   -------------------------------------------
   
     1. Define Kafka Producer properties

	   -> bootstrap.servers
	   -> key.serializer
	   -> value.serializer

	   Properties kafkaProps = new Properties();
	   kafkaProps.put("bootstrap.servers", "localhost:9092,localhost:9093,localhost:9094");
	   kafkaProps.put("key.serializer","org.apache.kafka.common.serialization.StringSerializer");         
	   kafkaProps.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

    2. Define KafkaProducer Object using the properties

	   Producer<String, String> producer = new KafkaProducer<>(kafkaProps);

    3. Construct the ProducerRecord object.

	   key = "A1";
	   value = "Hello";
	   ProducerRecord<String, String> record = new ProducerRecord<>(topicName, key, value); 

     4. Send the ProducerRecord object using KafkaProducer object

	   producer.send(record)


   ProducerRecord definitions
   ---------------------------

	ProducerRecord(String topic, V value)
	ProducerRecord(String topic, K key, V value)
	ProducerRecord(String topic, Integer partition, K key, V value)
	ProducerRecord(String topic, Integer partition, Long timestamp, K key, V value)    


   Ways to send mesages to Kafka
   -----------------------------

	1. Fire & Forget Messaging
		
	    -> Producer produces messages and does not wait for or accept responses from Kafka brokers.

		producer.send( ProducerRecord )

            Advantages:
		-> very fast

	    Disadvanages:
		-> We do not know if some messages fail. 
		-> Messages orderging may not be maintained

      2. Synchronous Messaging
	
	-> Produces produces message and blocks the 'producer.send()' and takes the return value. The 
	   next message is produced only after previous messages is commited to the partitions. 

		RecordMetaData metadata = producer.send( ProducerRecord ).get()

	     Advantages:
		-> We know about what happened to the message
		-> Messages orderging is maintained

	     Disadvanages:
		-> Very slow and can not achive high thorughput. 

     3. Asynchronous messaging
	
	-> Produces messages and attaches a callback method along with the message
	   The callback is trigger in separate thread, when the response does arrive in future.

		producer.send( ProducerRecord, Callback )

	     Advantages:
		-> Very fast
		-> We do acknowledge and process the responses using callback

	     Disadvanages:
		-> Messages orderging may not be maintained


    Producer Configurations
    -----------------------

	1.   bootstrap.servers
	2.   key.serializer
	3.   value.serializer

	4.   acks    
		-> The number of acknowledgements the producer requests the leader to have 
                   received before considering a request as complete.

		acks = 0   	-> no acks at all
		acks = 1	-> ack from only leader replica
		acks = all (-1) -> acks from all ISRs

        5. buffer.memory (def: 32 MB, bytes - 32*1024 * 1024)

        6. batch.size    (bytes - def: 16384 - 16KB)

        7. compression.type   (default: none)
		-> none, gzip, snappy, lz4, zstd	
    
        8. retries   ( default ;  > Kafka 2.x is Int.Max )
		-> Causes the client to resend the records whose send fails with a retriable error. 

		-> Controlled by "delivery.timeout.ms" (2 min)

        9. delivery.timeout.ms  ( default: 120000 )

        10. max.block.ms    (def: 60000)

        11. linger.ms

	12. max.request.size

	13. partitioner.class  (custom partitioner)
       
		
   Consumer API
   ------------
	
     Consumer Types :

	1. Standalone Consumers 
        2. Consumer Groups 

     Consumer Groups:
	-> Automatically assigns partitions to the consumers in a consumer group.

        Partition rebalance
	-> Moving partition ownership from one consumer to another consumer

	-> Happens:
		-> when a consumer joins a group
		-> when a consumer leaves a group
		-> when the partitions of a topic are altered
    

    Creating  a consumer application
    --------------------------------

	1. Define the properties

		-> bootstrap.servers 
		-> key.deserializer
		-> value.deserializer
		-> group.id

		Properties props = new Properties();
		props.put("bootstrap.servers", "localhost:9092,localhost:9093,localhost:9094");		
		props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
		props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");		
		props.put("group.id", "group-demotopic-api");	


        2. Create a KafkaConsumer object

		Consumer<String, String> consumer = new KafkaConsumer<>(props);

	3. Subscribe to one or more topic
	
		consumer.subscribe( Arrays.asList(topicName) );

        4. Start the poll loop - infinite loop from which poll messages are sent. 

		try {
			while (true) { 
			   // this is the pool loop
			}
		}
		catch ( ... ) {
		}
		

        5. Poll for the messages and process them inside poll loop
		
		try {
			while (true) { 
			    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));

			    for (ConsumerRecord<String, String> record : records) {	
				  ... put your business logic here ....
			    }

			    consumer.commitAsync(); 
			}
		}
		catch ( ... ) {			
			consumer.close()
		}


    Kafka Consumer Delivery Semantics
    --------------------------------- 
  
	1. At most once		
		-> Each message is consumed at most once. That means some messages may be not consumed at all
		-> The offsets are commited before the messages are processed.

	2. At least once  (default)
		-> Each message is consumed at least once. That means some messages may be consumed more than once.
		-> The offsets are commited after the messages are processed.

	3. Exactly once
		-> Not supported in Kafka => Sink workloads.
		-> Supported only in Kafka => Kafka workloads.


    AutoCommit Vs Manual Commit
    ---------------------------

	When "enable.auto.commit" is set to "true" (default is "true"), Kafka automatically
	commits the offset and then poll the next batch of messages. This follow "atleast once"
	semantics approach. 

	If you set "enable.auto.commit" to "false", we have to programmatically commit the offsets
	using consumer.commitSync() or consumer.commitAsync() methods.


    Important Consumer Configurations
    ---------------------------------

	1. bootstrap.servers

	2. key.deserializer

	3. value.deserializer

	4. group.id

	5. fetch.min.bytes  (def: 1)	
	      -> controls atleast how much data to pull in each request

	6. max.poll.records (def: 500)
		-> Controls how many messages to poll in each request
		-> increase if the message size is small amd you have lot of RAM

	7. max.partitions.fetch.bytes ( def:  1024*1024 )
		-> max data returned by the broker per partition

	8. fetch.max.bytes (def: 50 MB)
		-> max data returned for each fetch request (covers multiple partitions)

	9. enable.auto.commit  (def: true)

	10. heartbeat.intervel.ms  (def: 3000)

	11. session.timeout.ms (def: 10000)

        12. auto.offset.reset  (def: latest)
		-> possible values: latest, earliest, none
		-> Controls the behaviour as to what to do if the current offset is not 
		   set or is invalid. 
		
 
    Streams API
    -----------

	1. Define the properties objects

		Properties props = new Properties();
        	props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-wordcount");
        	props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        	props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);
        	props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());
        	props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());
       	 	props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "latest"); 

	2. Intialize the stream-builder

		StreamsBuilder builder = new StreamsBuilder();

	3. Create you input stream by reading from an input topic.

		KStream<String, String> source = builder.stream(
        	    "wordcount-input", 
        	    Consumed.with( Serdes.String(), Serdes.String() )
        	);

        4. Apply required transformation on the input KStream and produced the output to
	   destination topic.

		KTable<String, Long> counts = source
            		.flatMapValues(value -> Arrays.asList(value.split(" ")))
            		.groupBy((key, value) -> value)
            		.count();

		counts.toStream().to("wordcount-output", Produced.with(Serdes.String(), Serdes.Long()));


       5. Define KafkaStreams object using the builder and properties

		final KafkaStreams streams = new KafkaStreams(builder.build(), props);

	6. Start the KafkaStreams object

		streams.start()

  =================================================
        NiFi
  =================================================
      
   Apache NiFi supports powerful and scalable directed graphs of :
	-> data routing
	-> transformations
	-> system mediation logic

   Used to automate the flow of data between systems:

	-> JSON ---> Database
           FTP ----> Hadoop
	   Kafka ----> ElasticSearch  ....

    -> Drag & Drop interface

    -> Focus on configuration of Processors
    -> Scalable across cluster of machines
    -> Features:
	  -> Data Buffering
          -> Back Pressure Management
	  -> Proritization of Queues
          -> Latency Vs Throughput ...


  Installing Nifi
  ----------------

  Installing NiFi is very straight forward
	
	-> Download NiFi bianaries as a tar or zip file
	   URL: https://nifi.apache.org/download.html
	-> Extract the downloaded archive into a suitable folderd
	-> That's it !!


  => Let us assume that NiFi is installed in a directory called "nifi-1.11.4" 


 Starting the NiFi
 -----------------

   1. cd nifi-1.11.4

   2. For Windows Users
	-> For Windows users, navigate to the folder where NiFi was installed. 
        -> Within this folder is a subfolder named "bin". 
        -> Navigate to this and double-click the "run-nifi.bat" file.
	-> This will launch NiFi and leave it running in the foreground.
        -> To shut down NiFi, select the window that was launched and Ctrl+C

  3. For Linux/Mac OS X users
	-> $ bin/nifi.sh run    (to start NiFi app in the foreground)
        -> $ bin/nifi.sh status  (to get the status)

   4. Installing NiFi as a service
	-> This is supported only for Linux / Mac

	-> $ bin/nifi.sh install    
             -> This will install with the default service name 'nifi'
        -> $ bin/nifi.sh install dataflow
             -> This will install with the custome service name 'dataflow'

        -> Once installed, the service can be started and stopped as below:
	     -> $ sudo service nifi start    (or sudo service dataflow start)
		$ sudo service nifi stop
		$ sudo service nifi status


   Opening NiFi Web UI
  -------------------

     => Goto:   http://localhost:8080/nifi

     => The port can be changed by editing the nifi.properties file in the NiFi conf directory
     => The default port is 8080.



   Basic Components of NiFi
   ------------------------

	1. Flow File
		-> It is basically the data
		-> Comprises of
			1. Content -> the data itself
			2. Attributes -> (K, V) pairs assciated with the data.
		-> Persisted on the disk.

	2. Processor 
		-> There are ~288 processors in version 1.12
		-> Applies a set of transformations and rules to flow-files to generate
	           new flow files.
		-> Any processor can process any flow file.
		-> Processors are passing flowfile references to each other to 
		   advance the data processing.
		-> They are all running in parallel on different threads.

	3. Connector
		-> Its basically a queue of all the flow files that are yet to be processed
		   by the downstream processor.
		-> Defines rules about how flow files are proritized
		-> Can define backpressure to avoid overflow.

	4. Process Groups
		-> A container for defining a group of processors.

	5. Controller Services
		-> Different services provided by NiFi to help with data manipulations
		-> The controlers can be defined at Global scope (NiFi) or at process group 
		   scope
 

   Use-Cases-1  ==> CSV files to MySQL  
   ------------------------------------      

       -> GetFile (Source Data - CSV file)
	  -> SplitText (splits single FF into many FFs)
            -> ConvertRecord (Convert CSV into JSON FF)               
	       -> ConvertJSONtoSQL (Convert JSON to SQL statement)
                   -> PutSQL (to run SQL against MySQL)

     Configurations
     --------------

     Processor 1: GetFile
		-> Input Directory: source directory where you put CSV files

	GetFile  -> success -> SplitText

     Processor 2: SplitText
		-> Line Split Count: 1
		   Header Line Count: 1

	SplitText -> splits -> ConvertRecord

     Processor 3: ConvertRecord
		-> RecordReader: CSVReader (Controller Service)
		-> RecordWriter: JSONRecordSetWriter (Controller Service)

	CSVReader Properties
	-> Schema Access Strategy: Use String Fields From Header
	   Treat First Line As Header: true

	JSONRecordSetWriter Properties
	-> No changes
	
	ConvertRecord -> success -> ConvertJSONToSQL
	  
     Processor 4: ConvertJSONToSQL	  
	-> JDBCConnectionPool: DBCPConnectionPool (Controller Service)
	   Statement Type: INSERT
	   Table Name: <MySQL Table Name>
	   Catalog Name: <MYSQL Database Name>

	DBCPConnectionPool properties   
	-> Database Connection URL: jdbc:mysql://localhost:3306/<mysql db name>
	   Database Driver Class Name: com.mysql.jdbc.Driver
           Database Driver Locations: <directory path where you placed the mysql driver>
		    (ex: /home/cloudera/nifi-1.11.4/lib/mysql-connector-java-5.1.44.jar)
	   Database User: <mysql user name>
	   Database password: <mysql user password>

       ConvertJSONToSQL -> sql -> PutSQL 

     Processor 5: PutSQL  
	-> JDBCConnectionPool: DBCPConnectionPool (Controller Service)


   Use-Case-2
   ----------
     ==> Kafka - NiFi Integration
 
       -> GenerateFlowFile (Generates random Flow Files at the desired)
          -> PublishKafka (Publishes Flow Files to a Kafka Topic)

       -> ConsumeKafka (Subscribes to Kafka topic and created flow-file)
          -> LogAttribute (LOg the flow files) 
 

     Configurations
     --------------

     Processor 1: GenerateFlowFile
     -> File Size: 100B
	Batch Size: 5
	Unique Flow Files: true

	GenerateFlowFile -> success -> PublishKafka_0_10 1.11.4

     Processor 2: PublishKafka_0_10 1.11.4
     -> Kafka Brokers: localhost:9092
	Topic Name: <kafka topic name>   ex: nifi-topic


    NOTE: No connection between PublishKafka_0_10 1.11.4 & ConsumeKafka_0_10 1.11.4


    Processor 3: ConsumeKafka_0_10 1.11.4
     -> Kafka Brokers: localhost:9092
	Topic Name: <kafka topic name>   ex: nifi-topic
	
	ConsumeKafka_0_10 1.11.4 -> success -> LogAttribute

    Processor 4: LogAttribute
    -> No changes


   NiFi Repositories
   -----------------

   1. Flow Repository
	-> Stores the current state (content & attributes) of each flow file.

   2. Content Repository
	-> Contains all the content present in all flow files of NiFi
        -> This directory uses large disk space

   3. Provenance Repository   
	-> Tracks and stores all the events of all the flowfiles in NiFi.
 
  
  =============================================
     Spark
  =============================================
  
   -> Spark is written in SCALA language
    
   -> Spark is unified in-memory distributed computing framework.

	-> intermediate results can be persisted in memory and subsequent tasks can read
	   the data from memory. 

   -> Spark Unified Framework

        -> Provides a consistent set of API that run on the same execution engine to process various
	   types of analytics workloads.

		Hadoop Ecosystem
                ----------------
		-> Batch Processing of unstructured data    : MapReduce
		-> Batch Processing of structured data      : Hive, Impala, Drill
		-> Stream processing			    : Kafka, Storm, Samza
		-> Predictive analytics (Machine learning)  : Mahout
		-> Graph parallel computations              : Giraph

		Spark
                ----------------
		-> Batch Processing of unstructured data    : Spark Core API (RDDs)
		-> Batch Processing of structured data      : Spark SQL (DataFrames)
		-> Stream processing			    : Spark Streaming
		-> Predictive analytics (Machine learning)  : Spark MLlib
		-> Graph parallel computations              : Spark GraphX


   -> Spark supports multiple programming languages
	-> Scala (native language)
	-> Java
	-> Python  (PySpark)
	-> R

   -> Spark applications can be submitted to multiple cluster manager.	
	-> Spark Standalone scheduler
	-> YARN
	-> Mesos
	-> Kubernetes
	

   Spark Architecture
   ------------------

    1. Cluster Manager (CM)
	-> Applications are submitted to CM
	-> CM lauches the driver and allocates executors

    2. Driver
	-> Master Process
	-> Starts SparkContext (first thing that gets created)
	-> Contains the user code
	-> Sends tasks to the executors
 
    3. Executors
	-> Gets tasks from the driver
	-> Runs the tasks and reports the status to driver
	-> All tasks does the same functionality but on different data.

    4. SparkContext
	-> Represents an application context
	-> Is the link between driver and virious tasks running on the cluster 

	
    Getting started with Spark
    --------------------------   
	
     NOTE: I am using "Anaconda" as my python distribution

     1. Downloading and seting up Spark

	URL: https://spark.apache.org/downloads.html

	Download the Spark binaries file  (like spark-3.0.3-bin-hadoop2.7.tgz)
	Extract it to a suitable location (such as E:\spark-3.0.0-bin-hadoop2.7)

	Setup the environment variables:  (in my case)

	-> SPARK_HOME    : E:\spark-3.0.0-bin-hadoop2.7
 	-> HADOOP_HOME   : E:\spark-3.0.0-bin-hadoop2.7
        -> PYTHONPATH    : %SPARK_HOME%/python;%SPARK_HOME%/python/lib/py4j-0.10.9-src.zip;%PYTHONPATH%
	
	Add the following to your PATH env. variable:

		E:\spark-3.0.0-bin-hadoop2.7\bin
		%SPARK_HOME%\bin
		%SPARK_HOME%\python
		%SPARK_HOME%\python\lib\py4j-0.10.9-src.zip
		%PYTHONPATH%


    2. Setting up PySpark for Spyder / Jupytor Notebook
	
	-> Following the instruction given in the github document.

	    https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

    
    3. Signup to Databricks Community Edition Free Account

	   URL: https://databricks.com/try-databricks
	
	   -> Signup using the above account
	   -> Login to the account
	   => Read the "Explore the Quickstart Tutorial" link.



   RDD (Resilient Distributed Dataset)
   -----------------------------------

	-> A collection of in-memory distributed partition
		-> Each partition is a collection of objects of any type.

	->  RDD has two things

		1. RDD Lineage DAG : A logical plan
				     A set of instructions on how to create the RDD partitions

		2. RDDs data : The set in-memory partitions.

	
	-> RDDs are immutable

	-> RDDs lazily evaluated.
	

    Creating RDDs
    -------------
	Three ways:

	1. We can create RDD from some external text file. 

		rdd1 = sc.textFile( <filePath> )

		=> The default number of partitions is determined by "sc.defaultMinPartitions"
		=> The default value of "sc.defaultMinPartitions" is 2, if your app has atleast 2 cores. 

		rdd1 = sc.textFile( <filePath>, 10 )  # 10 partitions are created

	2. We can create an RDD from programmatic data

		rdd1 = sc.parallelize( range(1, 101) )	

		=> The default number of partitions is determined by "sc.defaultParallelism"
		=> The default value of "sc.defaultParallelism" is equal to the number of cores.

		rdd1 = sc.parallelize( range(1, 101), 5 )  # 5 partitions are created

	3. We can create RDDs by applying transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x+10)	


    What can we do with an RDD ?
    ----------------------------

	Only two things:

	1. Transformations
		-> Output of a transformation is an RDD.
		-> Transformations does not cause execution, but only create lineage DAG.

	2. Actions
		-> Action commands trigger execution of RDDs
		-> RDD's logical plan (lineage DAG) is converted into a physical execution plan and 
		   a set of tasks to compute the RDD partitions are sent to the executors.


    RDD Lineage DAG
    ----------------

	-> RDD Lineage is a logical plan that tracks all the dependencies of an RDD that caused
	   the creation of this RDD all the way from the very first RDD.

	-> RDD Lineage is a set of instructions on how to create the RDD maintained by the driver.

	rdd1 = sc.parallelize( range(1, 101) )
	  => Lineage DAG: rdd1 -> sc.parallelize

	rdd2 = rdd1.map(lambda x: x*2)
	  => Lineage DAG: rdd2 -> rdd1.map -> sc.parallelize
	
        rdd3 = rdd2.filter(lambda x: x > 50)
	  => Lineage DAG: rdd3 -> rdd2.filter -> rdd1.map -> sc.parallelize

	rdd4 = rdd3.flatMap(lambda x: [x, x+1])
	  => Lineage DAG: rdd4 -> rdd3.flatMap -> rdd2.filter -> rdd1.map -> sc.parallelize
	
	rdd4.collect()  => launches a job on the cluster
	
          => sc.parallelize (rdd1) -> map (rdd2) -> filter (rdd3) -> flatMap (r4) --> collect()


   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


   RDD Persistence
   ---------------

     	rdd1 = sc.textFile( ... )
	rdd2 = rdd1.t2( .. )
	rdd3 = rdd1.t3( .. )
	rdd4 = rdd3.t4( .. )
	rdd5 = rdd3.t5( .. )
	rdd6 = rdd5.t6( .. )
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )   
	rdd7 = rdd6.t7( .. )
		
        rdd6.collect()
	lineage -> rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile()		
	sc.textFile (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6) -> collect 

	rdd7.collect()
	lineage -> rdd7 -> rdd6.t7 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile()

	
	3 types of RDD Persistence :

	   -> 1. As in-memory deserialized
	   -> 2. As in-memory serialized
	   -> 3. On Disk


       Storage Levels
       --------------
	
	1. MEMORY_ONLY

	2. MEMORY_AND_DISK

	3. DISK_ONLY

	4. MEMORY_ONLY_2

	5. MEMORY_AND_DISK_2


       Persistence Commands
       --------------------
		
		rdd1.persist()    // default persistence
		rdd1.cache()      // same as persist()
		rdd1.persist( StorageLevel.DISK_ONLY )

		rdd1.unpersist()

    Executor Memory Structure
    --------------------------

	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 	


	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)



   RDD Transformations
   -------------------

   1. map 			P: U -> V
				Element to element transformation
				input RDD : N objects, output RDD : N objects

	rddFile.map(lambda x: x.split(" ")).collect()

   2. filter			P: U -> Boolean
				Will filter elements for which the function returns True.
				input RDD : N objects, output RDD : <= N objects

	rddFile.filter(lambda x: len(x.split(" ")) >= 9).collect()

   3. glom			P: None
				Will return one array with all the elements of each partition.

		rdd1			rdd2 = rdd1.glom()

		P0 : 2,3,1,2,1,3  -> glom  -> P0 : [2,3,1,2,1,3]
		P1 : 4,5,3,5,2,3  -> glom  -> P1 : [4,5,3,5,2,3]
		P2 : 8,9,0,5,1,2  -> glom  -> P2 : [8,9,0,5,1,2]
	
		rdd1.count = 18 (Int)	     rdd2.count = 3 (Array[Int])

  4. flatMap			P: U -> Iterable[V]
				flatMap flattens all the elements of the iterables produced by 	
				the function.

	rddWords = rddFile.flatMap(lambda x: x.split(" "))

  5. mapPartitions		P: Iterator[U] -> Iterator[V]
				A function is applied to the entore partition as a whole.
	
		rdd1		     rdd2 = rdd1.mapPartitions( lambda x: [sum(x)] )

		P0 : 2,3,1,2,1,3  -> mapPartitions  -> P0 : 12
		P1 : 4,5,3,5,2,3  -> mapPartitions  -> P1 : 22
		P2 : 8,9,0,5,1,2  -> mapPartitions  -> P2 : 25


   6. mapPartitionsWithIndex	P: (Int, Iterator[U]) -> Iterator[V]
				Same as mapPartitions except that you get the partition Index also as
				one of the parameters.

	   rdd1.mapPartitionsWithIndex(lambda pid, pdata: [(pid, sum(pdata))] ).collect()

   7. distinct			P: None, optional: numPartition
				Returns an RDD with distinct elements of the input RDD

		rddWords.distinct().collect()

		
   8. sortBy			P: U -> V, optional: ascending, numPartitions
				The elements of the RDD are sorted based on the function output. 
		
	rddWords.sortBy(lambda x: len(x)).glom().collect()
	rddWords.sortBy(lambda x: len(x), False).glom().collect()		
	rddWords.sortBy(lambda x: len(x), True, 8).glom().collect()
	rddPairs.sortBy(lambda x: x[1], False).collect()


   Types of RDDs
   -------------
	1. Generic RDDs  -> RDD[U]
	2. Pair RDD	 -> RDD[(U, V)]	


    9. mapValues		P: U -> V 
				Applied only to pair RDDs.
				Transforms the 'value' part of the (K, V) pairs by applying the function. 

		rddPairs.mapValues(lambda a: (a, a*a) ).collect()


   10. groupBy			P: U -> V, optional: Number of Partitions
				Will group the elements of the RDD based on the function output.
				Returns a pair RDD, where the 'key' is the unique value of the function output
				and 'value' is the iterable of all the elements of the RDD that produced the
				unique value of the key.

		file = "E:\\Spark\\wordcount.txt"

		rdd = sc.textFile(file, 4) \
        		.flatMap(lambda x: x.split(" ")) \
        		.groupBy(lambda x: x) \
        		.mapValues(len) \
        		.sortBy(lambda x: x[1], False)


   11. randomSplit		P: Array of ratios
				Returns an Array of RDDs randomly split in the given ratios.

		rddArr = rdd1.randomSplit([0.4, 0.6])
		rddArr = rdd1.randomSplit([0.4, 0.6], 464)   // 464 is a seed (can be any number)

   12. repartition		P: Number of partitions
				Is used to increase or decrease the number of output partitions.
				Causes global shuffle.
				
		rdd2 = rdd1.repartition(5)

   13. coalesce			P: Number of partitions
				Is used to only decrease the number of output partitions.
				Causes partition merging

		rdd2 = rdd1.coalesce(3)	

   14. partitionBy		P: Number of partitions, optional: partition function	
				Applied ONLY to pair RDDs

				Is used to control the partition arrangement based on the keys of the
				pair RDDs.

	transactions = [
    		{'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    		{'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
    		{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    		{'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    		{'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
    		{'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
    		{'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    		{'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    		{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    		{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]

	rdd1 = sc.parallelize(transactions, 3) \
        	.map(lambda d: (d['city'], d))
               
	rdd1.glom().collect()

	def custom_partitioner(city): 
    		if (city == 'Chennai'):
        		return 0;
    		elif (city == 'Hyderabad'):
        		return 1;
    		elif (city == 'Vijayawada'):
        		return 1;
    		elif (city == 'Pune'):
        		return 2;
    		else:
        		return 3;

	rdd2 = rdd1.partitionBy(4, custom_partitioner)				


   15. union, intersection, subtract, cartesian

	Let us say rdd1 has M partitions and rdd2 has N partitions

	command				num partitions
	----------------------------------------------
	rdd1.union(rdd2)		M + N, narrow
	rdd1.intersection(rdd2)		M + N, wide
	rdd1.subtract(rdd2)		M + N, wide
	rdd1.cartesian(rdd2)		M * N, wide
   

    ..ByKey transformations 
    
	-> Wide transformations
	-> Applied only to pair RDDs
	-> Applies some functionality on all elements of each unique key.


   16. sortByKey		P: None, Optional: Sorting Order, numPartitions
				Sorts the pair RDD based on the key.

		rddWordCounts.sortByKey().collect()
		rddWordCounts.sortByKey(False).collect()
		rddWordCounts.sortByKey(True, 5).collect()

   17. groupByKey		P: None, Optional: numPartitions
				Groups the values of each unique key, so that we will have unique
				keys and grouped values.

				RDD[(U, V)].groupByKey() => RDD[(U, ResultIterable[V])]		

				Note: Avoid it if possible. 

	rddWorCount = sc.textFile(filePath) \
                .flatMap(lambda x: x.split(" ")) \
                .map(lambda x: (x, 1)) \
                .groupByKey() \
                .mapValues(sum)


   18. reduceByKey		P: (U, U) -> U, Optional: numPartitions
				Will reduce all the values of each unique-key first within partitions,
				and then across partitions
 
	rddWorCount = sc.textFile(filePath) \
                .flatMap(lambda x: x.split(" ")) \
                .map(lambda x: (x, 1)) \
                .reduceByKey(lambda x, y: x + y)


   19. aggregateByKey	        Will aggregate all the values of each unique key.

		Three Parameters:

		 Three Parameters:

		 1. zero-value  : starting value of the type of the final result.

		 2. Sequence-function : Applied on each partition
				 -> Merges all the values of each unique key ineach partition 
				    with a zero-value

		 3. Combine-function : Applied across the outputs of seq-function
				-> reduces all the values across partitions into one final value
				   for each unique key.

		student_rdd = sc.parallelize([
  			("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  			("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  			("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  			("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  			("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  			("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  			("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
		student_rdd.glom().collect()

		avg_rdd = student_rdd.map(lambda x: (x[0], x[2])) \
            		  .aggregateByKey( (0,0),
                            		lambda z, v: (z[0] + v, z[1] + 1),
                            		lambda a, b: (a[0] + b[0], a[1] + b[1])) \
            		   .mapValues(lambda x: x[0]/x[1])



   RDD Actions
   -----------

	1. collect

	2. count

	3. saveAsTextFile

	4. reduce		P: (U, U) -> U
				Will reduce the entire RDD into one final value by reduing 
				each partition and then reducing across partitions. Reduction 
				will happen by iterativly applying the reduce function.
		rdd1
	
		P0: 8, 5, 3, 5, 1, 2	-> -8  -> 24
		P1: 3, 7, 6, 3, 1, 2	-> -16
		P2: 4, 9, 6, 4, 0, 1	-> -16


		rdd1.reduce(lambda x, y: x - y)


	5. aggregate		Will reduce an entire RDD into one final of the type of zero-value
				which can be different than the RDD type.

		Three Parameters:

		1. zero-value  : starting value of the type of the final result.

		2. Sequence-function : Applied on each partition
				 -> Merges all the values of each partition with a zero-value

		3. Combine-function : Applied across the outputs of seq-function
				-> reduces all the values across partitions		

	6. take(n)	-> returns an array with first n elements.

	7. takeOrdered   -> returns an array of 'n' sorted objects

		-> rdd1.takeOrdered(10)
		-> rdd1.takeOrdered(10, lambda x: x%3)

	8. takeSample

		-> rdd1.takeSample(True, 6)
		-> rdd1.takeSample(False, 6)
		-> rdd1.takeSample(True, 6, 464)

	9. first

	10. countByValue

	11. countByKey

	12. foreach  => takes a function as input and applies the function on all objects
			does not return anythings.

		
	13. saveAsSequenceFile	


   Use Case
   --------

	From cars.tsv file, find the average weight of each make of American Origin.
	Arrange the data in the descending order of average weight.
	Save the output into only one text file.

	-> Please try to solve


   spark-submit command
   ---------------------
   
     -> Is a sinlge command that is used to submit any spark application (Scala, Java, Python or R)
	to any cluster manager (spark standalone, local, yarn, mesos, k8l)

	spark-submit --master yarn 
		--deploy-mode cluster 
		--driver-memory 2G
		--executor-memory 10G
		--executor-cores 5
		--num-executors 20 
		wordcount.py <command-line-params>

      	spark-submit --master local E:\PySpark\spark_core\examples\wordcount.py

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmgargs.py wordcount.txt wcout 2


  ===============================
     Spark SQL (pyspark.sql)
  ===============================
  
    => Spark's Structured Data Processing API

	-> Structured file formats:  Parquet (default), ORC, CSV (delimited text), JSON
	-> Hive
	-> JDBC Source : RDBMS, NoSQL

   => SparkSession
	-> Starting point of execution
	-> Represents a user session within an application
		-> A single application can have many spark sessions. 

	spark = SparkSession \
        	.builder \
        	.appName("Dataframe Operations") \
        	.config("spark.master", "local[*]") \
        	.getOrCreate()

   => DataFrame (DF)
	-> Spark SQL's data abstraction. 

	-> Is a collection of distributed immutable, lazily evaluated in-memory partitions.
	-> DataFrame is a collection of "Row" objects  (pyspark.srl.Row)
		-> Each Row is a collection of "Column" objects
		-> Each column is stored internally in SPark SQL internal data types.

	=> Spark applies a lot of optimzations (catalyst optimizer, tungston etc) under the hood while
	   creating physical execution plan. hence DataFrames are far more efficient than RDDs.

	DataFrame
		=> data: collection of "Row" objects
		=> schema: meta-data which is a StructType object

		schema
		------
		StructType(
			List(
				StructField(age,LongType,true),
				StructField(gender,StringType,true),
				StructField(name,StringType,true),
				StructField(phone,StringType,true),
				StructField(userid,LongType,true)
			)
		)


   Steps in a Spark SQL application
   --------------------------------

	1. Read/load the data from some data source (external or programmatic) into a dataframe

		df1 = spark.read.format("json").load(inputFilePath)
		df1 = spark.read.json(inputFilePath)


	2. Apply transformations on the dataframe using dataframe API or using SQL

		=> Using DataFrame API

			df2 = df1.select("userid", "name", "age", "phone", "gender") \
        			.where("age is not null") \
        			.orderBy("gender", "age") \
        			.groupBy("gender", "age").count() \
        			.limit(4)

		=> Using SQL

			df1.createOrReplaceTempView("users")

			spark.catalog.listTables()

			qry = """select age, count(1) as count
       				from users 
       				where age is not null
       				group by age
       				order by count
       				limit 5"""

			df3 = spark.sql(qry)

			df3.show()

	
	3. Write/Save the contents of the DF into a structured data file or other external database (such 
	   as Hive, MySQL)

		df2.write.format("json").save(outputDir)
		df2.write.json(outputDir)
			


   LocalTempView & GlobalTempView
   ------------------------------
	Local Temp View :
		-> Created in SparkSession's local catalog.
			df1.createOrReplaceTempView("users")
		-> Accessible only from with in that sparksession.

	Global Temp View:
		-> Created at application scope
			df1.createGlobalTempView("gusers")
		-> Accessible from any sparksession in that application.


   DataFrame Transformations
   -------------------------

   1. select

   2. where / filter 

   3. orderBy / sort

   4. groupBy  => groupBy returns a "GroupedData" object, not DataFrame
		  apply some aggregation methos on the groupBy to get a DataFrame.

   5. limit


   Working with different file formats
   -----------------------------------

	1. JSON

		read:
			df1 = spark.read.format("json").load(inputFilePath)
			df1 = spark.read.json(inputFilePath)

		write:
			df2.write.format("json").save(outputDir)
			df2.write.json(outputDir)

	2. CSV
		read: 	
			df1 = spark.read.format("csv").load(inputFilePath, header=True, inferSchema=True)
			df1 = spark.read.csv(inputFilePath, header=True, inferSchema=True)
			df1 = spark.read.csv(inputFilePath, header=True, inferSchema=True, sep="|")

		write:		
			df2.write.mode("overwrite").format("csv").save(outputDir, header=True)
			df2.write.mode("overwrite").csv(outputDir, header=True)
			df2.write.mode("overwrite").csv(outputDir, header=True, sep="|")

	3, Parquet

		read:
			df1 = spark.read.format("parquet").load(inputFilePath)
			df1 = spark.read.parquet(inputFilePath)

		write:
			df2.write.format("parquet").save(outputDir)
			df2.write.parquet(outputDir)


	4. ORC
		read:
			df1 = spark.read.format("orc").load(inputFilePath)
			df1 = spark.read.orc(inputFilePath)

		write:
			df2.write.format("orc").save(outputDir)
			df2.write.orc(outputDir)
			
 
   Creating an RDD from DataFrame
   ------------------------------

	rdd1 = df1.rdd


   Save Modes
   ----------
	-> Define the behaviour as to what should happen when you write to an existing directory.

	1. errorIfExists  (default)
	2. ignore
	3. append
	4. overwrite

		df2.write.mode("append").json(outputDir)
		df2.write.mode("overwrite").json(outputDir)
	












