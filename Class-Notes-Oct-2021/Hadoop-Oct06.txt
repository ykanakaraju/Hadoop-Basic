
  Agenda
  ------------

  Big Data - Basic
  Hadoop 
   - HDFS - File Ops & Architecture
   - YARN - Basics & Architecture
   - MapReduce - Basics & Examples & Architecture
  Hive
  Impala (introduction)
  Sqoop 
  HBase
  Kafka 
  NiFi
  Spark ( PySpark ) 

  ==============================================

   Materials
   
        => PDF Presentations
	=> Code Modules  
	=> Class Notes 

 ===============================================

   Big Data  
   --------
     -> Large & Complex Data that can not be reasonbly stored and processed using traditional
        data management systems that rely on single server architectures.

     	-> Volume 
	-> Velocity
	-> Variety  (unstructured & semi structured data)
	-> Veracity (data in doubt)
	-> Value (performing deep analytics)

   Big Data Problems
   -----------------
	-> Storing huge amount of data
	-> Processing data faster
	-> Processing complex data

   => As the problems mainly arise because of the limitations of a single machine, 
      we solve this by using the combined resource of many machines. 
	 -> We use "a computing cluster".

   What is a Computing Cluster ?
   -----------------------------
     => Is a unified entity that comprises of many nodes whose cumulative resources can be used
        to stores and process huge amount of data.

   What is Hadoop ?
   ---------------- 
    => Hadoop is an open source framework to storage and process big data using a cluster
       made of commodity hardware. 
	
    => Written in Java

	
	Hadoop  => HDFS 	: Distributed Storage Framework
		=> MapReduce 	: Distributed Processing Framework
		=> YARN		: Resource Management Tool (Cluster Manager)

	

   Hadoop Distributed File System (HDFS)
   -------------------------------------
	
     -> Distributed File System
	
     -> Stores data in blocks of 128 MB each
     -> Each block is replicated 3 times by default to provide fail-safety.
	 --> 3 copies of each block will be there on different nodes
	

   HDFS Daemons
   ------------
	Daemon -> A continuosly running background job (java program)

	1. Name Node (NN)
		-> Runs on the master machine
		-> NN receives heart-beats from DNs. 
		   -> If three successive heart-beats are missed, then the NN marks the DN as "unavailable" 

		-> NN receives block reports and maintains this info in two files:
                 
                   1. EditLog file
			-> dynamic file maintained in the RAM

		   2. FSImage file
			-> static file which is updated only during check-point process.
	

	2. Data Node (DN)
		-> Runs on all the slave machines.
		-> Data Nodes send heart-beats to NN once every three seconds.
		-> Data Nodes send "block reports"
			-> Files, Blocks, ReplicationFactor, Owner, Permissions
			-> Total storage, Reserverd Storage, Available Storage

	3. Secondary Name Node
		-> Master Process
		-> Performs check-pointing once every hour where it merges the edit-log with 
		   FSIMage files and copies and updated (checkpointed) FSImages file to the master node. 

	4. Balancer
		-> Maintain replication-factor of files.
		-> Background job that monitor under-replicated & over-replicated files.		
		

   Hadoop Distributions
   -------------------- 
   
     => Will provide a collection of pre-packaged and configured tools that are commonly required 
	for all big data projects, so that we do not have to download, install, configure and tune 
        those components separtly
	
     => Also provide some management tool to manage, monitor, repair, start  & stop services etc. to
        make managing te cluster easy. 
	
    On-Primesis Clusters
    --------------------
	1. Cloudera 
	2. HortonWorks  (merged with Cloudera)
	3. MapR

    Cloud Clusters
    ---------------
	1. AWS    - EMR (Elastic MapReduce - Managed Hadoop Cluster)
	2. Azure  - HDInsight
	3. GCP    - DataProc 
	

    HDFS File Operations   (CDH 4.7)
    --------------------------------
      
      User: cloudera, Password: cloudera

      Home Directory: 

	On Linux File System:  /home/cloudera
	On HDFS File System:   /user/cloudera

	HDFS Commands => hadoop fs -<command>
			 hdfs dfs -<command>

     
    Configuration Files:
    =====================	
	* /etc/hadoop/conf/core-site.xml
	* /etc/hadoop/conf/hdfs-site.xml
	* /etc/hadoop/conf/mapred-site.xml
	
    Hadoop HDFS commands
    ====================

    NOTE: HDFS is a "non-ediatble' file system
	  -> You can not edit the content of a HDFS file.

	hadoop fs -help
	hadoop fs -help cat

	hadoop fs -ls			=> list the files and directories
	hadoop fs -ls datasets
	hadoop fs -ls -R practice1	=> recursive listing

	hadoop fs -mkdir practice	=> create a directory

	hadoop fs -rmdir practice	=> delete an empty directory

	hadoop fs -rm practice2/students.txt
	hadoop fs -rm practice2/students_I*.txt	  => delete one or more files 
	hadoop fs -rm -R practice1   => recursive delete - deletes a non-empty directory

	hadoop fs -mv practice2/*.sh practice1  => move files/directory from one dir to another within HDFS

	hadoop fs -cp practice1/datasets  practice2 => copy files/directory from one dir to another within HDFS
	
	hadoop fs -cat wordcount_input.txt  => display the content on stdout

	hadoop fs -chmod 755 practice1		=> Change permission of files/directories
	hadoop fs -chmod -R 777 practice1

 
   Copy data between Linux & HDFS file systems
   -------------------------------------------
    
	Linux to HDFS:
		hadoop fs -put <linux-file-path> <hdfs-directory>
		hadoop fs -copyFromLocal <linux-file-path> <hdfs-directory>
		hadoop fs -moveFromLocal <linux-file-path> <hdfs-directory>		

		hadoop fs -put users.csv practice1
     		hadoop fs -copyFromLocal datasets/NYSE_daily.txt practice1
		hadoop fs -copyFromLocal datasets/NYSE_daily.txt   // copies to home directory	
		hadoop fs -moveFromLocal dir1/student*.txt practice2   // files deleted from source dir	

		// Copy with a set rep-factor
		hadoop fs -D dfs.replication=5 -copyFromLocal datasets/NYSE_daily.txt


	HDFS to Linux:	
		hadoop fs -get <hdfs-file-path> <linux-directory>
		hadoop fs -copyToLocal <hdfs-file-path> <linux-directory>

		hadoop fs -get NYSE_daily.txt dir1
		hadoop fs -copyToLocal datasets dir1



   More Commands
   --------------

 * du - disk usage  
      
        ex:  hadoop fs -du /user/cloudera


 * du -s : Like -du, but prints a summary of disk usage of all files/directories in the path

        ex: hadoop fs -du -s /user/cloudera


 * df : Shows the capacity, free and used space of the filesystem
         -h  Formats the sizes of files in a human-readable fashion.

        ex: hadoop fs -df -h /user/cloudera/
            hadoop fs -df /user/cloudera/


 * getmerge <src> <localDest>    :  Retrieves all files that match the path src in HDFS, 
                                    and copies them to a single, merged file in the local 
                                    file system identified by localDest. 

        ex: hadoop fs -getmerge test/*.txt test1/merged.txt


 * Get the replication factor of a file:

        ex: hadoop fs -stat %r test/empinfo.txt


 * Set the replication factor of a file while copying or moving from linux

        ex: hadoop fs -D dfs.replication=5 -copyFromLocal <sourc e> <destination>
  
            hadoop fs -D dfs.replication=5 -copyFromLocal datasets/empinfo.txt test2
            hadoop fs -stat %r test2/empinfo.txt


 * Set the replication factor for an existing file:  -setrep [-R] [-w] <rep-factor> <path>
       
        ex: hadoop fs -setrep 2 test2/empinfo.txt   => sets the rep-factor of the file to 2
	    hadoop fs -stat %r test2/empinfo.txt    => test the rep-factor of the file


 * appendToFile

	Usage: hdfs dfs -appendToFile <localsrc> ... <dst>

	Append single src, or multiple srcs from local file system to the destination file system. 
	Also reads input from stdin and appends to destination file system.

	hdfs dfs -appendToFile localfile /user/hadoop/hadoopfile
	hdfs dfs -appendToFile localfile1 localfile2 /user/hadoop/hadoopfile
	hdfs dfs -appendToFile localfile hdfs://nn.example.com/hadoop/hadoopfile
	hdfs dfs -appendToFile - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin.

 * chmod, chgrp, chown  => Change the permissions of files.	
     
   ==============================================
      YARN  (Yet Another Resource Negotiator)
   ==============================================

     => Resource Management tool for Hadoop (from Hadoop 2.0 onwards)
       
     Basic Components of YARN :

	1. Resource Manager : One per cluster, master daemon
	2. Node Manager: One per node, slave daemon
	3. Application Master: One per application/job
			      (starts when job starts, and closed when the job is completed)
	4. Containers: Application processes where App Master sends the tasks to be executed.

	Typical application execution with YARN follows this flow:
	   => Client program submits the MR application to the RM, along with information to launch 
	      the application-specific Application Master.
	   => RM negotiates a container for the AM and launches the AM.
	   => AM boots and registers with the RM, allowing the original calling client to interface 
	      directly with the AM.
	   => AM negotiates resources (resource containers) for client application.
	   => AM gives the container launch specification to the NM, which launches a container for 
	      the application.
	   => During execution, client polls AM for application status and progress.
	   => Upon completion, AM deregisters with the RM and shuts down.

  
   ==============================================
      MapReduce
   ==============================================
    
     => Hadoop's default distributed processing framework. 
	
     Can be seen from two perspectives:


	=> Map Reduce execution framework lifecycle
               => How a Mapreduce Program is executed by the MR framework (the life cycle process)


	=> Map Reduce Programming Paradigm
		=> How should a developer write MR programs	

	   MapReduce Application Classes (that a developer has to write)

		1. Driver Class (required)
		     => Where you define all the application configurations.

		2. Mapper Class (required)
		     => Extends "Mapper" base class and implements some life cycle methods:
		     => Implements "map" method, where you write you application logic.		     

		3. Reducer Class (required only if aggregation is needed)
		      => Extends "Reducer" base class and implements some life cycle methods.
		     => Implements "reduce" method, where you write you application logic.

		4. ???  (Optional)
		5. ???  (Optional)

	



    MapReduce Framework
    -------------------

    1. Creates InputSplits
	-> A logical enity derived from blocks as whole number of records
	-> Input-Splits can span multiple blocks.
	-> The number of input splits created is equal to number of mappers lauched. 



IS-1

[hadoop hive mapreduce hdfs hadoop mapreduce
mapper reducer sqoop flume hadoop mapreduce
mapreduce hive flume kafka hadoop hive
hadoop hadoop mapreduce mapreduce hive hive]

IS-2

[sqoop flume kafka mapper reducer partitioner combiner
driver hadoop mapreduce driver mapper combiner
reducer partitioner hadoop sqoop mapreduce
hadoop hive mapreduce hdfs hadoop mapreduce
mapper reducer sqoop flume hadoop mapreduce
mapreduce hive flume kafka hadoop hive]

IS-3

[hadoop hadoop mapreduce mapreduce hive hive
sqoop flume kafka mapper reducer partitioner combiner
driver hadoop mapreduce driver mapper combiner
reducer partitioner hadoop sqoop mapreduce
hadoop hive mapreduce hdfs hadoop mapreduce]


IS-4

[mapper reducer sqoop flume hadoop mapreduce
mapreduce hive flume kafka hadoop hive
hadoop hadoop mapreduce mapreduce hive hive
sqoop flume kafka mapper reducer partitioner combiner
driver hadoop mapreduce driver mapper combiner
reducer partitioner hadoop sqoop mapreduce]






		



 


	
	
	








		































