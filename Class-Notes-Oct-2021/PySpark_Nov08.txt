
  Agenda - PySpark
  -----------------

	Spark - Basics & Architecture
	Spark Core API - RDDs
		-> RDD Transformations & Actions
		-> Shared Variables
	Spark SQL - DataFrames
	Spark MLlib & Machine Learning 
	Introduction to Spart Streaming


   Materials
   ---------
	-> PDF presentations
	-> Code & Data
	-> Class Notes

	URL: https://github.com/ykanakaraju/pyspark


   Cluster
   --------
	-> A group of nodes whose combined resources can be used to distribute our
	   storage and processing across many nodes.


   What is Spark ?
   --------------
	-> Spark framework is written in SCALA language

    	-> Spark is a unified in-memory distributed computing framework.


	Spark in-memory computation
        ---------------------------
	-> Spark can store intermediate results in memory. 
	    -> If there is not enough memory spark can store the results on the disk as well.


	Spark Unified Framework
	-----------------------
	-> Spark provides a set of consistent APIs running on the same execution engine for
	   performing different types of anlytical workloads.	

	
		Batch Processing of Unstructured Data	-> Spark Core API (RDDs)
		Batch Processing of Structured Data	-> Spark SQL
		Streaming Analytics (real time)		-> Spark Streaming
		Predictive Analytics (Machine Learning) -> Spark MLlib
		Graph Parallel Computations		-> Spark GraphX


   Spark Architecture
   ------------------

     1. Cluster Manager (CM)
	-> Spark Jobs are submitted to a cluster manager
	-> CM schedules the job, launches the job
	-> CM allocates some executors to the application

	=> Supported:
		Spark Standalone, YARN< Mesos, Kubernetes.

     2. Driver
	-> When an application is launched, a drive process is created
	-> The first object that gets created when an application is launched is "SparkContext"
	-> Is the master process
	-> Will store all the data and meta data related to the user program
	-> Will analyze the user code and sends tasks on the executors
    
     3. Executors
	-> Executors receives tasks from the driver
	-> All tasks does the same process on different partitions of data
	-> After the task completion, the status is reported to the driver.
	
     4. SparkContext
	-> Is an application context
	-> Is a link between the driver process and several tasks that are running on the cluster.


   Getting started with Spark
   --------------------------

      1. Downloading & setting up Spark 
	
	    URL: https://spark.apache.org/downloads.html
            -> Download the tar (.tgz) file and extract it to a suitable location.
	    -> Setup SPARK_HOME & HADOOP_HOME environment variable poiting to Spark installation dir.
	    -> PYTHONPATH: %SPARK_HOME%/python;%SPARK_HOME%/python/lib/py4j-0.10.9-src.zip;%PYTHONPATH%

      2. Installing and setup PySpark with Spyder
	   -> Install "Anaconda Navigator"
	   -> Follow the step mentioned in the document shared on the GitHub

      3. Signup to Databricks Community Account
           URL: https://databricks.com/try-databricks

	   -> Signup to the free account (15 GB space)
	   -> Login to Databricks account
	   -> Read the "Quick Start Tutorial"

To Download a file from databricks:
-----------------------------------

/FileStore/<FILEPATH>
https://community.cloud.databricks.com/files/<FILEPATH>?o=4949609693130439#tables/new/dbfs

Example:
/FileStore/tables/wordcount-5.txt
https://community.cloud.databricks.com/files/tables/wordcount-5.txt?o=4949609693130439#tables/new/dbfs


/FileStore/tables/WordcountExample1Output/part-00000

/FileStore/tables/demo_wordcount_out/part-00001

https://community.cloud.databricks.com/files/tables/demo_wordcount_out/part-00001?o=4949609693130439#tables/new/dbfs




   Resilient Distributed Datasets (RDD)
   ------------------------------------

	-> RDD is a fundamental in-memory data abstraction of Spark Core API

	-> RDD is a collection of in-memory distributed partitions.
		-> Partition is a collection of objects. 

	-> RDDs have two aspects:

		Meta Data : Lineage DAG of RDD (Logical Plan)
			    -> describes how the RDD has to be created

		Data : The actual in-memory partitions

	-> RDDs are immutable
		-> RDD partition's data can not be changed.

	-> RDDs are lazily evaluated
		-> Transformations does not cause execution
		-> Only action commands cause execution.

	-> RDDs are resilient
		-> RDD are resilient to missing in-memory partitions. Any missing partition
		   does not cause failure of the job. RDDs can recreate the missing partitions
		   on-the-fly.  

	
   How to create RDDs ?
   --------------------	
	Three ways:

	1. Create an RDD from some external text file

		rdd1 = sc.textFile( <filePath> )

		-> The default number of partitions is decided by "sc.defaultMinPartitions", whose
		   value is 2 if you have atleast two cores.

		rddFile = sc.textFile( file, 4 )   // 4 partitions are created.

	2.  Create an RDD from programmatic data. 
	
		rdd1 = sc.parallelize( range(1, 101) )

		-> The default number of partitions is decided by "sc.defaultParallelism", whose
		   value is equal to the number of cores allocated to your application.


		rdd1 = sc.parallelize( range(1, 101), 3 )

	3. By applying transformation on existing RDDs we can create new RDD.

		rdd2 = rdd1.map( ... )


   What can we do with an RDD ?
   -----------------------------
	Only Two things:

	1. Transformations
	    -> Transformations does not execution 
	    -> Transformations cause only the lineage DAG to be created at the driver side.

	2. Actions
	    -> Triggers the actual execution of the RDDs and some output is generated
	    -> Cause the driver the convert the ogical plan to a physical execution ad several
	       tasks are sent to the executor.

   RDD Lineage DAG
   ---------------

	-> Lineage DAG of an RDD tracks all the dependencies as to how to create RDD all way from
	   the very first RDD. This is a logical plan maintained by the driver process.

	rddFile = sc.textFile( file )
		Lineage:   rddFile -> sc.textFile

	rdd2 = rddFile.map(lambda x: x.upper())
		Lineage:  rdd2 -> rddFile.map -> sc.textFile

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))
		Lineage: rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile

	rdd4 = rdd3.filter(lambda x: len(x) > 3)
		Lineage: rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile

        rdd4.collect()  --> returns an Array()

 	sc.textFile (rddFile) -> map (rdd2) -> flatMap (rdd3) -> filter (rdd4) ==> driver


   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD

   RDD Persistence
   ---------------
	rdd1 = sc.textFile( ... )
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd3.t4(..)
	rdd5 = rdd3.t5(..)
	rdd5.persist( StorageLevel.MEMORY_AND_DISK )    --> instruction to spark to not delete those partitions
	rdd6 = rdd5.t6(..)

	rdd6.collect()	
	rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	sc.textFile (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6) -> collect

	rdd5.collect()	
	
       Types of persistence:
	 
             -> Persist in-memory as deserialized objects
	     -> Persist in-memory as serialized data (bytearrays)
	     -> Persist on disk

     Storage Levels
     ---------------
	1. MEMORY_ONLY	        -> Only in the RAM
	2. MEMORY_AND_DISK	-> Tries to store in memory if available, else on disk
	3. DISK_ONLY
	4. MEMORY_ONLY_SER	-> (default) Only in the RAM in serliazed format	
	5. MEMORY_AND_DISK_SER	
	6. MEMORY_ONLY_2
	7. MEMORY_AND_DISK_2	
 
    Commands
    --------
	rdd1.persist()
	rdd1.cache()    -> is similar to the above
	rdd1.persist( StotageLavel.MEMORY_AND_DISK )
	rdd1.unpersist()	

    Executor Memory Structure
    --------------------------

	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 	


	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   RDD Transformations & Actions
   -----------------------------

    1. map		P: U -> V 
			Element to element transformation
			input RDD: N objects, output RDD: N objects   
  
    2. filter		P: U -> Boolean
			Filters the elements that return True for the function.
  			input RDD: N objects, output RDD: <= N objects 

    3. glom		P: None
			Will return one array per partition of the input RDD
	

	rdd1			rdd2 = rdd1.glom()

	P0:  2,3,1,3,2,4,3 -> glom -> P0: [2,3,1,3,2,4,3]
	P1:  4,2,1,3,2,6,5 -> glom -> P1: [4,2,1,3,2,6,5]
	P2:  5,6,7,8,1,2,3 -> glom -> P2: [5,6,7,8,1,2,3]

	rdd1.count() = 21		rdd2.count() = 3

	rdd1.glom().collect()
	rdd1.glom().map(lambda x: sum(x)).collect()


   4. flatMap			P: U -> Iterable[V]   ( U -> Collection[V] )
				flatMap flattens all the iterable elements returned by the function.
				input RDD: N objects, output RDD: >= N objects 


	rddWords = rddFile.flatMap(lambda x: x.split(" "))
	rddWords.flatMap(lambda x: x.upper()).collect()
	rdd1.flatMap(lambda x: range(1, x+1)).collect()


   5. mapPartitions		P: Iterator[U] -> Iterator[V]	
				Applies a function on the entire partition. All the elements of each 
				partition is passed as function input.

	rdd1.mapPartitions(lambda a: [sum(a)] ).collect()


   6. mapPartitionsWithIndex	P: Int, Iterator[U] -> Iterator[V]
				The function take two params - partition-index & partition-data.	

	rdd1.mapPartitionsWithIndex(lambda id, data : [(id, len(list(data)))] ).collect()


   7. distinct			P: None, Optional: number of output partitions
				Returns the distinct elements of the RDD

   8. sortBy			P: U -> V, optional: ascending (True/False), numPartitions
				Elements of the RDD are sorted based the function output they produce.
	
	rdd1.sortBy(lambda x: x%5).glom().collect()
	rdd1.sortBy(lambda x: x%5, False).glom().collect()
	rdd1.sortBy(lambda x: x%5, True, 5).glom().collect()
	rdd1.sortBy(lambda x: x%5, numPartitions=5).glom().collect()  
	// specifying partition count by named parameter


   Types of RDDs
   -------------
	-> Generic RDDs : RDD[U]
	-> Pair RDDs:     RDD[(U, V)]


   9. mapValues			P: U -> V
				Applied ONLY to pair RDDs
				Applies the function on the values part of the pair RDDs.

	rddPairs2.mapValues(lambda x: x.upper()).collect()


   10. groupBy			P: U -> V, optional: numPartitions

				Returns a pair RDD, where the 'key' is the unique values of the function output
				and the 'value' is an iterable object (ResultIterable) contains all the elements 
				of the RDD that produced the function output.

   	rdd1.groupBy(lambda x: x%3 ).mapValues(list).collect()
	rddWords.groupBy(lambda x: x).mapValues(len).collect()
	rdd1.groupBy(lambda x: x%3, 2).mapValues(list).collect()

	rdd = sc.textFile(file, 4) \
        .flatMap(lambda x: x.split(" ")) \
        .groupBy(lambda x: x) \
        .mapValues(len) \
        .sortBy(lambda x: x[1], False, 1)


   11. randomSplit		P: Array of ratios
				Returns an Array of RDDs split approximatly in the specified ratios.

	rddArr = rdd1.randomSplit([1,2,3])
	rddArr = rdd1.randomSplit([1,2,3], 3538)   // here 3538 is a seed
		-> A seed fixes the randomness and produce the same output every time.

   12. repartition		P: numPartitions
				Creates an RDD with specified number of partitions
				Is used to increase or decrease the number of output partitions.
				Causes global shuffle

   13. coalesce			P: numPartitions
				Creates an RDD with specified number of partitions
				Is used only to decrease the number of output partitions.
				Cause partition merging.

   14. partitionBy		P: numPartitions, Optional: partitioning function
				Applied only on pair RDDs
				Used to control which elements goto which partition based on a partition function
				applied to the keys of the (K,V) pairs.
   			
	rddPairs.partitionBy(4, lambda x: len(x)).glom().collect()


   15. union, intersection, subtract, cartesian   	P: rdd
	
	Let us say rdd1 has M partitions and rdd2 has N partitions:

	Command					Number of output partitions
        ------------------------------------------------------------------
	rdd1.union(rdd2)			M + N, narrow
	rdd1.intersection(rdd2)			M + N, wide
	rdd1.subtract(rdd2)			M + N, wide
	rdd1.cartesian(rdd2)			M * N, wide
				
	
   ...ByKey Transformations    
   	=> Applied only to Pair RDD
	=> Are wide transformations
	

   16. sortByKey		P: None, Optional: Sorting Order, numPartitions
				Sorts the RDD based on the keys of the pair RDD.

	rddPairs.sortByKey().collect()
	rddPairs.sortByKey(False).collect()
	rddPairs.sortByKey(False, 5).collect()


   17. groupByKey		P: None, Optional: numPartitions
				Returns a pair RDD where the key is the unique key of input RDD and values are
				aggregated values for that key. Output will have unique keys and aggregated values.

				RDD[(U, V)].groupByKey() => RDD[(U, ResultIterable[V])]

				Note: Avoid groupByKey if possible.

		rdd = sc.textFile(file, 4) \
        		.flatMap(lambda x: x.split(" ")) \
        		.map(lambda a: (a,1)) \
        		.groupByKey() \
        		.mapValues(sum)

   18. reduceByKey		P: (U, U) -> U,    optional: numPartitions
				Wil reduce all the values of each unique key with in each partition
				in the first stage (narrow), and then reduces the outputs across partitions	
				to one final value. 

		rdd = sc.textFile(file, 4) \
        		.flatMap(lambda x: x.split(" ")) \
        		.map(lambda x: (x, 1)) \
        		.reduceByKey(lambda x, y: x + y, 1)

   19. aggregateByKey		Will reduce all thr values of each unique key into a value of the type
				of zero-value. (zero-value can be of different type than RDD elements).
	
		Three parameters: 
	
		1. zero-value : initial value based on the final value you want to produce
		2. Sequence Function: merges all the values of each partition with the zero-value
		3. Combine function: reduces all the output per partition produced by seq.fn into one final value.

	student_rdd = sc.parallelize([
  		("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  		("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  		("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  		("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  		("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  		("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  		("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
	

	avg_rdd = student_rdd.map(lambda x: (x[0], x[2])) \
            	.aggregateByKey( (0,0),
                            lambda z, v: (z[0] + v, z[1] + 1), 
                            lambda a, b: (a[0] + b[0], a[1] + b[1])
                            ) \
            	.mapValues(lambda x: x[0]/x[1])


   20. join transformations	=> join, leftOuterJoin, rightOuterJoin, fullOuterJoin

		RDD[(U, V)].join( RDD[(U, W)] ) => RDD[(U, (V, W))]

		names1 = sc.parallelize(["vijay", "aditya", "raju", "amrita"]).map(lambda a: (a, 1))
		names2 = sc.parallelize(["amrita", "raju", "pavan", "pranav"]).map(lambda a: (a, 2))

		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)

   21. cogroup   		=> Is used when you wantto join RDDs with duplicate keys.
				=> groupByKey on each RDD -> fullOuterJoin on those grouped RDDs


	   rdd1 => [('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
			-> (key1, [10, 7]), (key2, [12, 6]), (key3, [6])
	
	   rdd2 => [('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
			-> (key1, [5, 17]), (key2, [4, 7]), (key4, [17])

	   rdd3 = rdd1.cogroup(rdd2)
		-> (key1, ([10, 7], [5, 17])), (key2, ([12, 6], [4, 7])) (key3, ([6], [])), (key4, ([], [17]))



   RDD Actions
   ------------

	1. collect

	2. count

	3. saveAsTextFile

	4. reduce		P: (U, U) -> U
				Will reduce the entire RDD into one final value by iterativly applying the 
				reduce function in each partition first, and then across partitions.

		P0: 6, 3, 1, 2, 3, 4    => -7   => 42
		P1: 6, 8, 9, 0, 6, 3    => -20
		P2: 4, 5, 7, 4, 8, 9, 0 => -29

		rdd1.reduce(lambda x, y: x - y)

       5. aggregate		Will reduce entire RDD into one final value of a type different 
				from the elements of the RDD.	
		
		Three parameters: 
	
		1. zero-value : initial value based on the final value you want to produce
		2. Sequence Function: merges all the values of each partition with the zero-value
		3. Combine function: reduces all the output per partition produced by seq.fn into one final value.

		rdd1.aggregate( (0,0), 
				lambda z, v: (z[0] + v, z[1] + 1), 
				lambda a, b: (a[0] + b[0], a[1] + b[1]) )

	6. take(n)   => Returns an array with first n elements of the RDD


	7. takeOrdered(n, [optioning fn]) -> returns first n elements of the sorted RDD.

		rdd1.takeOrdered(10)
		rdd1.takeOrdered(10, lambda x: x%2)


	8. takeSample(withReplcement, n, [seed])

		rdd1.takeSample(True, 6, 678)   - withReplacement: True, 678 is a seed
		rdd1.takeSample(False, 6, 678)  - withReplacement: False, 678 is a seed

	9. countByValue

	10. countByKey

	11. first

	12. saveAsSequenceFile

		rddWordCounts.coalesce(1).saveAsSequenceFile("E:\\PySpark\\output\\seq")

	13. foreach

		rddWordCounts.foreach(lambda a: print("key = " + a[0] + ", value = " + str(a[1])) )
		

   Use-Case
   --------
   From cars.tsv file find average weight of each of the makes of American cars.
   Arransge the data in the descending order of averahe weight
   Save the output as one text file. 

      --> Please try yourself



   Closure
   -------
	-> In Spark, a closure constitutes all the variables and methods which must be visible 
           for the executor to perform its computations on the RDD. 

	-> This closure is serialized and sent to each executor.

   
   Shared Variables
   ----------------

    => Because of closure, we CAN NOT use local variables to implement global counter. To solve this
       Spark provide a special shared variable called "accumulator"

       
	counter = 0

	def isPrime(n) :
	   returns 1 if n is Prime
	   else returns 0
	
	def f1(n) :
	   if (isPrime(n) == 1) counter = counter + 1
	   retunr n*2
	
	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()
 

	print("counter = " + counter)     // this will not fetch the total count

  
   1. Accumulator

	-> Accumulator is maintained by the driver and is not part of the function closure.
	-> Accumulators can be used to implement global counters.
	
	counter = sc.accumulator(0)

	def isPrime(n) :
	   returns 1 if n is Prime
	   else returns 0
	
	def f1(n) :
	   if (isPrime(n) == 1) counter.add(1)
	   retunr n*2
	
	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect() 

	print("counter = " + counter)     // this will not fetch the total count



   2. Broadcast

	-> A single copy of a "broadcast variable" is sent to every executor node
	-> Saved in the storage memory  (along-side the RDD paertitions)
	-> All the tasks that are running in that executor can lookup from that one copy.


	d1 = sc.broadcast({1: 'A', 2: 'B', 3: 'C', ....})   

	def f1 (n) :
	    global d1
            return d1.value[n]

	rdd1 = sc.parallelize( [2,1,3,2,5,6,8,9,8], 3 )

	rdd2 = rdd1.map( f1 )


  Spark-submit command
  --------------------









	
