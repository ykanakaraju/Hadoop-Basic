
  Agenda
  ------------

  Big Data - Basic
  Hadoop 
   - HDFS - File Ops & Architecture
   - YARN - Basics & Architecture
   - MapReduce - Basics & Examples & Architecture
  Hive
  Impala (introduction)
  Sqoop 
  HBase
  Kafka 
  NiFi
  Spark ( PySpark ) 

  ==============================================

   Materials
   
        => PDF Presentations
	=> Code Modules  
	=> Class Notes 

 ===============================================

   Big Data  
   --------
     -> Large & Complex Data that can not be reasonbly stored and processed using traditional
        data management systems that rely on single server architectures.

     	-> Volume 
	-> Velocity
	-> Variety  (unstructured & semi structured data)
	-> Veracity (data in doubt)
	-> Value (performing deep analytics)

   Big Data Problems
   -----------------
	-> Storing huge amount of data
	-> Processing data faster
	-> Processing complex data

   => As the problems mainly arise because of the limitations of a single machine, 
      we solve this by using the combined resource of many machines. 
	 -> We use "a computing cluster".

   What is a Computing Cluster ?
   -----------------------------
     => Is a unified entity that comprises of many nodes whose cumulative resources can be used
        to stores and process huge amount of data.

   What is Hadoop ?
   ---------------- 
    => Hadoop is an open source framework to storage and process big data using a cluster
       made of commodity hardware. 
	
    => Written in Java

	
	Hadoop  => HDFS 	: Distributed Storage Framework
		=> MapReduce 	: Distributed Processing Framework
		=> YARN		: Resource Management Tool (Cluster Manager)

	

   Hadoop Distributed File System (HDFS)
   -------------------------------------
	
     -> Distributed File System
	
     -> Stores data in blocks of 128 MB each
     -> Each block is replicated 3 times by default to provide fail-safety.
	 --> 3 copies of each block will be there on different nodes
	

   HDFS Daemons
   ------------
	Daemon -> A continuosly running background job (java program)

	1. Name Node (NN)
		-> Runs on the master machine
		-> NN receives heart-beats from DNs. 
		   -> If three successive heart-beats are missed, then the NN marks the DN as "unavailable" 

		-> NN receives block reports and maintains this info in two files:
                 
                   1. EditLog file
			-> dynamic file maintained in the RAM

		   2. FSImage file
			-> static file which is updated only during check-point process.
	

	2. Data Node (DN)
		-> Runs on all the slave machines.
		-> Data Nodes send heart-beats to NN once every three seconds.
		-> Data Nodes send "block reports"
			-> Files, Blocks, ReplicationFactor, Owner, Permissions
			-> Total storage, Reserverd Storage, Available Storage

	3. Secondary Name Node
		-> Master Process
		-> Performs check-pointing once every hour where it merges the edit-log with 
		   FSIMage files and copies and updated (checkpointed) FSImages file to the master node. 

	4. Balancer
		-> Maintain replication-factor of files.
		-> Background job that monitor under-replicated & over-replicated files.		
		

   Hadoop Distributions
   -------------------- 
   
     => Will provide a collection of pre-packaged and configured tools that are commonly required 
	for all big data projects, so that we do not have to download, install, configure and tune 
        those components separtly
	
     => Also provide some management tool to manage, monitor, repair, start  & stop services etc. to
        make managing te cluster easy. 
	
    On-Primesis Clusters
    --------------------
	1. Cloudera 
	2. HortonWorks  (merged with Cloudera)
	3. MapR

    Cloud Clusters
    ---------------
	1. AWS    - EMR (Elastic MapReduce - Managed Hadoop Cluster)
	2. Azure  - HDInsight
	3. GCP    - DataProc 
	

    HDFS File Operations   (CDH 4.7)
    --------------------------------
      
      User: cloudera, Password: cloudera

      Home Directory: 

	On Linux File System:  /home/cloudera
	On HDFS File System:   /user/cloudera

	HDFS Commands => hadoop fs -<command>
			 hdfs dfs -<command>

     
    Configuration Files:
    =====================	
	* /etc/hadoop/conf/core-site.xml
	* /etc/hadoop/conf/hdfs-site.xml
	* /etc/hadoop/conf/mapred-site.xml
	
    Hadoop HDFS commands
    ====================

    NOTE: HDFS is a "non-ediatble' file system
	  -> You can not edit the content of a HDFS file.

	hadoop fs -help
	hadoop fs -help cat

	hadoop fs -ls			=> list the files and directories
	hadoop fs -ls datasets
	hadoop fs -ls -R practice1	=> recursive listing

	hadoop fs -mkdir practice	=> create a directory

	hadoop fs -rmdir practice	=> delete an empty directory

	hadoop fs -rm practice2/students.txt
	hadoop fs -rm practice2/students_I*.txt	  => delete one or more files 
	hadoop fs -rm -R practice1   => recursive delete - deletes a non-empty directory

	hadoop fs -mv practice2/*.sh practice1  => move files/directory from one dir to another within HDFS

	hadoop fs -cp practice1/datasets  practice2 => copy files/directory from one dir to another within HDFS
	
	hadoop fs -cat wordcount_input.txt  => display the content on stdout

	hadoop fs -chmod 755 practice1		=> Change permission of files/directories
	hadoop fs -chmod -R 777 practice1

 
   Copy data between Linux & HDFS file systems
   -------------------------------------------
    
	Linux to HDFS:
		hadoop fs -put <linux-file-path> <hdfs-directory>
		hadoop fs -copyFromLocal <linux-file-path> <hdfs-directory>
		hadoop fs -moveFromLocal <linux-file-path> <hdfs-directory>		

		hadoop fs -put users.csv practice1
     		hadoop fs -copyFromLocal datasets/NYSE_daily.txt practice1
		hadoop fs -copyFromLocal datasets/NYSE_daily.txt   // copies to home directory	
		hadoop fs -moveFromLocal dir1/student*.txt practice2   // files deleted from source dir	

		// Copy with a set rep-factor
		hadoop fs -D dfs.replication=5 -copyFromLocal datasets/NYSE_daily.txt


	HDFS to Linux:	
		hadoop fs -get <hdfs-file-path> <linux-directory>
		hadoop fs -copyToLocal <hdfs-file-path> <linux-directory>

		hadoop fs -get NYSE_daily.txt dir1
		hadoop fs -copyToLocal datasets dir1



   More Commands
   --------------

 * du - disk usage  
      
        ex:  hadoop fs -du /user/cloudera


 * du -s : Like -du, but prints a summary of disk usage of all files/directories in the path

        ex: hadoop fs -du -s /user/cloudera


 * df : Shows the capacity, free and used space of the filesystem
         -h  Formats the sizes of files in a human-readable fashion.

        ex: hadoop fs -df -h /user/cloudera/
            hadoop fs -df /user/cloudera/


 * getmerge <src> <localDest>    :  Retrieves all files that match the path src in HDFS, 
                                    and copies them to a single, merged file in the local 
                                    file system identified by localDest. 

        ex: hadoop fs -getmerge test/*.txt test1/merged.txt
	    hadoop fs -getmerge wcout/part-r-* test1/merged.txt


 * Get the replication factor of a file:

        ex: hadoop fs -stat %r test/empinfo.txt


 * Set the replication factor of a file while copying or moving from linux

        ex: hadoop fs -D dfs.replication=5 -copyFromLocal <sourc e> <destination>
  
            hadoop fs -D dfs.replication=5 -copyFromLocal datasets/empinfo.txt test2
            hadoop fs -stat %r test2/empinfo.txt


 * Set the replication factor for an existing file:  -setrep [-R] [-w] <rep-factor> <path>
       
        ex: hadoop fs -setrep 2 test2/empinfo.txt   => sets the rep-factor of the file to 2
	    hadoop fs -stat %r test2/empinfo.txt    => test the rep-factor of the file


 * appendToFile

	Usage: hdfs dfs -appendToFile <localsrc> ... <dst>

	Append single src, or multiple srcs from local file system to the destination file system. 
	Also reads input from stdin and appends to destination file system.

	hdfs dfs -appendToFile localfile /user/hadoop/hadoopfile
	hdfs dfs -appendToFile localfile1 localfile2 /user/hadoop/hadoopfile
	hdfs dfs -appendToFile localfile hdfs://nn.example.com/hadoop/hadoopfile
	hdfs dfs -appendToFile - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin.

 * chmod, chgrp, chown  => Change the permissions of files.	
     
   ==============================================
      YARN  (Yet Another Resource Negotiator)
   ==============================================

     => Resource Management tool for Hadoop (from Hadoop 2.0 onwards)
       
     Basic Components of YARN :

	1. Resource Manager : One per cluster, master daemon
	2. Node Manager: One per node, slave daemon
	3. Application Master: One per application/job
			      (starts when job starts, and closed when the job is completed)
	4. Containers: Application processes where App Master sends the tasks to be executed.

	Typical application execution with YARN follows this flow:
	   => Client program submits the MR application to the RM, along with information to launch 
	      the application-specific Application Master.
	   => RM negotiates a container for the AM and launches the AM.
	   => AM boots and registers with the RM, allowing the original calling client to interface 
	      directly with the AM.
	   => AM negotiates resources (resource containers) for client application.
	   => AM gives the container launch specification to the NM, which launches a container for 
	      the application.
	   => During execution, client polls AM for application status and progress.
	   => Upon completion, AM deregisters with the RM and shuts down.

  
   ==============================================
      MapReduce
   ==============================================
    
     => Hadoop's default distributed processing framework. 
	
     Can be seen from two perspectives:

	=> Map Reduce execution framework lifecycle
               => How a Mapreduce Program is executed by the MR framework (the life cycle process)

		1. Mapper Phase

		2. Reducer Phase

		    2.1 Shuffle & Sort

		    2.2  Reduce


	=> Map Reduce Programming Paradigm
		=> How should a developer write MR programs	

	   MapReduce Application Classes (that a developer has to write)

		1. Driver Class (required)
		     => Where you define all the application configurations.

		2. Mapper Class (required)
		     => Extends "Mapper" base class and implements some life cycle methods:
		     => Implements "map" method, where you write you application logic.		     

		3. Reducer Class (required only if aggregation is needed)
		     => Extends "Reducer" base class and implements some life cycle methods.
		     => Implements "reduce" method, where you write you application logic.

		4. Combiner (Optional)
		     => Is like a mini-reducer which aggregates the output of each mapper.
		     => Acts on each mapper's output to reduce the amount of data to be shuffled.
		     => By reducing the amount of data shuffled, we can increase the performance.

		5. Partitioner (Optional)
		      => Acts on the output of S&S phase and applies custom logic to decide which
			 data goes to which partition (and hence to which reducer).
		      => Is applicably only if you use 2 or more reducers.


    MapReduce Framework
    -------------------

    1. Creates InputSplits
	-> A logical enity derived from blocks as whole number of records
	-> Input-Splits can span multiple blocks.
	-> The number of input splits created is equal to number of mappers lauched. 

	InputFormat => InputSplit => RecordReader

	1. InputFormat creates InputSplits
	2. RecordReader class reads from InputSplits and creates (K, V) pairs


	IS-1
     [InputSplit]			[RecordReader]
	[hadoop hive mapreduce hdfs  => (0, hadoop hive mapreduce hdfs)   ==>  map(...)  ==> (hadoop, 1) (hive, 1) (mapreduce, 1) (hdfs, 1)         
	mapper reducer sqoop flume   => (32, mapper reducer sqoop flume ) ==>  map(...)  ==> (mapper, 1) (reducer,1) (sqoop, 1) (flume, 1)
	mapreduce hive flume kafka   => (75, mapreduce hive flume kafka )	
	hadoop hadoop mapreduce	     => (120, hadoop hadoop mapreduce)
	sqoop flume kafka mapper]    => (154, sqoop flume kafka mapper )


     Sort & Shuffle
     --------------
	(flume, [1,1,1,1,1,1,1,1,...])       --> reduce(...)  --> (flume, 74)
	(hadoop, [1,1,1,1,1,1,1,1,...])	     --> reduce(...)  --> (hadoop, 90)
	(hive, [1,1,1,1,1,1,1,1,...])
	(scala, [1,1,1,1,1,1,1,1,...])


   Hadoop Data Types
   ------------------
   
   => All hadoop framework classes use only Hadoop Custom Data Types
   => Hadoop custom data type are optimized for Serialization & Deserialization

	Java Type		Hadoop Types
        ----------              ------------
	String			Text
	Int			IntWritable
	Long			LongWritable
	Null			NullWritable
	Double			DoubleWritable
	Byte			ByTeWriteable
	Boolean			BoolWritable


    Submitting an MR program
    ------------------------

      	hadoop jar /home/cloudera/wordcount.jar com.cts.WordCountJob wordcount_input.txt wordcount_output
	yarn jar /home/cloudera/wordcount.jar com.cts.WordCountJob wordcount_input.txt wordcount_output


   Mapper Life Cycle
   -----------------

	1. setup
		=> Called 'only once' at the beggining before the map method is called.
		=> Define initialization code here.

	2. map
		=> Called after setup method.
		=> Called once per every input (K, V) pair.

	3. cleanup
		=> Called 'only once'  after map is called for all input records.
		=> Do any cleaup tasks, clean-up your resources, connections etc. 

	4. run  (not generally overridden)


   Reducer Life Cycle
   ------------------
	1. setup
		=> Called 'only once' at the beggining before the reduce method is called.
		=> Define initialization code here.

	2. reduce
		=> Called after setup method.
		=> Called once per every input (K, Iterable[V]) pair.

	3. cleanup
		=> Called 'only once'  after reduce is called for all input records.
		=> Do any cleaup tasks, clean-up your resources, connections etc. 

	4. run  (not generally overridden)


    Facebook Dataset Analysis
    -------------------------	
     
     Use-Case: From facebook dataset, find out the record with highest 'likes_received' from
	       among the users who are born between 1990 and 2000.

     Two MR programs:
	1. age mapper => Filters the records with dob_year between 1990 and 2000
	2. maxlikes mapper  => Filters the record with highest maxlikes from the output of age mapper.

	In this example there is no need of reduce as this is simple filtering operation.
	No aggregation is involved, hence no reducer is required. 
 


   Creating MR java project from scratch 
   --------------------------------------

	=> Create a Java Project
	=> Add the following "external jar files" to build path of the Java project
	   -> /usr/lib/hadoop/hadoop-common.jar
	   -> /usr/lib/hadoop/hadoop-0.20-mapreduce/hadoop-core.jar

 ===============================================
     Hive
 ===============================================
    
   -> Data Warehousing Platform built on top of Hadoop
   -> Hive used HDFS as storage layer and used MapReduce as its execution engine.

   -> Hive provides an SQL abstraction on top of MR, so that user need not write MR programs

   -> Users can send HQL statements to Hive Engine.
	 -> Hive converts user queries to MR programs and runs them on the cluster.
   
     
    How to work with Hive
    ---------------------

    1. Command Line Interface (CLI) 

	1. hive shell
		-> Connects to local Hive engine. No JDBC required.

	2. beeline shell
		-> preferred in corporate environemnts, as it offers more security
		-> Connects to Hive engine through JDBC connection.

    2. Using Client applications (JDBC/ODBC Drivers)

    3. Hive User Environemnt (HUE) - (uses Beeswax protocol)  


    Hive Components
    ----------------

	1. Hive Warehouse   
	   -> Is a hdfs directory, where Hive stores all the data files of managed tables
	   -> defined in hive-site.xml config file ( /user/hive/warehouse )
	
	   -> We will have a default database called "default" 
	       -> This database will not have any directory of its own 

	   -> We can create user defined databases and tables
		-> Each user-defined database has a directory in hive warehouse

	  Some Notes
          ----------
		=> 'default' database wil not any separate directory in the warehouse.

        2. Metastore
           -> Usually an external service (such as MySQL) where Hve stores its metadata.
	
	
	
    Schema on Read
    ---------------
     => Hive does not validate the data while loading (schema on write is not supported)
     => data is validated against the schema while reading the data
	 -> If the data can not casted to defined types, or is invalid, then Hive will show NULL
	 -> Hive also shows NULL is empty data. 
	
    
  
==== Database ===

 create database ctsdb;
 create database if not exists ctsdb;
 

 drop database ctsdb1;   	  // drop an empty database
 drop database ctsdb1 cascade;    // drop a non-empty database

 // the terms 'database' and 'schema' can be interchangably used.
 create schema ctsdb;
 create schema if not exists ctsdb;
 drop schema ctsdb1;


 ==== Tables ====

create table empinfo(empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
row format delimited
fields terminated by ","
lines terminated by "\n"
stored as textfile;


create table empinfo2(empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
row format delimited
fields terminated by ",";

create table empinfo_tab(empid INT, empname STRING, salary DOUBLE, deptid SMALLINT)
row format delimited
fields terminated by "\t";


Describe Table 
--------------
desc empinfo;
desc formatted empinfo;
desc extended empinfo;


loading data from data files
----------------------------

// loading from local file system -- COPY operation

load data local inpath '<file-path>' into table <table-name>    	// append mode
load data local inpath '<file-path>' overwrite into table <table-name>  // overwrite mode


// loading from hdfs -- MOVE operation

load data inpath '<file-path>' into table <table-name>    	  // append mode
load data inpath '<file-path>' overwrite into table <table-name>  // overwrite mode

examples
--------
load data local inpath '/home/cloudera/hive/Hive/data/emp.txt' into table empinfo; 
load data local inpath '/home/cloudera/hive/Hive/data/emp.txt' overwrite into table empinfo; 
load data local inpath '/home/cloudera/hive/Hive/data/emp2.txt' into table empinfo; 
load data local inpath '/home/cloudera/hive/Hive/data/emp_tab.txt' into table empinfo_tab; 


   Hive Table Types
   ----------------
	=> Managed tables
	=> External tables


   Hive Managed Tables
   -------------------
	-> Tables managed by hive
	-> When you srop a managed table, both schema as well as data are droped. 


   Hive External Tables
   --------------------
	-> The data of external tables is not managed by Hive.
	-> External tables are created in a user-specified location (outside of hive warehouse directory)

	-> When you drop an external table:
		-> ONly the schema of the table is dropped from the metastore
		-> The datafiles of the table are not dropped. 
		
  Partitioned Tables
  ------------------

    In Hive, Partitions provide a way of dividing a table into coarse-grained parts 
    based on the value of a partition column, such as a date or country.

    => We can load data into partitioned tables using two approaches:
	
	1. Static Partitioning

	   -> When we know precisly into which partition we are loading the data.
	   -> We can 'load data' command to load data using static partitioning.

	2. Dynamic Partitioning

	   -> When a specific data file can not loaded into any given partition
		-> Your file may have partition data also and may have data for several partitions.

	   -> Here, the data must be loaded from a base table using "INSERT .. SELECT" command
		-> We can not use 'load data' command in this case.



CREATE TABLE students_partition_static (id INT, name STRING, mobile STRING, email STRING, project STRING)
PARTITIONED BY (country STRING)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE

show partitions students_partition_static;

CREATE TABLE students_partition_dynamic (id INT, name STRING, mobile STRING, email STRING, project STRING)
PARTITIONED BY (country STRING, batch INT)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
STORED AS TEXTFILE

CREATE TABLE students_partition_dynamic_tab (id INT, name STRING, mobile STRING, email STRING, project STRING)
PARTITIONED BY (country STRING, batch INT)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE

== loading data into partitioned tables ====

load data local inpath '/home/cloudera/hive/Hive/data/students_IN.txt' into table students_partition_static 
partition (country = 'UK')

-- loading data using dynamic partitioning method ---


-- load data into a base table
load data local inpath '/home/cloudera/hive/Hive/data/students_country_batch.csv' into table students;

-- make dynamic partition mode to nonstrict
set hive.exec.dynamic.partition.mode=nonstrict;

-- load data into partitioned table using insert .. select command
insert into students_partition_dynamic
partition(country, batch)
select id, name, mobile, email, project, country, batch from students;

insert into students_partition_dynamic
partition(country='India', batch)
select id, name, mobile, email, project, batch from students
where country = 'India';

insert into students_partition_dynamic
partition(country='India', batch=1)
select id, name, mobile, email, project from students
where country = 'India' and batch=1;

// ERROR
// Line 2:10 Dynamic partition cannot be the parent of a static partition '1'
insert into students_partition_dynamic
partition(country, batch=1)
select id, name, mobile, email, project, country from students
where batch=1;


// insert overwrite => overwrite mode
insert overwrite table students_partition_dynamic
partition(country, batch)
select id, name, mobile, email, project, country, batch from students;

  NOTE: The data files of the partitioned tables does not contain partition column info
	The partition info comes from directory-name.


    Bucketting of Tables
    ---------------------
     => Bucketting adds an additional layer of granularity at the table or at the partition level 
	where Hive creates fixed number of bucketted files based on hash of a column (called clustered column)


--- bucketted table ---

CREATE TABLE students_bucket (id INT, name STRING, mobile STRING, email STRING, project STRING, country STRING, batch INT)
CLUSTERED BY (id) 
INTO 4 BUCKETS;
CREATE TABLE students_bucket_pipe (id INT, name STRING, mobile STRING, email STRING, project STRING, country STRING, batch INT)


CLUSTERED BY (country) 
SORTED BY (id) 
INTO 3 BUCKETS
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

--loading data into bucketted table---
insert into students_bucket_pipe select * from students; 

-- bucketting a partitioned table
CREATE TABLE students_part_bucket (id INT, name STRING, mobile STRING, email STRING, project STRING, country STRING)
PARTITIONED BY (batch INT)
CLUSTERED BY (id) INTO 3 BUCKETS
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
STORED AS TEXTFILE

insert into students_part_bucket 
partition(batch)
select id, name, mobile, email, project, country, batch from students;




















