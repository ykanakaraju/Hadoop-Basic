
  Agenda
  ------------

  Big Data - Basic
  Hadoop 
   - HDFS - File Ops & Architecture
   - YARN - Basics & Architecture
   - MapReduce - Basics & Examples & Architecture
  Hive
  Impala (introduction)
  Sqoop 
  HBase
  Kafka 
  NiFi
  Spark ( PySpark ) 

  ==============================================

   Materials
   
        => PDF Presentations
	=> Code Modules  
	=> Class Notes 

 ===============================================

   Big Data  
   --------
     -> Large & Complex Data that can not be reasonbly stored and processed using traditional
        data management systems that rely on single server architectures.

     	-> Volume 
	-> Velocity
	-> Variety  (unstructured & semi structured data)
	-> Veracity (data in doubt)
	-> Value (performing deep analytics)

   Big Data Problems
   -----------------
	-> Storing huge amount of data
	-> Processing data faster
	-> Processing complex data

   => As the problems mainly arise because of the limitations of a single machine, 
      we solve this by using the combined resource of many machines. 
	 -> We use "a computing cluster".

   What is a Computing Cluster ?
   -----------------------------
     => Is a unified entity that comprises of many nodes whose cumulative resources can be used
        to stores and process huge amount of data.

   What is Hadoop ?
   ---------------- 
    => Hadoop is an open source framework to storage and process big data using a cluster
       made of commodity hardware. 
	
    => Written in Java

	
	Hadoop  => HDFS :  Distributed Storage Framework
		=> MapReduce : Distributed Processing Framework

	

   Hadoop Distributed File System (HDFS)
   -------------------------------------
	
     -> Distributed File System
	
     -> Stores data in blocks of 128 MB each
     -> Each block is replicated 3 times by default to provide fail-safety.
	 --> 3 copies of each block will be there on different nodes
	

   HDFS Daemons
   ------------
	Daemon -> A continuosly running background job (java program)

	1. Name Node (NN)
		-> Runs on the master machine
		-> NN receives heart-beats from DNs. 
		   -> If three successive heart-beats are missed, then the NN marks the DN as "unavailable" 

		-> NN receives block reports and maintains this info in two files:
                 
                   1. EditLog file
			-> dynamic file maintained in the RAM

		   2. FSImage file
			-> static file which is updated only during check-point process.
	

	2. Data Node (DN)
		-> Runs on all the slave machines.
		-> Data Nodes send heart-beats to NN once every three seconds.
		-> Data Nodes send "block reports"
			-> Files, Blocks, ReplicationFactor, Owner, Permissions
			-> Total storage, Reserverd Storage, Available Storage

	3. Secondary Name Node
		-> Master Process
		-> Performs check-pointing once every hour where it merges the edit-log with 
		   FSIMage files and copies and updated (checkpointed) FSImages file to the master node. 

	4. Balancer
		-> Maintain replication-factor of files.
		-> Background job that monitor under-replicated & over-replicated files.		
		

   Hadoop Distributions
   -------------------- 
   
     => Will provide a collection of pre-packaged and configured tools that are commonly required 
	for all big data projects, so that we do not have to download, install, configure and tune 
        those components separtly
	
     => Also provide some management tool to manage, monitor, repair, start  & stop services etc. to
        make managing te cluster easy. 
	
    On-Primesis Clusters
    --------------------
	1. Cloudera 
	2. HortonWorks  (merged with Cloudera)
	3. MapR

    Cloud Clusters
    ---------------
	1. AWS    - EMR (Elastic MapReduce - Managed Hadoop Cluster)
	2. Azure  - HDInsight
	3. GCP    - DataProc 
	

    HDFS File Operations   (CDH 4.7)
    --------------------------------
      
      User: cloudera, Password: cloudera

      Home Directory: 

	On Linux File System:  /home/cloudera
	On HDFS File System:   /user/cloudera

	HDFS Commands => hadoop fs -<command>
			 hdfs dfs -<command>

     

    
     	
     

	
           



		































